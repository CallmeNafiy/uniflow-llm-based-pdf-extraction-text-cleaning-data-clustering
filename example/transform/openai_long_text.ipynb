{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Example of generating QAs for a 10K\n",
    "In this example, we will show you how to generate question-answers (QAs) from a pdf using OpenAI's models via `uniflow`'s [OpenAIJsonModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/model_flow.py#L125).\n",
    "\n",
    "For this example, we're using a [10K from Nike](https://investors.nike.com/investors/news-events-and-reports/).\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we are storing the Nike 10K in the `data\\raw_input` directory as \"nike-10k-2023.pdf\". You can download the file from [here](https://s1.q4cdn.com/806093406/files/doc_downloads/2023/414759-1-_5_Nike-NPS-Combo_Form-10-K_WR.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping uniflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -q python-dotenv openai\n",
    "!{sys.executable} -m pip uninstall -y uniflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformOpenAIConfig\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts\n",
    "\n",
    "First, we need to demonstrate sample prompts for LLM. We do this by giving a sample list of `Context` examples to the `PromptTemplate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    few_shot_prompt=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"\"\"The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantum\n",
    "for InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements; \"\"\",\n",
    "            question=\"What does the Compute & Networking segment include?\",\n",
    "            answer=\"\"\"The Compute & Networking segment includes Data Center accelerated computing platforms, end-to-end networking platforms (Quantum for InfiniBand and Spectrum for Ethernet), the NVIDIA DRIVE automated-driving platform, and automotive development agreements.\"\"\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for the given `page_contents` above, we convert them to the `Context` class to be processed by `uniflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17)'s default LLM to generate questions and answers.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `OpenAIConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformOpenAIConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=OpenAIModelConfig(response_format={\"type\": \"text\"}),\n",
    ")\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a95e6486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Context(context='CHAPTER I \"Well, Prince, so Genoa and Lucca are no...')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def read_and_chunk(file_path, words_per_chunk=2500):\n",
    "    # Initialize variables\n",
    "    contexts = []\n",
    "    current_chunk_words = []\n",
    "\n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line into words\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                current_chunk_words.append(word)\n",
    "                # Check if the current chunk reached the specified number of words\n",
    "                if len(current_chunk_words) >= words_per_chunk:\n",
    "                    # Join the words to form a context and add to the list\n",
    "                    contexts.append(Context(context=' '.join(current_chunk_words)))\n",
    "                    current_chunk_words = []  # Reset for the next chunk\n",
    "\n",
    "    # Add the last chunk if there are any remaining words\n",
    "    if current_chunk_words:\n",
    "        contexts.append(Context(context=' '.join(current_chunk_words)))\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# Example usage\n",
    "file_path = './data/raw_input/book-war-and-peace.txt'\n",
    "contexts = read_and_chunk(file_path)\n",
    "for context in contexts[:1]:  # Just printing the first Context for brevity\n",
    "    print(f\"---\\nContext(context='{context.context[:50]}...')\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "output = client.run(contexts[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230239f6",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74ccffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['question: Who is the young Princess Bolkonskaya married to?\\n'\n",
      " 'answer: The young Princess Bolkonskaya is married to Prince Bolkonski, and '\n",
      " 'she is known as the most fascinating woman in Petersburg.']\n",
      "['question: Who entered the drawing room as another visitor?\\n'\n",
      " \"answer: Prince Andrew Bolkonski, the little princess' husband.\"]\n",
      "['question: Who was speaking French and stressing the last syllable of the '\n",
      " \"general's name like a Frenchman?\\n\"\n",
      " 'answer: Bolkonski.']\n",
      "[\"question: Who was the lady's gown for a house dress as fresh and elegant as \"\n",
      " 'the other?\\n'\n",
      " 'answer: The princess.']\n",
      "['question: Who was betting with Stevens, an English naval officer, that he '\n",
      " 'would drink a bottle of rum sitting on the outer ledge of the third floor '\n",
      " 'window with his legs hanging out?\\n'\n",
      " 'answer: Dolokhov was betting with Stevens.']\n",
      "['question: Who are the three men known for their misadventures with a bear?\\n'\n",
      " \"answer: Anatole Kuragin, Prince Vasili's son, and a certain Dolokhov.\"]\n",
      "['question: Who is Sonya in the context?\\n'\n",
      " \"answer: Sonya is a slender little brunette who is the count's \"\n",
      " 'fifteen-year-old niece.']\n",
      "['question: Who is the Countess inviting to dinner?\\n'\n",
      " 'answer: The Countess is inviting Pierre to dinner.']\n",
      "[\"question: Who is Boris's mother?\\n\"\n",
      " 'answer: Princess Anna Mikhaylovna Drubetskaya.']\n",
      "['question: Who wrote A Mathematical Theory of Communication in 1948?\\n'\n",
      " 'answer: Claude E. Shannon.']\n",
      "['question: What were the guests doing before the ices and champagne were '\n",
      " 'served?\\n'\n",
      " 'answer: The guests were seated at the table and engaging in lively '\n",
      " 'conversation.']\n",
      "['question: What did the old priest say about the limits of human life?\\n'\n",
      " 'answer: The old priest said, \"The limits of human life... are fixed and may '\n",
      " 'not be o\\'erpassed.\"']\n",
      "['question: Who accompanied Pierre in the carriage as it drove into the court '\n",
      " \"of Count Bezukhov's house?\\n\"\n",
      " 'answer: Anna Mikhaylovna.']\n",
      "['question: Who was the dying man that everyone was attending to in the large '\n",
      " 'room?\\n'\n",
      " 'answer: The dying man was Count Bezukhov, the father of Pierre.']\n",
      "['question: Who was speaking in excited whispers to Anna Mikhaylovna?\\n'\n",
      " 'answer: The younger of the two speakers was speaking in excited whispers to '\n",
      " 'Anna Mikhaylovna.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(len(output))\n",
    "for o in output:\n",
    "    pprint(o['output'][0]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
