{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Example of generating QAs for a 10K\n",
    "In this example, we will show you how to generate question-answers (QAs) from a pdf using OpenAI's models via `uniflow`'s [OpenAIJsonModelFlow](https://github.com/CambioML/uniflow/blob/main/uniflow/flow/model_flow.py#L125).\n",
    "\n",
    "For this example, we're using a [10K from Nike](https://investors.nike.com/investors/news-events-and-reports/).\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we are storing the Nike 10K in the `data\\raw_input` directory as \"nike-10k-2023.pdf\". You can download the file from [here](https://s1.q4cdn.com/806093406/files/doc_downloads/2023/414759-1-_5_Nike-NPS-Combo_Form-10-K_WR.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping uniflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -q python-dotenv openai\n",
    "!{sys.executable} -m pip uninstall -y uniflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformOpenAIConfig, TransformForGenerationOpenAIGPT3p5Config\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts\n",
    "\n",
    "First, we need to demonstrate sample prompts for LLM. We do this by giving a sample list of `Context` examples to the `PromptTemplate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    few_shot_prompt=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"\"\"The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantum\n",
    "for InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements; \"\"\",\n",
    "            question=\"What does the Compute & Networking segment include?\",\n",
    "            answer=\"\"\"The Compute & Networking segment includes Data Center accelerated computing platforms, end-to-end networking platforms (Quantum for InfiniBand and Spectrum for Ethernet), the NVIDIA DRIVE automated-driving platform, and automotive development agreements.\"\"\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for the given `page_contents` above, we convert them to the `Context` class to be processed by `uniflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17)'s default LLM to generate questions and answers.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `OpenAIConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = TransformOpenAIConfig(\n",
    "#     prompt_template=guided_prompt,\n",
    "#     model_config=OpenAIModelConfig(response_format={\"type\": \"text\"}),\n",
    "#     # auto_split_long_text=False\n",
    "# )\n",
    "\n",
    "config2 = TransformForGenerationOpenAIGPT3p5Config(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=OpenAIModelConfig(),\n",
    "    auto_split_long_text=False\n",
    ")\n",
    "\n",
    "client = TransformClient(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a95e6486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Context(context='CHAPTER I \"Well, Prince, so Genoa and Lucca are no...')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def read_and_chunk(file_path, words_per_chunk=2500):\n",
    "    # Initialize variables\n",
    "    contexts = []\n",
    "    current_chunk_words = []\n",
    "\n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line into words\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                current_chunk_words.append(word)\n",
    "                # Check if the current chunk reached the specified number of words\n",
    "                if len(current_chunk_words) >= words_per_chunk:\n",
    "                    # Join the words to form a context and add to the list\n",
    "                    contexts.append(Context(context=' '.join(current_chunk_words)))\n",
    "                    current_chunk_words = []  # Reset for the next chunk\n",
    "\n",
    "    # Add the last chunk if there are any remaining words\n",
    "    if current_chunk_words:\n",
    "        contexts.append(Context(context=' '.join(current_chunk_words)))\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# Example usage\n",
    "file_path = './data/raw_input/book-war-and-peace.txt'\n",
    "contexts = read_and_chunk(file_path)\n",
    "for context in contexts[:1]:  # Just printing the first Context for brevity\n",
    "    print(f\"---\\nContext(context='{context.context[:50]}...')\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.81s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# print(contexts[0])\n",
    "\n",
    "output = client.run(contexts[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230239f6",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74ccffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "{'output': [{'error': 'No errors.',\n",
      "             'response': ['question: Who was the speaker in the dialogue '\n",
      "                          'mentioned in the context?\\n'\n",
      "                          'answer: Anna Pavlovna Scherer was the speaker in '\n",
      "                          'the dialogue mentioned in the context.']}],\n",
      " 'root': <uniflow.node.Node object at 0x1221dc100>}\n",
      "['question: Who was the speaker in the dialogue mentioned in the context?\\n'\n",
      " 'answer: Anna Pavlovna Scherer was the speaker in the dialogue mentioned in '\n",
      " 'the context.']\n",
      "['question: Who graced the drawing room with her presence in the context '\n",
      " 'mentioned?\\n'\n",
      " 'answer: The little princess.']\n",
      "['question: What is the topic of conversation at the beginning of the '\n",
      " 'document?\\n'\n",
      " 'answer: The conversation involves debates about Napoleon, the Revolution, '\n",
      " 'the rights of man, and the ideals of liberty and equality.']\n",
      "['question: Who is the article \"A Mathematical Theory of Communication\" by?\\n'\n",
      " 'answer: The article \"A Mathematical Theory of Communication\" was published '\n",
      " 'by Claude E. Shannon.']\n",
      "['question: What are the main topics of discussion in the last context?\\n'\n",
      " 'answer: The main topics of discussion are Pierre and Anatole being at '\n",
      " \"Kuragin's place where drinking and gambling take place. Anatole and Dolokhov \"\n",
      " 'are the prominent characters here. At this gathering, Pierre is seen '\n",
      " 'drinking and observing all the activities happening.']\n",
      "['question: Who is the son of Marya Ivanovna Dolokhova, a worthy woman?\\n'\n",
      " 'answer: Dolokhov.']\n",
      "['question: Who is in love with Boris?\\nanswer: Sonya is in love with Boris.']\n",
      "['question: Who is the count referring to when he mentions inviting someone to '\n",
      " 'dinner, and who is he hoping is feeling better?\\n'\n",
      " 'answer: The count is referring to inviting Pierre to dinner, and he is '\n",
      " 'hoping that Count Cyril Vladimirovich is feeling better.']\n",
      "['question: What is the content of the context provided?\\n'\n",
      " 'answer: The context is about a scene from a book where characters are '\n",
      " 'entering a house, and Mother is asking Boris to be respectful to Count Cyril '\n",
      " 'Vladimirovich. Prince Vasili comes in and his arrival makes Boris and Anna '\n",
      " 'Mikhaylovna uneasy. Prince Vasili and Anna Mikhaylovna converse about the '\n",
      " \"prince's health and the future of Boris. Later, Pierre visits the house and \"\n",
      " \"is met with a cold reception from the count's family. Boris arrives and \"\n",
      " 'tries to introduce himself to Pierre, who clearly has no recollection of '\n",
      " 'Boris.']\n",
      "[\"question: What does the Count's room, full of tobacco smoke, turn to for \"\n",
      " 'discussion?\\n'\n",
      " \"answer: The conversation in the Count's room, full of tobacco smoke, turns \"\n",
      " 'to war and the recruiting process.']\n",
      "['question: What is the topic of conversation at the table?\\n'\n",
      " 'answer: The conversation at the table revolves around the impending war and '\n",
      " 'the declaration of war against Bonaparte. Additionally, there is '\n",
      " 'light-hearted banter about the sweets and ice pudding being served.']\n",
      "['question: What was the name of the dance that the count performed at the '\n",
      " 'ball?\\n'\n",
      " \"answer: The count's favorite dance was the Daniel Cooper, which he had \"\n",
      " 'danced in his youth.']\n",
      "['question: Who are the two men who look like tradespeople and what do they do '\n",
      " 'while Pierre is getting down from the carriage steps?\\n'\n",
      " 'answer: Two men who look like tradespeople ran hurriedly from the entrance '\n",
      " 'and hid in the shadow of the wall while Pierre was getting down from the '\n",
      " 'carriage steps.']\n",
      "['question: Who is the dying man that Anna Mikhaylovna and Pierre are with?\\n'\n",
      " \"answer: The dying man was Pierre's father, Count Bezukhov.\"]\n",
      "['question: Who were the people gathered around the sick man in the room?\\n'\n",
      " 'answer: The sick man was surrounded by doctors, princesses, and servants.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(len(output))\n",
    "\n",
    "pprint(output[0])\n",
    "\n",
    "for o in output:\n",
    "    pprint(o['output'][0]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
