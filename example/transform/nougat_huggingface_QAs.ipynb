{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23393b1c-b26c-4372-ba4e-58cb2033dfda",
   "metadata": {},
   "source": [
    "# Generate QAs based on the target PDF extracted from `Nougat`\n",
    "\n",
    "This package uses `nougat`, a tool for extracting information from specific PDF files. It also includes a feature that uses OpenAI's language models to check the accuracy of this extracted information. Additionally, the package uses `uniflow` to create questions and answers based on the information taken from the PDFs.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we are storing the Nike 10K in the `data\\raw_input` directory as \"nike-10k-2023.pdf\". You can download the file from [here](https://s1.q4cdn.com/806093406/files/doc_downloads/2023/414759-1-_5_Nike-NPS-Combo_Form-10-K_WR.pdf).\n",
    "\n",
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c26092-47bd-424d-af91-102acbc9cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa47b52-401c-4dc2-a0d8-753d079a1be8",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b336c909-2c3f-4df5-8fef-0672eaeda8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip -q install transformers accelerate bitsandbytes scipy nougat-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76abb4c-59fd-4514-9c78-8c383f2b3dd8",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e3cb8c-3a98-4599-a9c2-f0b1be59bbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.flow.config import TransformOpenAIConfig, TransformHuggingFaceConfig, HuggingfaceModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e9a6a-c027-4f86-9e72-4ed8b11fce97",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "First, we need to pre-process the PDF to get text chunks that we can feed into the model. We will use `Nougat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b503110-0dde-4d5e-a88c-789dd2a347b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"amazon-10k-2023.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756067b-39e7-468a-815e-b8d0382985da",
   "metadata": {},
   "source": [
    "Set current directory and input data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2be9e7-907c-4062-b160-bb1b940b7c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/data/raw_input/amazon-10k-2023.pdf\n"
     ]
    }
   ],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", pdf_file)\n",
    "print(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d75887-9fe9-42bf-8d1c-4a0cdbc6881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/data\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.splitext(pdf_file)[0]\n",
    "output_directory = os.path.join(dir_cur, \"data\")\n",
    "print(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6dae-1a2b-462d-b46e-783510830d79",
   "metadata": {},
   "source": [
    "Run `Nougat` model to process content of target PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d93208d-4276-4864-b955-0395c32b89fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "INFO:root:Skipping amazon-10k-2023.pdf, already computed. Run with --recompute to convert again.\n"
     ]
    }
   ],
   "source": [
    "!nougat {input_file} -o {output_directory} -m 0.1.0-base --markdown --no-skipping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cf469-ed3e-45d9-9560-9403c8e3cfda",
   "metadata": {},
   "source": [
    "Below are helper functions designed to process the output of `Nougat`, ensuring that the output context is efficiently processed by the Hugging Face model.\n",
    "\n",
    "#### Overview:\n",
    "The `process_mmd_file` function is designed to process markdown files, particularly handling large sections and table content. It reads a markdown file, splits it into manageable sections, processes these sections to handle table content, and optionally utilizes OpenAI for further processing.\n",
    "\n",
    "#### Inputs:\n",
    "- `file_path`: A string representing the path to the markdown file to be processed.\n",
    "- `client_openAI`: An object representing the OpenAI client, used for processing sections of the markdown file.\n",
    "\n",
    "#### Workflow:\n",
    "1. **Reading the File**: The function starts by reading the entire content of the markdown file specified by `file_path`.\n",
    "2. **Initial Splitting**: The content is split into sections based on '##' headers. The first section is skipped if it's empty.\n",
    "3. **Sub-Splitting for Large Sections**: Sections larger than a predefined word count (`max_word_count`) are further split using '###' headers.\n",
    "4. **Processing for Table Content**: Each section is processed for table content if its word count exceeds `max_word_count_for_table`. This involves reducing the word count while preserving essential information.\n",
    "5. **Word Count Reduction Check**: After processing, if the word count of a section is reduced below a certain threshold (`reduction_threshold`), the section is further processed using the OpenAI client.\n",
    "6. **Compilation of Processed Sections**: All processed sections that are not empty are compiled into a list.\n",
    "7. **Statistics**: The function prints the number of sections that were further split and the number of sections that were significantly reduced in word count.\n",
    "  \n",
    "#### Output:\n",
    "- Returns a list of strings, where each string is a processed section of the original markdown file. This list represents the cleaned and potentially AI-processed sections of the markdown content.\n",
    "\n",
    "#### Note\n",
    "- We've observed that some text chunks, post table syntax removal processing, contain only headers. To enhance the relevance of the output, you can eliminate these header-only chunks by setting a minimum length requirement for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87232ac-0f06-4d91-b951-bab710e141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mmd_file(file_path, client_openAI):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Constants and counters\n",
    "    max_word_count_for_table = 25\n",
    "    max_word_count = 4096\n",
    "    reduction_threshold = 0.30\n",
    "    further_splitted_count = 0\n",
    "    significantly_reduced_count = 0\n",
    "\n",
    "    # Splitting the content\n",
    "    sections = content.split('##')\n",
    "    intermediate_sections = []\n",
    "\n",
    "    for i, section in enumerate(sections):\n",
    "        if i == 0 and not section.strip():\n",
    "            continue\n",
    "\n",
    "        # Add '##' back to the section header\n",
    "        if not section.lstrip().startswith('#'):\n",
    "            section = '##' + section\n",
    "\n",
    "        # Split large sections using '###'\n",
    "        if len(section.split()) > max_word_count:\n",
    "            sub_sections = section.split('###')\n",
    "            for sub_section in sub_sections:\n",
    "                if len(sub_section.split()) > max_word_count:\n",
    "                    further_splitted_sub_sections = split_large_section(sub_section, max_word_count)\n",
    "                    further_splitted_count += len(further_splitted_sub_sections) - 1\n",
    "                    intermediate_sections.extend(further_splitted_sub_sections)\n",
    "                else:\n",
    "                    intermediate_sections.append(sub_section)\n",
    "        else:\n",
    "            intermediate_sections.append(section)\n",
    "\n",
    "    # Process each section for table content and check word count reduction\n",
    "    cleaned_sections = []\n",
    "    for section in intermediate_sections:\n",
    "        original_word_count = len(section.split())\n",
    "        processed_section = process_for_table_content(section, max_word_count_for_table)\n",
    "\n",
    "        # Calculate word count reduction\n",
    "        processed_word_count = len(processed_section.split())\n",
    "        if processed_word_count == 0 or processed_word_count / original_word_count < reduction_threshold:\n",
    "            significantly_reduced_count += 1\n",
    "            # Use OpenAI-based processing for sections that are significantly reduced\n",
    "            temp_processed_section = clean_text_from_table_syntax_with_openAI(section, client_openAI)\n",
    "            if temp_processed_section:\n",
    "                processed_section = temp_processed_section\n",
    "\n",
    "        if processed_section:\n",
    "            cleaned_sections.append(processed_section)\n",
    "\n",
    "    print(f\"Number of chunks further split: {further_splitted_count}\")\n",
    "    print(f\"Number of significantly reduced chunks: {significantly_reduced_count}\")\n",
    "\n",
    "    return cleaned_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34ff65-87c8-41c0-a425-652d750ba013",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `split_large_section` function is designed to split a large text section into smaller chunks based on a specified maximum word count. This function is particularly useful for processing large blocks of text that need to be broken down for readability or specific processing requirements.\n",
    "\n",
    "#### Inputs:\n",
    "- `section`: A string representing the text section to be split.\n",
    "- `max_word_count`: An integer specifying the maximum word count for each chunk.\n",
    "\n",
    "#### Output:\n",
    "- Returns a list of strings, where each string represents a chunk of the original section. Each chunk contains words up to the specified `max_word_count`, ensuring no chunk exceeds this limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19c1d61-76af-4ca9-b787-12061623f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_section(section, max_word_count):\n",
    "    words = section.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk)) > max_word_count:\n",
    "            chunks.append(' '.join(current_chunk[:-1]))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654e832-904a-4d75-ba6e-347b6403c991",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `process_for_table_content` function is designed to filter and process text sections, specifically targeting content structured like tables. It aims to retain meaningful content while considering a maximum word count for each processed chunk.\n",
    "\n",
    "#### Inputs:\n",
    "- `section`: A string representing the text section to be processed. This section typically contains markdown content.\n",
    "- `max_word_count_for_table`: An integer specifying the maximum word count for each chunk within a table-like structure.\n",
    "\n",
    "#### Output:\n",
    "- Returns a string that represents the processed section. This string is composed of filtered lines that meet the criteria of having an appropriate word count and not being markdown headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4493d69-fc1c-4bbe-8628-7379d6691ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_table_content(section, max_word_count_for_table):\n",
    "    lines = [line for line in section.split('\\n') if line.strip() and not line.strip().startswith('##') and not line.strip().startswith('###')]\n",
    "    filtered_lines = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        end_index = min(i + 4, len(lines))\n",
    "        word_count = sum(len(line.split()) for line in lines[i:end_index])\n",
    "\n",
    "        if word_count >= max_word_count_for_table or end_index - i < 4:\n",
    "            filtered_lines.extend(lines[i:end_index])\n",
    "        i = end_index\n",
    "\n",
    "    return '\\n'.join(filtered_lines).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafd668-5b8a-44da-b6ea-4468ddb45ed8",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `clean_text_from_table_syntax_with_openAI` function is designed to process a text chunk, particularly focusing on cleaning and formatting text from table-like syntax, using the OpenAI API for advanced processing. This function is ideal for refining and simplifying complex text structures.\n",
    "\n",
    "#### Inputs:\n",
    "- `text_chunk`: A string representing the text chunk to be processed. It is expected to be potentially complex or table-like in structure.\n",
    "- `client_openAI`: An OpenAI client object used to process the text chunk.\n",
    "\n",
    "#### Output:\n",
    "- Returns the cleaned and processed text as a string if a valid 'cleaned_context' is extracted from the OpenAI client's response.\n",
    "- Returns an empty list if the input is invalid, or if the necessary data isn't found in the OpenAI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfe991a-0063-4352-95b3-f8c90501306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_from_table_syntax_with_openAI(text_chunk, client_openAI):\n",
    "    # Validate input\n",
    "    if not isinstance(text_chunk, str):\n",
    "        return []\n",
    "\n",
    "    input_data = [Context(context=text_chunk)]\n",
    "    output_openAI = client_openAI.run(input_data)\n",
    "\n",
    "    # Check if 'output' is in the first item of the output_openAI list\n",
    "    if isinstance(output_openAI, list) and len(output_openAI) > 0 and 'output' in output_openAI[0]:\n",
    "        first_output = output_openAI[0]['output']\n",
    "\n",
    "        # Check if first_output is a list and not empty\n",
    "        if isinstance(first_output, list) and len(first_output) > 0:\n",
    "            first_response = first_output[0]\n",
    "\n",
    "            # Check if 'response' is in the first_response and it's not empty\n",
    "            if isinstance(first_response, dict) and 'response' in first_response and isinstance(first_response['response'], list) and len(first_response['response']) > 0:\n",
    "                first_responses = first_response['response'][0]\n",
    "\n",
    "                # Check if 'responses' is in first_responses and it has at least two elements\n",
    "                if isinstance(first_responses, dict) and 'responses' in first_responses and isinstance(first_responses['responses'], list) and len(first_responses['responses']) > 1:\n",
    "                    cleaned_context = first_responses['responses'][1].get('cleaned_context')\n",
    "\n",
    "                    # Check if cleaned_context is not None\n",
    "                    if cleaned_context is not None:\n",
    "                        return cleaned_context\n",
    "\n",
    "    return []  # Return an empty list if the conditions are not met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c99194-3df6-4905-8de2-0c4b04a3aeed",
   "metadata": {},
   "source": [
    "Print the location of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb0a971-78ff-411b-92f7-d5fb61e04f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/data/amazon-10k-2023.mmd\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.splitext(pdf_file)[0]\n",
    "output_file = os.path.join(output_directory, f\"{base_name}.mmd\")\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542e933-5470-47cd-8555-d2c7dc487ef4",
   "metadata": {},
   "source": [
    "Create OpenAI client instance from `uniflow`, for further usage of `process_mmd_file` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fc93b1e-c6b4-4946-8151-5708a273c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt_openAI = PromptTemplate(\n",
    "instruction=\"\"\"Revise the original text, focusing on fully retaining the core textual content while removing elements resembling table \n",
    "syntax, including lines with a single number and a sign. Preserve headers like '##' and '###' in markdown format. Follow the format of the \n",
    "examples below to include original_context and cleaned_context in the response, under the 'responses' key in the JSON object.\"\"\",   \n",
    "few_shot_prompt=[\n",
    "    Context(\n",
    "        original_context=\"Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. \\[\\text{NON-U.S. RETAIL STORES}\\] Shannon introduced the concept of\\ninformation entropy for the first time. \\[\\frac{\\text{$}}{\\text{$}}\\]. \\n21%\\n507\\n25%\\n25%\\n\",\n",
    "        cleaned_context=\"Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. Shannon introduced the concept of\\ninformation entropy for the first time.\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "config_openAI = TransformOpenAIConfig(\n",
    "    prompt_template=guided_prompt_openAI,\n",
    "    model_config=OpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")\n",
    "\n",
    "client_openAI = TransformClient(config_openAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21f0bc-ff55-4d18-8691-c210537ec326",
   "metadata": {},
   "source": [
    "Number of chunks split by the processed output using the helper function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25d8639d-d422-4624-92a0-d688c739120e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a28deed34104cf2a5aa5541d5703d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d74c06894a440b9b13e8a293686722e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecbbf8b2e224d92a7cf576b2fc02254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655aeeac8d2d4107a3e142ccd3963d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3a3ed857f34345aa78c9747bc01a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4648034959274030bcf2ab3c7bdd3720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7171618d5e349bd8f30e796dbce01e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342023c5a8204b54b048511d9eb6ff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488c8b69b91d4461a35f77f6c90cd2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23d38514b1e47a4b1b00cd2bb30158f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93191b829044fe592fac07ee91a7c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3d96f99538436ca7d08083bbb8babe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443dfd5fcaa64425a0c1aaf3432ecee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163bbd11eb5d4dac870bfaa5ed25d809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb1f5c5b2324b58b63a597977da9438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea98428c8d224423a06df60113941534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12017934e8d4ab298e2a5753d11a4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48f06dee11547a4a831c8fa0434de81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c65929c8d641d4bbda1dbfa5ca0e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba78ad38b83c42c98763fa47f0b003d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654033e9450749939b09d76406ba15e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b879aa9fc34cec9a62d7ca7e1b5c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65aadc6e784e4da89c3d8a320e17b8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a93bb410834d3daa2a7b3307aaed53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c639ae422c4a0781f729beb222d64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO [abs_llm_processor]: Attempt 1 failed, retrying...\n",
      "INFO [abs_llm_processor]: Attempt 2 failed, retrying...\n",
      "INFO [abs_llm_processor]: Attempt 3 failed, retrying...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e1f52e51cf40489ac37d3cbc62eb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c741f5e5b134d0c8a8dc36fd9e8808f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0501114e7beb45b686b4022500fcd485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fe2a38836c41c8851dcb62db08b31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ace49a502c248c58b3bc0c1e6ca130d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82616a65e3f845268e3d3fd63a8287ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ac3f06d2b444bd9aff5f19a06404cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafacf7bdbbd495da0268863ff3418ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca113f4704684bd9898c508da0dfff95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512b5b458fbe4be1863e028c1205efef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f1f7897a924fd7aeaf5fa015d08f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6348763aa448c38e6746f7adde3d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e54ec5a917844b6a39a56d19bff71b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb9c3dc866345d196fdfd79fa2efc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e7c494dd25479487501dd4ad97d3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87cf6435e864ccca5617d4652383047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9e54d0dbab439881ac2457565a10fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c6fcd057fa48f5914e66646f13d70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb41533e7c843c5ab7ca46b11fb1076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e535f24f5f034ee79ca0690f1105de70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4610ba5bec4a709fe7d77c04b10013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841d72b396aa4a069985ffcd8c64b864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a7b3a1718f4cd496c6b01cb7b40448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c5e05211294f77aac5c11baecbab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dbbe7c85714b26b80b2e8c78d22a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d8d78551464682a53503847d3f5029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks further split: 12\n",
      "Number of significantly reduced chunks: 51\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "page_contents = process_mmd_file(output_file, client_openAI)\n",
    "print(len(page_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccda576-f497-4e80-bda4-89b04e0105b2",
   "metadata": {},
   "source": [
    "### Prepare sample prompts\n",
    "\n",
    "First, we need to demonstrate sample prompts for LLM, those include instruction and sample json format. We do this by giving a sample instruction and list of `Context` examples to the `PromptTemplate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e365fc-19d3-4935-9d70-2697bc4ccf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_instruction = \"\"\"Generate one question and its corresponding answer based on the context. Following \\\n",
    "the format of the examples below to include only question and answer in the response with reasonable length.\"\"\"\n",
    "\n",
    "sample_examples = [\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy dog.\",\n",
    "            question=\"What is the color of the fox?\",\n",
    "            answer=\"brown.\"\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy black dog.\",\n",
    "            question=\"What is the color of the dog?\",\n",
    "            answer=\"black.\"\n",
    "        )]\n",
    "\n",
    "guided_prompt = PromptTemplate(\n",
    "    instruction=sample_instruction,\n",
    "    few_shot_prompt=sample_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230701b-af7e-41e5-a7d2-14fb63e3ff7a",
   "metadata": {},
   "source": [
    "Next, for the given page_contents above, we convert them to the Context class to be processed by uniflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af49ab6d-b63f-465c-b68d-2a29df94fbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(context='Our businesses encompass a large variety of product types, service offerings, and delivery channels. The worldwide marketplace in which we compete is evolving rapidly and intensely competitive, and we face a broad array of competitors from many different industry sectors around the world. Our current and potential competitors include: (1) physical, e-commerce, and omnichannel retailers, publishers, vendors, distributors, manufacturers, and producers of the products we offer and sell to consumers and businesses; (2) publishers, producers, and distributors of physical, digital, and interactive media of all types and all distribution channels; (3) web search engines, comparison shopping websites, social networks, web portals, and other online and app-based means of discovering, using, or acqu', summary=''),\n",
       " Context(context='We regard our trademarks, service marks, copyrights, patents, domain names, trade dress, trade secrets, proprietary technologies, and similar intellectual property as critical to our success, and we rely on trademark, copyright, and patent law, trade-secret protection, and confidentiality and/or license agreements with our employees, customers, partners, and others to protect our proprietary rights. We have registered, or applied for the registration of, a number of U.S. and international domain names, trademarks, service marks, and copyrights. Additionally, we have filed U.S. and international patent applications covering certain of our proprietary technology.', summary=''),\n",
       " Context(context=\"Our employees are critical to our mission of being Earth's most customer-centric company. As of December 31, 2022, we employed approximately 1,541,000 full-time and part-time employees. Additionally, we use independent contractors and temporary personnel to supplement our workforce. Competition for qualified personnel is intense, particularly for software engineers, computer scientists, and other technical staff, and constrained labor markets have increased competition for personnel across other parts of our business.\\nAs we strive to be Earth's best employer, we focus on investment and innovation, inclusion and diversity, safety, and engagement to hire and develop the best talent. We rely on numerous and evolving initiatives to implement these objectives and invent mechanisms for talent de\", summary=''),\n",
       " Context(context='Our investor relations website is amazon.com/ir and we encourage investors to use it as a way of easily finding information about us. We promptly make available on this website, free of charge, the reports that we file or furnish with the Securities and Exchange Commission (\"SEC\"), corporate governance information (including our Code of Business Conduct and Ethics), and select press releases.', summary=''),\n",
       " Context(context='59Senior Vice President and Chief Financial OfficerShelley L. Reynolds\\n58Vice President, Worldwide Controller, and Principal Accounting OfficerAdam N. Selipsky\\n56CEO Amazon Web ServicesDavid A. Zapolsky\\n59Senior Vice President, General Counsel, and SecretaryJeffrey P. Bezos, Mr. Bezos founded Amazon.com in 1994 and has served as Executive Chair since July 2021. He has served as Chair of the Board since 1994 and served as Chief Executive Officer from May 1996 until July 2021, and as President from 1994 until June 1999 and again from October 2000 to July 2021.\\n**Andrew R. Jassy.** Mr. Jassy has served as President and Chief Executive Officer since July 2021, CEO Amazon Web Services from April 2016 until July 2021, and Senior Vice President, Amazon Web Services, from April 2006 until April 20', summary=''),\n",
       " Context(context='# Our International Operations Expose Us to a Number of Risks\\nOur international activities are significant to our revenues and profits, and we plan to further expand internationally. In certain international market segments, we have relatively little operating experience and may not benefit from any first-to-market advantages or otherwise succeed. It is costly to establish, develop, and maintain international operations and stores, and promote our brand internationally. Our international operations may not become profitable on a sustained basis.\\nIn addition to risks described elsewhere in this section, our international sales and operations are subject to a number of risks, including:\\n* local economic and political conditions;\\n* government regulation (such as regulation of our product and ', summary=''),\n",
       " Context(context='# The Variability in Our Retail Business Places Increased Strain on Our Operations\\nDemand for our products and services can fluctuate significantly for many reasons, including as a result of seasonality, promotions, product launches, or unforeseeable events, such as in response to global economic conditions such as recessionary fears or rising inflation, natural or human-caused disasters (including public health crises) or extreme weather (including as a result of climate change), or geopolitical events. For example, we expect a disproportionate amount of our retail sales to occur during our fourth quarter. Our failure to stock or restock popular products in sufficient amounts such that we fail to meet customer demand could significantly affect our revenue and our future growth. When we ov', summary=''),\n",
       " Context(context=\"# We Are Impacted by Fraudulent or Unlawful Activities of Sellers\\nThe law relating to the liability of online service providers is currently unsettled. In addition, governmental agencies have in the past and could in the future require changes in the way this business is conducted. Under our seller programs, we maintain policies and processes designed to prevent sellers from collecting payments, fraudulently or otherwise, when buyers never receive the products they ordered or when the products received are materially different from the sellers' descriptions, and to prevent sellers in our stores or through other stores from selling unlawful, counterfeit, pirated, or stolen goods, selling goods in an unlawful or unethical manner, violating the proprietary rights of others, or otherwise viola\", summary='')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ Context(context=p[:800], summary=\"\") for p in page_contents[8:18] if len(p) > 200 ]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398300-09f3-4420-a8a0-335a3c32ecbb",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [HuggingfaceModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L39)'s default LLM to generate questions and answers. Let's import the config and client of this model.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `HuggingfaceConfig` to use our customized instructions and examples, instead of the `uniflow` default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88035b71-1055-4d4d-8e1c-35d4324f6afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604a900463624cd59a7623df5db1e776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = TransformHuggingFaceConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=HuggingfaceModelConfig(batch_size=128))\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85594569-5cdc-44f5-be1e-2b7116334d33",
   "metadata": {},
   "source": [
    "Now we call the run method on the client object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada50841-d847-440e-b3e2-76a6261effa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec265813424c4bd0a10f76ea5d8c8ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfaac3-8895-48e4-8e72-34364037b0be",
   "metadata": {},
   "source": [
    "### Process the output\n",
    "\n",
    "Let's take a look of the generated output. We need to do a little postprocessing on the raw output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d9ea69-1b00-41ae-9638-28b33363182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': ' Our businesses encompass a large variety of product types, '\n",
      "            'service offerings, and delivery channels. The worldwide '\n",
      "            'marketplace in which we compete is evolving rapidly and intensely '\n",
      "            'competitive, and we face a broad array of competitors from many '\n",
      "            'different industry sectors around the world. Our current and '\n",
      "            'potential competitors include: (1) physical, e-commerce, and '\n",
      "            'omnichannel retailers, publishers, vendors, distributors, '\n",
      "            'manufacturers, and producers of the products we offer and sell to '\n",
      "            'consumers and businesses; (2) publishers, producers, and '\n",
      "            'distributors of physical, digital, and interactive media of all '\n",
      "            'types and all distribution channels; (3) web search engines, '\n",
      "            'comparison shopping websites, social networks, web portals, and '\n",
      "            'other online and app-based means of discovering, using, or acqu\\n',\n",
      " 'question': ' Which type of companies does the business compete against in '\n",
      "             'the global marketplace?\\n',\n",
      " 'answer': ' Physical, e-commerce, and omnichannel retailers, publishers, '\n",
      "           'vendors, distributors, manufacturers, producers, web search '\n",
      "           'engines, comparison shopping websites, social networks, web '\n",
      "           'portals, and other online and app-based means of discovering, '\n",
      "           'using, or acquiring products and services.'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "keywords = [\"context:\", \"question:\", \"answer:\"]\n",
    "pattern = '|'.join(map(re.escape, keywords))\n",
    "\n",
    "o = output[0]['output'][0]['response'][0] ## we only postprocess the first output\n",
    "segments = [segment for segment in re.split(pattern, o) if segment.strip()]\n",
    "result = {\n",
    "    \"context\": segments[-3].rstrip(\"summary:   \"),\n",
    "    \"question\": segments[-2],\n",
    "    \"answer\": segments[-1]\n",
    "}\n",
    "\n",
    "pprint(result, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d05cf-f9a6-4f51-a543-197bded734d2",
   "metadata": {},
   "source": [
    "Congrats! Your question answers from the given knowledge context are generated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015bb33a-fef2-43aa-a147-2ff86c510ea0",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
