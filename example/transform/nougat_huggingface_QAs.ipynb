{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23393b1c-b26c-4372-ba4e-58cb2033dfda",
   "metadata": {},
   "source": [
    "# Generate QAs based no the target PDF extracted from `Nougat`\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we are storing the Nike 10K in the `data\\raw_input` directory as \"nike-10k-2023.pdf\". You can download the file from [here](https://s1.q4cdn.com/806093406/files/doc_downloads/2023/414759-1-_5_Nike-NPS-Combo_Form-10-K_WR.pdf).\n",
    "\n",
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c26092-47bd-424d-af91-102acbc9cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa47b52-401c-4dc2-a0d8-753d079a1be8",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b336c909-2c3f-4df5-8fef-0672eaeda8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip -q install transformers accelerate bitsandbytes scipy nougat-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76abb4c-59fd-4514-9c78-8c383f2b3dd8",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e3cb8c-3a98-4599-a9c2-f0b1be59bbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.flow.config import TransformOpenAIConfig, TransformHuggingFaceConfig, HuggingfaceModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e9a6a-c027-4f86-9e72-4ed8b11fce97",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "First, we need to pre-process the PDF to get text chunks that we can feed into the model. We will use `Nougat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b503110-0dde-4d5e-a88c-789dd2a347b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"nike-10k-2023.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756067b-39e7-468a-815e-b8d0382985da",
   "metadata": {},
   "source": [
    "Set current directory and input data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2be9e7-907c-4062-b160-bb1b940b7c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/data/raw_input/nike-10k-2023.pdf\n"
     ]
    }
   ],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", pdf_file)\n",
    "print(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d75887-9fe9-42bf-8d1c-4a0cdbc6881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/data\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.splitext(pdf_file)[0]\n",
    "output_directory = os.path.join(dir_cur, \"data\")\n",
    "print(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6dae-1a2b-462d-b46e-783510830d79",
   "metadata": {},
   "source": [
    "Run `Nougat` model to process content of target PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d93208d-4276-4864-b955-0395c32b89fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "INFO:root:Skipping nike-10k-2023.pdf, already computed. Run with --recompute to convert again.\n"
     ]
    }
   ],
   "source": [
    "!nougat {input_file} -o {output_directory} -m 0.1.0-base --markdown --no-skipping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cf469-ed3e-45d9-9560-9403c8e3cfda",
   "metadata": {},
   "source": [
    "Below are helper functions designed to process the output of `Nougat`, ensuring that the output context is efficiently processed by the Hugging Face model.\n",
    "\n",
    "#### Overview:\n",
    "The `process_mmd_file` function is designed to process markdown files, particularly handling large sections and table content. It reads a markdown file, splits it into manageable sections, processes these sections to handle table content, and optionally utilizes OpenAI for further processing.\n",
    "\n",
    "#### Inputs:\n",
    "- `file_path`: A string representing the path to the markdown file to be processed.\n",
    "- `client_openAI`: An object representing the OpenAI client, used for processing sections of the markdown file.\n",
    "\n",
    "#### Workflow:\n",
    "1. **Reading the File**: The function starts by reading the entire content of the markdown file specified by `file_path`.\n",
    "2. **Initial Splitting**: The content is split into sections based on '##' headers. The first section is skipped if it's empty.\n",
    "3. **Sub-Splitting for Large Sections**: Sections larger than a predefined word count (`max_word_count`) are further split using '###' headers.\n",
    "4. **Processing for Table Content**: Each section is processed for table content if its word count exceeds `max_word_count_for_table`. This involves reducing the word count while preserving essential information.\n",
    "5. **Word Count Reduction Check**: After processing, if the word count of a section is reduced below a certain threshold (`reduction_threshold`), the section is further processed using the OpenAI client.\n",
    "6. **Compilation of Processed Sections**: All processed sections that are not empty are compiled into a list.\n",
    "7. **Statistics**: The function prints the number of sections that were further split and the number of sections that were significantly reduced in word count.\n",
    "  \n",
    "#### Output:\n",
    "- Returns a list of strings, where each string is a processed section of the original markdown file. This list represents the cleaned and potentially AI-processed sections of the markdown content.\n",
    "\n",
    "#### Note\n",
    "- We've observed that some text chunks, post table syntax removal processing, contain only headers. To enhance the relevance of the output, you can eliminate these header-only chunks by setting a minimum length requirement for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87232ac-0f06-4d91-b951-bab710e141f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mmd_file(file_path, client_openAI):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Constants and counters\n",
    "    max_word_count_for_table = 25\n",
    "    max_word_count = 4096\n",
    "    reduction_threshold = 0.30\n",
    "    further_splitted_count = 0\n",
    "    significantly_reduced_count = 0\n",
    "\n",
    "    # Splitting the content\n",
    "    sections = content.split('##')\n",
    "    intermediate_sections = []\n",
    "\n",
    "    for i, section in enumerate(sections):\n",
    "        if i == 0 and not section.strip():\n",
    "            continue\n",
    "\n",
    "        # Add '##' back to the section header\n",
    "        if not section.lstrip().startswith('#'):\n",
    "            section = '##' + section\n",
    "\n",
    "        # Split large sections using '###'\n",
    "        if len(section.split()) > max_word_count:\n",
    "            sub_sections = section.split('###')\n",
    "            for sub_section in sub_sections:\n",
    "                if len(sub_section.split()) > max_word_count:\n",
    "                    further_splitted_sub_sections = split_large_section(sub_section, max_word_count)\n",
    "                    further_splitted_count += len(further_splitted_sub_sections) - 1\n",
    "                    intermediate_sections.extend(further_splitted_sub_sections)\n",
    "                else:\n",
    "                    intermediate_sections.append(sub_section)\n",
    "        else:\n",
    "            intermediate_sections.append(section)\n",
    "\n",
    "    # Process each section for table content and check word count reduction\n",
    "    cleaned_sections = []\n",
    "    for section in intermediate_sections:\n",
    "        original_word_count = len(section.split())\n",
    "        processed_section = process_for_table_content(section, max_word_count_for_table)\n",
    "\n",
    "        # Calculate word count reduction\n",
    "        processed_word_count = len(processed_section.split())\n",
    "        if processed_word_count == 0 or processed_word_count / original_word_count < reduction_threshold:\n",
    "            significantly_reduced_count += 1\n",
    "            # Use OpenAI-based processing for sections that are significantly reduced\n",
    "            temp_processed_section = clean_text_from_table_syntax_with_openAI(section, client_openAI)\n",
    "            if temp_processed_section:\n",
    "                processed_section = temp_processed_section\n",
    "\n",
    "        if processed_section:\n",
    "            cleaned_sections.append(processed_section)\n",
    "\n",
    "    print(f\"Number of chunks further split: {further_splitted_count}\")\n",
    "    print(f\"Number of significantly reduced chunks: {significantly_reduced_count}\")\n",
    "\n",
    "    return cleaned_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34ff65-87c8-41c0-a425-652d750ba013",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `split_large_section` function is designed to split a large text section into smaller chunks based on a specified maximum word count. This function is particularly useful for processing large blocks of text that need to be broken down for readability or specific processing requirements.\n",
    "\n",
    "#### Inputs:\n",
    "- `section`: A string representing the text section to be split.\n",
    "- `max_word_count`: An integer specifying the maximum word count for each chunk.\n",
    "\n",
    "#### Output:\n",
    "- Returns a list of strings, where each string represents a chunk of the original section. Each chunk contains words up to the specified `max_word_count`, ensuring no chunk exceeds this limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19c1d61-76af-4ca9-b787-12061623f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_section(section, max_word_count):\n",
    "    words = section.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk)) > max_word_count:\n",
    "            chunks.append(' '.join(current_chunk[:-1]))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654e832-904a-4d75-ba6e-347b6403c991",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `process_for_table_content` function is designed to filter and process text sections, specifically targeting content structured like tables. It aims to retain meaningful content while considering a maximum word count for each processed chunk.\n",
    "\n",
    "#### Inputs:\n",
    "- `section`: A string representing the text section to be processed. This section typically contains markdown content.\n",
    "- `max_word_count_for_table`: An integer specifying the maximum word count for each chunk within a table-like structure.\n",
    "\n",
    "#### Output:\n",
    "- Returns a string that represents the processed section. This string is composed of filtered lines that meet the criteria of having an appropriate word count and not being markdown headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4493d69-fc1c-4bbe-8628-7379d6691ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_table_content(section, max_word_count_for_table):\n",
    "    lines = [line for line in section.split('\\n') if line.strip() and not line.strip().startswith('##') and not line.strip().startswith('###')]\n",
    "    filtered_lines = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        end_index = min(i + 4, len(lines))\n",
    "        word_count = sum(len(line.split()) for line in lines[i:end_index])\n",
    "\n",
    "        if word_count >= max_word_count_for_table or end_index - i < 4:\n",
    "            filtered_lines.extend(lines[i:end_index])\n",
    "        i = end_index\n",
    "\n",
    "    return '\\n'.join(filtered_lines).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafd668-5b8a-44da-b6ea-4468ddb45ed8",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "The `clean_text_from_table_syntax_with_openAI` function is designed to process a text chunk, particularly focusing on cleaning and formatting text from table-like syntax, using the OpenAI API for advanced processing. This function is ideal for refining and simplifying complex text structures.\n",
    "\n",
    "#### Inputs:\n",
    "- `text_chunk`: A string representing the text chunk to be processed. It is expected to be potentially complex or table-like in structure.\n",
    "- `client_openAI`: An OpenAI client object used to process the text chunk.\n",
    "\n",
    "#### Output:\n",
    "- Returns the cleaned and processed text as a string if a valid 'cleaned_context' is extracted from the OpenAI client's response.\n",
    "- Returns an empty list if the input is invalid, or if the necessary data isn't found in the OpenAI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfe991a-0063-4352-95b3-f8c90501306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_from_table_syntax_with_openAI(text_chunk, client_openAI):\n",
    "    # Validate input\n",
    "    if not isinstance(text_chunk, str):\n",
    "        return []\n",
    "\n",
    "    input_data = [Context(context=text_chunk)]\n",
    "    output_openAI = client_openAI.run(input_data)\n",
    "\n",
    "    # Check if 'output' is in the first item of the output_openAI list\n",
    "    if isinstance(output_openAI, list) and len(output_openAI) > 0 and 'output' in output_openAI[0]:\n",
    "        first_output = output_openAI[0]['output']\n",
    "\n",
    "        # Check if first_output is a list and not empty\n",
    "        if isinstance(first_output, list) and len(first_output) > 0:\n",
    "            first_response = first_output[0]\n",
    "\n",
    "            # Check if 'response' is in the first_response and it's not empty\n",
    "            if isinstance(first_response, dict) and 'response' in first_response and isinstance(first_response['response'], list) and len(first_response['response']) > 0:\n",
    "                first_responses = first_response['response'][0]\n",
    "\n",
    "                # Check if 'responses' is in first_responses and it has at least two elements\n",
    "                if isinstance(first_responses, dict) and 'responses' in first_responses and isinstance(first_responses['responses'], list) and len(first_responses['responses']) > 1:\n",
    "                    cleaned_context = first_responses['responses'][1].get('cleaned_context')\n",
    "\n",
    "                    # Check if cleaned_context is not None\n",
    "                    if cleaned_context is not None:\n",
    "                        return cleaned_context\n",
    "\n",
    "    return []  # Return an empty list if the conditions are not met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c99194-3df6-4905-8de2-0c4b04a3aeed",
   "metadata": {},
   "source": [
    "Print the location of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb0a971-78ff-411b-92f7-d5fb61e04f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/data/nike-10k-2023.mmd\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.splitext(pdf_file)[0]\n",
    "output_file = os.path.join(output_directory, f\"{base_name}.mmd\")\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542e933-5470-47cd-8555-d2c7dc487ef4",
   "metadata": {},
   "source": [
    "Create OpenAI client instance from `uniflow`, for further usage of `process_mmd_file` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fc93b1e-c6b4-4946-8151-5708a273c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt_openAI = PromptTemplate(\n",
    "instruction=\"\"\"Revise the original text, focusing on fully retaining the core textual content while removing elements resembling table \n",
    "syntax, including lines with a single number and a sign. Preserve headers like '##' and '###' in markdown format. Follow the format of the \n",
    "examples below to include original_context and cleaned_context in the response, under the 'responses' key in the JSON object.\"\"\",   \n",
    "few_shot_prompt=[\n",
    "    Context(\n",
    "        original_context=\"Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. \\[\\text{NON-U.S. RETAIL STORES}\\] Shannon introduced the concept of\\ninformation entropy for the first time. \\[\\frac{\\text{$}}{\\text{$}}\\]. \\n21%\\n507\\n25%\\n25%\\n\",\n",
    "        cleaned_context=\"Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. Shannon introduced the concept of\\ninformation entropy for the first time.\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "config_openAI = TransformOpenAIConfig(\n",
    "    prompt_template=guided_prompt_openAI,\n",
    "    model_config=OpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")\n",
    "\n",
    "client_openAI = TransformClient(config_openAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21f0bc-ff55-4d18-8691-c210537ec326",
   "metadata": {},
   "source": [
    "Number of chunks split by the processed output using the helper function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25d8639d-d422-4624-92a0-d688c739120e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6147eb66793a42839238dfa1e4f7edd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cbbbf37e374215afe784c30432adab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aed874bdb64ad988756987aa175c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d59c4db612d4459a7183b41bc298724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe7f14627fb439892bcc02f089e1cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bab9de10474dae9bc7558f7ba8e4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd0b1295cfd41889a485d12c916b707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c74106a82d46f79e15037a04baf7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969eb9166de84c678badffb8dc1f34c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2682fa7fe5bb4a34bd093a175a8414ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1569e0007b457082674aec8d7c7840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a52c58243144f08dd28a9e544542bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bb678fdfb2406885fd43f2d5f67a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9f9587f4df41f6b3a7d54e8295f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38abcc190a1d47508e0140552e9f6ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b399c4e7e0bd41f885f2ecc8e47e78f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e6918f7a004ab5a83efc556457c227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bca728035e14ed09a1f6ce100d79cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd80706ca634131a0865faae17f6f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0286ca0fb8d948c4aab58836d3d4c452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54aa1d245fa0446b914b6ff263d0a7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884aa2c73862419a815086bdce61aea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee0a4b6d4c040658d4098fbfb8f4510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0887a134851242d995b9b8f5ca26be1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec54570c4b848daa4b1a92041670e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18ab06c772f485b8b4c77fe01ab6263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks further split: 7\n",
      "Number of significantly reduced chunks: 26\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "page_contents = process_mmd_file(output_file, client_openAI)\n",
    "print(len(page_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccda576-f497-4e80-bda4-89b04e0105b2",
   "metadata": {},
   "source": [
    "### Prepare sample prompts\n",
    "\n",
    "First, we need to demonstrate sample prompts for LLM, those include instruction and sample json format. We do this by giving a sample instruction and list of `Context` examples to the `PromptTemplate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e365fc-19d3-4935-9d70-2697bc4ccf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_instruction = \"\"\"Generate one question and its corresponding answer based on the context. Following \\\n",
    "the format of the examples below to include context, question, and answer in the response.\"\"\"\n",
    "\n",
    "sample_examples = [\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy dog.\",\n",
    "            question=\"What is the color of the fox?\",\n",
    "            answer=\"brown.\"\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"The quick brown fox jumps over the lazy black dog.\",\n",
    "            question=\"What is the color of the dog?\",\n",
    "            answer=\"black.\"\n",
    "        )]\n",
    "\n",
    "guided_prompt = PromptTemplate(\n",
    "    instruction=sample_instruction,\n",
    "    few_shot_prompt=sample_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230701b-af7e-41e5-a7d2-14fb63e3ff7a",
   "metadata": {},
   "source": [
    "Next, for the given page_contents above, we convert them to the Context class to be processed by uniflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af49ab6d-b63f-465c-b68d-2a29df94fbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(context='We experience moderate fluctuations in aggregate sales volume during the year. Historically, revenues in the first and fourth fiscal quarters have slightly exceeded those in the second and third fiscal quarters. However, the mix of product sales may vary considerably as a result of changes in seasonal and geographic demand for particular types of footwear, apparel and equipment, as well as other macroeconomic, strategic, operating and logistics-related factors.\\nBecause NIKE is a consumer products company, the relative popularity and availability of various sports and fitness activities, as well as changing design trends, affect the demand for our products. We must, therefore, respond to trends and shifts in consumer preferences by adjusting the mix of existing product offerings, developing', summary=''),\n",
       " Context(context='We report our NIKE Brand operations based on our internal geographic organization. Each NIKE Brand geographic segment operates predominantly in one industry: the design, development, marketing and selling of athletic footwear, apparel and equipment. The Company\\'s reportable operating segments for the NIKE Brand are: North America; Europe, Middle East & Africa (\"EMEA\"); Greater China; and Asia Pacific & Latin America (\"APLA\"), and include results for the NIKE and Jordan brands. Sales through our NIKE Direct operations are managed within each geographic operating segment.\\nConverse is also a reportable operating segment and operates predominately in one industry: the design, marketing, licensing and selling of casual sneakers, apparel and accessories. Converse direct to consumer operations, i', summary=''),\n",
       " Context(context='For fiscal 2023, NIKE Brand and Converse sales in the United States accounted for approximately 43% of total revenues, compared to 40% and 39% for fiscal 2022 and fiscal 2021, respectively. We sell our products to thousands of retail accounts in the United States, including a mix of footwear stores, sporting goods stores, athletic specialty stores, department stores, skate, tennis and golf shops and other retail accounts. In the United States, we utilize NIKE sales offices to solicit such sales. During fiscal 2023, our three largest United States customers accounted for approximately 22% of sales in the United States.\\nOur NIKE Direct and Converse direct to consumer operations sell our products to consumers through various digital platforms. In addition, our NIKE Direct and Converse direct ', summary=''),\n",
       " Context(context=\"For fiscal 2023, non-U.S. NIKE Brand and Converse sales accounted for approximately 57% of total revenues, compared to 60% and 61% for fiscal 2022 and fiscal 2021, respectively. We sell our products to retail accounts through our own NIKE Direct operations and through a mix of independent distributors, licensees and sales representatives around the world. We sell to thousands of retail accounts and ship products from 67 distribution centers outside of the United States. Refer to Item 2. Properties for further information on distribution facilities outside of the United States. During fiscal 2023, NIKE's three largest customers outside of the United States accounted for approximately 14% of total non-U.S. sales.\\nIn addition to NIKE-owned and Converse-owned digital commerce platforms in over\", summary=''),\n",
       " Context(context='# PRODUCT RESEARCH, DESIGN AND DEVELOPMENT\\nWe believe our research, design and development efforts are key factors in our success. Technical innovation in the design and manufacturing process of footwear, apparel and athletic equipment receives continued emphasis as we strive to produce products that help to enhance athletic performance, reduce injury and maximize comfort, while decreasing our environmental impact.\\nIn addition to our own staff of specialists in the areas of biomechanics, chemistry, exercise physiology, engineering, digital technologies, industrial design, sustainability and related fields, we also utilize research committees and advisory boards made up of athletes, coaches, trainers, equipment managers, orthopedists, podiatrists, physicians and other experts who consult wi', summary=''),\n",
       " Context(context='Nearly all of our footwear and apparel products are manufactured outside the United States by independent manufacturers (\"contract manufacturers\"), many of which operate multiple factories. We are also supplied, primarily indirectly, by a number of materials, or \"Tier 2\" suppliers, who provide the principal materials used in footwear and apparel finished goods products. As of May 31, 2023, we had 146 strategic Tier 2 suppliers.\\nAs of May 31, 2023, our contract manufacturers operated 123 finished goods footwear factories located in 11 countries. For fiscal 2023, NIKE Brand footwear finished goods were manufactured by 15 contract manufacturers, many of which operate multiple factories. The largest single finished goods footwear factory accounted for approximately 9% of total fiscal 2023 NIKE', summary=''),\n",
       " Context(context='Our international operations and sources of supply are subject to the usual risks of doing business abroad, such as the implementation of, or potential changes in, foreign and domestic trade policies, increases in import duties, anti-dumping measures, quotas, safeguard measures, trade restrictions, restrictions on the transfer of funds and, in certain parts of the world, political tensions, instability, conflicts, nationalism and terrorism, and resulting sanctions and other measures imposed in response to such issues. We have not, to date, been materially affected by any such risk but cannot predict the likelihood of such material effects occurring in the future.\\nIn recent years, uncertain global and regional economic and political conditions have affected international trade and increased', summary=''),\n",
       " Context(context='The athletic footwear, apparel and equipment industry is highly competitive on a worldwide basis. We compete internationally with a significant number of athletic and leisure footwear companies, athletic and leisure apparel companies, sports equipment companies and large companies having diversified lines of athletic and leisure footwear, apparel and equipment, including aidas, Anta, ASICS, Li Ning, lulullemon athletica, New Balance, Puma, Under Armour and V.F. Corporation, among others. The intense competition and the rapid changes in technology and consumer preferences in the markets for athletic and leisure footwear and apparel and athletic equipment constitute significant risk factors in our operations. Refer to Item 1A. Risk Factors for additional information.\\nNIKE is the largest sell', summary='')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ Context(context=p[:800], summary=\"\") for p in page_contents[6:16] if len(p) > 200 ]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398300-09f3-4420-a8a0-335a3c32ecbb",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [HuggingfaceModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L39)'s default LLM to generate questions and answers. Let's import the config and client of this model.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `HuggingfaceConfig` to use our customized instructions and examples, instead of the `uniflow` default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88035b71-1055-4d4d-8e1c-35d4324f6afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b872f4bd752f4febade1a5a05f69896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = TransformHuggingFaceConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=HuggingfaceModelConfig(batch_size=128))\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85594569-5cdc-44f5-be1e-2b7116334d33",
   "metadata": {},
   "source": [
    "Now we call the run method on the client object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada50841-d847-440e-b3e2-76a6261effa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d963bbadc5f5492b8bbe6a72995b8ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfaac3-8895-48e4-8e72-34364037b0be",
   "metadata": {},
   "source": [
    "### Process the output\n",
    "\n",
    "Let's take a look of the generated output. We need to do a little postprocessing on the raw output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d9ea69-1b00-41ae-9638-28b33363182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': ' We experience moderate fluctuations in aggregate sales volume '\n",
      "            'during the year. Historically, revenues in the first and fourth '\n",
      "            'fiscal quarters have slightly exceeded those in the second and '\n",
      "            'third fiscal quarters. However, the mix of product sales may vary '\n",
      "            'considerably as a result of changes in seasonal and geographic '\n",
      "            'demand for particular types of footwear, apparel and equipment, '\n",
      "            'as well as other macroeconomic, strategic, operating and '\n",
      "            'logistics-related factors.\\n'\n",
      "            'Because NIKE is a consumer products company, the relative '\n",
      "            'popularity and availability of various sports and fitness '\n",
      "            'activities, as well as changing design trends, affect the demand '\n",
      "            'for our products. We must, therefore, respond to trends and '\n",
      "            'shifts in consumer preferences by adjusting the mix of existing '\n",
      "            'product offerings, developing\\n'\n",
      "            'summary:   ',\n",
      " 'question': \" How do consumer preferences and trends impact Nike's product \"\n",
      "             'offerings?\\n',\n",
      " 'answer': \" Consumer preferences and trends significantly influence Nike's \"\n",
      "           'product offerings. The company must respond to these trends and '\n",
      "           'shifts by adjusting the mix of existing products and developing '\n",
      "           'new ones accordingly.'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "keywords = [\"context:\", \"question:\", \"answer:\"]\n",
    "pattern = '|'.join(map(re.escape, keywords))\n",
    "\n",
    "o = output[0]['output'][0]['response'][0] ## we only postprocess the first output\n",
    "segments = [segment for segment in re.split(pattern, o) if segment.strip()]\n",
    "result = {\n",
    "    \"context\": segments[-3],\n",
    "    \"question\": segments[-2],\n",
    "    \"answer\": segments[-1]\n",
    "}\n",
    "\n",
    "pprint(result, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d05cf-f9a6-4f51-a543-197bded734d2",
   "metadata": {},
   "source": [
    "Congrats! Your question answers from the given knowledge context are generated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015bb33a-fef2-43aa-a147-2ff86c510ea0",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
