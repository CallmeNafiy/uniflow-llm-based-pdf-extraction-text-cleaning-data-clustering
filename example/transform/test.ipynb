{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import ExtractClient, TransformClient\n",
    "from uniflow.flow.config import ExtractHTMLConfig, TransformAzureOpenAIConfig\n",
    "from uniflow.flow.flow_factory import FlowFactory\n",
    "from uniflow.op.model.model_config import AzureOpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extract': ['ExtractHTMLFlow',\n",
       "  'ExtractImageFlow',\n",
       "  'ExtractIpynbFlow',\n",
       "  'ExtractMarkdownFlow',\n",
       "  'ExtractPDFFlow',\n",
       "  'ExtractTxtFlow'],\n",
       " 'transform': ['TransformAzureOpenAIFlow',\n",
       "  'TransformCopyFlow',\n",
       "  'TransformHuggingFaceFlow',\n",
       "  'TransformLMQGFlow',\n",
       "  'TransformOpenAIFlow'],\n",
       " 'rater': ['RaterFlow']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlowFactory.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file = \"22.11_information-theory.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [{\"filename\": input_file}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_client = ExtractClient(ExtractHTMLConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_output = extract_client.run(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output': [{'text': ['Quick search',\n",
       "     'Show Source',\n",
       "     'Table Of Contents',\n",
       "     '1. Introduction\\n2. Preliminaries\\n2.1. Data Manipulation\\n2.2. Data Preprocessing\\n2.3. Linear Algebra\\n2.4. Calculus\\n2.5. Automatic Differentiation\\n2.6. Probability and Statistics\\n2.7. Documentation\\n3. Linear Neural Networks for Regression\\n3.1. Linear Regression\\n3.2. Object-Oriented Design for Implementation\\n3.3. Synthetic Regression Data\\n3.4. Linear Regression Implementation from Scratch\\n3.5. Concise Implementation of Linear Regression\\n3.6. Generalization\\n3.7. Weight Decay\\n4. Linear Neural Networks for Classification\\n4.1. Softmax Regression\\n4.2. The Image Classification Dataset\\n4.3. The Base Classification Model\\n4.4. Softmax Regression Implementation from Scratch\\n4.5. Concise Implementation of Softmax Regression\\n4.6. Generalization in Classification\\n4.7. Environment and Distribution Shift\\n5. Multilayer Perceptrons\\n5.1. Multilayer Perceptrons\\n5.2. Implementation of Multilayer Perceptrons\\n5.3. Forward Propagation, Backward Propagation, and Computational Graphs\\n5.4. Numerical Stability and Initialization\\n5.5. Generalization in Deep Learning\\n5.6. Dropout\\n5.7. Predicting House Prices on Kaggle\\n6. Builders’ Guide\\n6.1. Layers and Modules\\n6.2. Parameter Management\\n6.3. Parameter Initialization\\n6.4. Lazy Initialization\\n6.5. Custom Layers\\n6.6. File I/O\\n6.7. GPUs\\n7. Convolutional Neural Networks\\n7.1. From Fully Connected Layers to Convolutions\\n7.2. Convolutions for Images\\n7.3. Padding and Stride\\n7.4. Multiple Input and Multiple Output Channels\\n7.5. Pooling\\n7.6. Convolutional Neural Networks (LeNet)\\n8. Modern Convolutional Neural Networks\\n8.1. Deep Convolutional Neural Networks (AlexNet)\\n8.2. Networks Using Blocks (VGG)\\n8.3. Network in Network (NiN)\\n8.4. Multi-Branch Networks (GoogLeNet)\\n8.5. Batch Normalization\\n8.6. Residual Networks (ResNet) and ResNeXt\\n8.7. Densely Connected Networks (DenseNet)\\n8.8. Designing Convolution Network Architectures\\n9. Recurrent Neural Networks\\n9.1. Working with Sequences\\n9.2. Converting Raw Text into Sequence Data\\n9.3. Language Models\\n9.4. Recurrent Neural Networks\\n9.5. Recurrent Neural Network Implementation from Scratch\\n9.6. Concise Implementation of Recurrent Neural Networks\\n9.7. Backpropagation Through Time\\n10. Modern Recurrent Neural Networks\\n10.1. Long Short-Term Memory (LSTM)\\n10.2. Gated Recurrent Units (GRU)\\n10.3. Deep Recurrent Neural Networks\\n10.4. Bidirectional Recurrent Neural Networks\\n10.5. Machine Translation and the Dataset\\n10.6. The Encoder–Decoder Architecture\\n10.7. Sequence-to-Sequence Learning for Machine Translation\\n10.8. Beam Search\\n11. Attention Mechanisms and Transformers\\n11.1. Queries, Keys, and Values\\n11.2. Attention Pooling by Similarity\\n11.3. Attention Scoring Functions\\n11.4. The Bahdanau Attention Mechanism\\n11.5. Multi-Head Attention\\n11.6. Self-Attention and Positional Encoding\\n11.7. The Transformer Architecture\\n11.8. Transformers for Vision\\n11.9. Large-Scale Pretraining with Transformers\\n12. Optimization Algorithms\\n12.1. Optimization and Deep Learning\\n12.2. Convexity\\n12.3. Gradient Descent\\n12.4. Stochastic Gradient Descent\\n12.5. Minibatch Stochastic Gradient Descent\\n12.6. Momentum\\n12.7. Adagrad\\n12.8. RMSProp\\n12.9. Adadelta\\n12.10. Adam\\n12.11. Learning Rate Scheduling\\n13. Computational Performance\\n13.1. Compilers and Interpreters\\n13.2. Asynchronous Computation\\n13.3. Automatic Parallelism\\n13.4. Hardware\\n13.5. Training on Multiple GPUs\\n13.6. Concise Implementation for Multiple GPUs\\n13.7. Parameter Servers\\n14. Computer Vision\\n14.1. Image Augmentation\\n14.2. Fine-Tuning\\n14.3. Object Detection and Bounding Boxes\\n14.4. Anchor Boxes\\n14.5. Multiscale Object Detection\\n14.6. The Object Detection Dataset\\n14.7. Single Shot Multibox Detection\\n14.8. Region-based CNNs (R-CNNs)\\n14.9. Semantic Segmentation and the Dataset\\n14.10. Transposed Convolution\\n14.11. Fully Convolutional Networks\\n14.12. Neural Style Transfer\\n14.13. Image Classification (CIFAR-10) on Kaggle\\n14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle\\n15. Natural Language Processing: Pretraining\\n15.1. Word Embedding (word2vec)\\n15.2. Approximate Training\\n15.3. The Dataset for Pretraining Word Embeddings\\n15.4. Pretraining word2vec\\n15.5. Word Embedding with Global Vectors (GloVe)\\n15.6. Subword Embedding\\n15.7. Word Similarity and Analogy\\n15.8. Bidirectional Encoder Representations from Transformers (BERT)\\n15.9. The Dataset for Pretraining BERT\\n15.10. Pretraining BERT\\n16. Natural Language Processing: Applications\\n16.1. Sentiment Analysis and the Dataset\\n16.2. Sentiment Analysis: Using Recurrent Neural Networks\\n16.3. Sentiment Analysis: Using Convolutional Neural Networks\\n16.4. Natural Language Inference and the Dataset\\n16.5. Natural Language Inference: Using Attention\\n16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications\\n16.7. Natural Language Inference: Fine-Tuning BERT\\n17. Reinforcement Learning\\n17.1. Markov Decision Process (MDP)\\n17.2. Value Iteration\\n17.3. Q-Learning\\n18. Gaussian Processes\\n18.1. Introduction to Gaussian Processes\\n18.2. Gaussian Process Priors\\n18.3. Gaussian Process Inference\\n19. Hyperparameter Optimization\\n19.1. What Is Hyperparameter Optimization?\\n19.2. Hyperparameter Optimization API\\n19.3. Asynchronous Random Search\\n19.4. Multi-Fidelity Hyperparameter Optimization\\n19.5. Asynchronous Successive Halving\\n20. Generative Adversarial Networks\\n20.1. Generative Adversarial Networks\\n20.2. Deep Convolutional Generative Adversarial Networks\\n21. Recommender Systems\\n21.1. Overview of Recommender Systems\\n21.2. The MovieLens Dataset\\n21.3. Matrix Factorization\\n21.4. AutoRec: Rating Prediction with Autoencoders\\n21.5. Personalized Ranking for Recommender Systems\\n21.6. Neural Collaborative Filtering for Personalized Ranking\\n21.7. Sequence-Aware Recommender Systems\\n21.8. Feature-Rich Recommender Systems\\n21.9. Factorization Machines\\n21.10. Deep Factorization Machines\\n22. Appendix: Mathematics for Deep Learning\\n22.1. Geometry and Linear Algebraic Operations\\n22.2. Eigendecompositions\\n22.3. Single Variable Calculus\\n22.4. Multivariable Calculus\\n22.5. Integral Calculus\\n22.6. Random Variables\\n22.7. Maximum Likelihood\\n22.8. Distributions\\n22.9. Naive Bayes\\n22.10. Statistics\\n22.11. Information Theory\\n23. Appendix: Tools for Deep Learning\\n23.1. Using Jupyter Notebooks\\n23.2. Using Amazon SageMaker\\n23.3. Using AWS EC2 Instances\\n23.4. Using Google Colab\\n23.5. Selecting Servers and GPUs\\n23.6. Contributing to This Book\\n23.7. Utility Functions and Classes\\n23.8. The\\nd2l\\nAPI Document',\n",
       "     'Table Of Contents',\n",
       "     '1. Introduction\\n2. Preliminaries\\n2.1. Data Manipulation\\n2.2. Data Preprocessing\\n2.3. Linear Algebra\\n2.4. Calculus\\n2.5. Automatic Differentiation\\n2.6. Probability and Statistics\\n2.7. Documentation\\n3. Linear Neural Networks for Regression\\n3.1. Linear Regression\\n3.2. Object-Oriented Design for Implementation\\n3.3. Synthetic Regression Data\\n3.4. Linear Regression Implementation from Scratch\\n3.5. Concise Implementation of Linear Regression\\n3.6. Generalization\\n3.7. Weight Decay\\n4. Linear Neural Networks for Classification\\n4.1. Softmax Regression\\n4.2. The Image Classification Dataset\\n4.3. The Base Classification Model\\n4.4. Softmax Regression Implementation from Scratch\\n4.5. Concise Implementation of Softmax Regression\\n4.6. Generalization in Classification\\n4.7. Environment and Distribution Shift\\n5. Multilayer Perceptrons\\n5.1. Multilayer Perceptrons\\n5.2. Implementation of Multilayer Perceptrons\\n5.3. Forward Propagation, Backward Propagation, and Computational Graphs\\n5.4. Numerical Stability and Initialization\\n5.5. Generalization in Deep Learning\\n5.6. Dropout\\n5.7. Predicting House Prices on Kaggle\\n6. Builders’ Guide\\n6.1. Layers and Modules\\n6.2. Parameter Management\\n6.3. Parameter Initialization\\n6.4. Lazy Initialization\\n6.5. Custom Layers\\n6.6. File I/O\\n6.7. GPUs\\n7. Convolutional Neural Networks\\n7.1. From Fully Connected Layers to Convolutions\\n7.2. Convolutions for Images\\n7.3. Padding and Stride\\n7.4. Multiple Input and Multiple Output Channels\\n7.5. Pooling\\n7.6. Convolutional Neural Networks (LeNet)\\n8. Modern Convolutional Neural Networks\\n8.1. Deep Convolutional Neural Networks (AlexNet)\\n8.2. Networks Using Blocks (VGG)\\n8.3. Network in Network (NiN)\\n8.4. Multi-Branch Networks (GoogLeNet)\\n8.5. Batch Normalization\\n8.6. Residual Networks (ResNet) and ResNeXt\\n8.7. Densely Connected Networks (DenseNet)\\n8.8. Designing Convolution Network Architectures\\n9. Recurrent Neural Networks\\n9.1. Working with Sequences\\n9.2. Converting Raw Text into Sequence Data\\n9.3. Language Models\\n9.4. Recurrent Neural Networks\\n9.5. Recurrent Neural Network Implementation from Scratch\\n9.6. Concise Implementation of Recurrent Neural Networks\\n9.7. Backpropagation Through Time\\n10. Modern Recurrent Neural Networks\\n10.1. Long Short-Term Memory (LSTM)\\n10.2. Gated Recurrent Units (GRU)\\n10.3. Deep Recurrent Neural Networks\\n10.4. Bidirectional Recurrent Neural Networks\\n10.5. Machine Translation and the Dataset\\n10.6. The Encoder–Decoder Architecture\\n10.7. Sequence-to-Sequence Learning for Machine Translation\\n10.8. Beam Search\\n11. Attention Mechanisms and Transformers\\n11.1. Queries, Keys, and Values\\n11.2. Attention Pooling by Similarity\\n11.3. Attention Scoring Functions\\n11.4. The Bahdanau Attention Mechanism\\n11.5. Multi-Head Attention\\n11.6. Self-Attention and Positional Encoding\\n11.7. The Transformer Architecture\\n11.8. Transformers for Vision\\n11.9. Large-Scale Pretraining with Transformers\\n12. Optimization Algorithms\\n12.1. Optimization and Deep Learning\\n12.2. Convexity\\n12.3. Gradient Descent\\n12.4. Stochastic Gradient Descent\\n12.5. Minibatch Stochastic Gradient Descent\\n12.6. Momentum\\n12.7. Adagrad\\n12.8. RMSProp\\n12.9. Adadelta\\n12.10. Adam\\n12.11. Learning Rate Scheduling\\n13. Computational Performance\\n13.1. Compilers and Interpreters\\n13.2. Asynchronous Computation\\n13.3. Automatic Parallelism\\n13.4. Hardware\\n13.5. Training on Multiple GPUs\\n13.6. Concise Implementation for Multiple GPUs\\n13.7. Parameter Servers\\n14. Computer Vision\\n14.1. Image Augmentation\\n14.2. Fine-Tuning\\n14.3. Object Detection and Bounding Boxes\\n14.4. Anchor Boxes\\n14.5. Multiscale Object Detection\\n14.6. The Object Detection Dataset\\n14.7. Single Shot Multibox Detection\\n14.8. Region-based CNNs (R-CNNs)\\n14.9. Semantic Segmentation and the Dataset\\n14.10. Transposed Convolution\\n14.11. Fully Convolutional Networks\\n14.12. Neural Style Transfer\\n14.13. Image Classification (CIFAR-10) on Kaggle\\n14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle\\n15. Natural Language Processing: Pretraining\\n15.1. Word Embedding (word2vec)\\n15.2. Approximate Training\\n15.3. The Dataset for Pretraining Word Embeddings\\n15.4. Pretraining word2vec\\n15.5. Word Embedding with Global Vectors (GloVe)\\n15.6. Subword Embedding\\n15.7. Word Similarity and Analogy\\n15.8. Bidirectional Encoder Representations from Transformers (BERT)\\n15.9. The Dataset for Pretraining BERT\\n15.10. Pretraining BERT\\n16. Natural Language Processing: Applications\\n16.1. Sentiment Analysis and the Dataset\\n16.2. Sentiment Analysis: Using Recurrent Neural Networks\\n16.3. Sentiment Analysis: Using Convolutional Neural Networks\\n16.4. Natural Language Inference and the Dataset\\n16.5. Natural Language Inference: Using Attention\\n16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications\\n16.7. Natural Language Inference: Fine-Tuning BERT\\n17. Reinforcement Learning\\n17.1. Markov Decision Process (MDP)\\n17.2. Value Iteration\\n17.3. Q-Learning\\n18. Gaussian Processes\\n18.1. Introduction to Gaussian Processes\\n18.2. Gaussian Process Priors\\n18.3. Gaussian Process Inference\\n19. Hyperparameter Optimization\\n19.1. What Is Hyperparameter Optimization?\\n19.2. Hyperparameter Optimization API\\n19.3. Asynchronous Random Search\\n19.4. Multi-Fidelity Hyperparameter Optimization\\n19.5. Asynchronous Successive Halving\\n20. Generative Adversarial Networks\\n20.1. Generative Adversarial Networks\\n20.2. Deep Convolutional Generative Adversarial Networks\\n21. Recommender Systems\\n21.1. Overview of Recommender Systems\\n21.2. The MovieLens Dataset\\n21.3. Matrix Factorization\\n21.4. AutoRec: Rating Prediction with Autoencoders\\n21.5. Personalized Ranking for Recommender Systems\\n21.6. Neural Collaborative Filtering for Personalized Ranking\\n21.7. Sequence-Aware Recommender Systems\\n21.8. Feature-Rich Recommender Systems\\n21.9. Factorization Machines\\n21.10. Deep Factorization Machines\\n22. Appendix: Mathematics for Deep Learning\\n22.1. Geometry and Linear Algebraic Operations\\n22.2. Eigendecompositions\\n22.3. Single Variable Calculus\\n22.4. Multivariable Calculus\\n22.5. Integral Calculus\\n22.6. Random Variables\\n22.7. Maximum Likelihood\\n22.8. Distributions\\n22.9. Naive Bayes\\n22.10. Statistics\\n22.11. Information Theory\\n23. Appendix: Tools for Deep Learning\\n23.1. Using Jupyter Notebooks\\n23.2. Using Amazon SageMaker\\n23.3. Using AWS EC2 Instances\\n23.4. Using Google Colab\\n23.5. Selecting Servers and GPUs\\n23.6. Contributing to This Book\\n23.7. Utility Functions and Classes\\n23.8. The\\nd2l\\nAPI Document',\n",
       "     'Open the notebook in Colab',\n",
       "     'Open the notebook in Colab',\n",
       "     'Open the notebook in Colab',\n",
       "     'Open the notebook in Colab',\n",
       "     'Open the notebook in SageMaker Studio Lab',\n",
       "     'The universe is overflowing with information. Information provides a\\ncommon language across disciplinary rifts: from Shakespeare’s Sonnet to\\nresearchers’ paper on Cornell ArXiv, from Van Gogh’s printing Starry\\nNight to Beethoven’s music Symphony No.\\xa05, from the first programming\\nlanguage Plankalkül to the state-of-the-art machine learning algorithms.\\nEverything must follow the rules of information theory, no matter the\\nformat. With information theory, we can measure and compare how much\\ninformation is present in different signals. In this section, we will\\ninvestigate the fundamental concepts of information theory and\\napplications of information theory in machine learning.',\n",
       "     'Section 4.1',\n",
       "     'Section 4.1',\n",
       "     'Consider the following thought experiment. We have a friend with a deck\\nof cards. They will shuffle the deck, flip over some cards, and tell us\\nstatements about the cards. We will try to assess the information\\ncontent of each statement.',\n",
       "     'First, they flip over a card and tell us, “I see a card.” This provides\\nus with no information at all. We were already certain that this was the\\ncase so we hope the information should be zero.',\n",
       "     '\\\\(0\\\\textrm{ bits}\\\\)',\n",
       "     '\\\\(2\\\\textrm{ bits}\\\\)',\n",
       "     '\\\\(~5.7\\\\textrm{ bits}\\\\)',\n",
       "     '\\\\(~225.6\\\\textrm{ bits}\\\\)',\n",
       "     'If we read through these thought experiments, we see a natural idea. As\\na starting point, rather than caring about the knowledge, we may build\\noff the idea that information represents the degree of surprise or the\\nabstract possibility of the event. For example, if we want to describe\\nan unusual event, we need a lot information. For a common event, we may\\nnot need much information.',\n",
       "     'Shannon, 1948',\n",
       "     'We can calculate self information as shown below. Before that, let’s\\nfirst import all the necessary packages in this section.',\n",
       "     '# Define nansum, as pytorch does not offer it inbuilt.',\n",
       "     'As self-information only measures the information of a single discrete\\nevent, we need a more generalized measure for any random variable of\\neither discrete or continuous distribution.',\n",
       "     'The information we gain by observing a random variable does not\\ndepend on what we call the elements, or the presence of additional\\nelements which have probability zero.\\nThe information we gain by observing two random variables is no more\\nthan the sum of the information we gain by observing them separately.\\nIf they are independent, then it is exactly the sum.\\nThe information gained when observing (nearly) certain events is\\n(nearly) zero.',\n",
       "     'While proving this fact is beyond the scope of our text, it is important\\nto know that this uniquely determines the form that entropy must take.\\nThe only ambiguity that these allow is in the choice of fundamental\\nunits, which is most often normalized by making the choice we saw before\\nthat the information provided by a single fair coin flip is one bit.',\n",
       "     'We can define entropy as below.',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '\\\\(p(x) = f_1(x) f_2(x) \\\\ldots, f_n(x)\\\\)',\n",
       "     '\\\\([0, 1]\\\\)',\n",
       "     '\\\\({s_1, \\\\ldots, s_k}\\\\)',\n",
       "     '\\\\({p_1, \\\\ldots, p_k}\\\\)',\n",
       "     '\\\\(H(X) \\\\geq 0\\\\)\\nfor all discrete\\n\\\\(X\\\\)\\n(entropy can be\\nnegative for continuous\\n\\\\(X\\\\)\\n).\\nIf\\n\\\\(X \\\\sim P\\\\)\\nwith a p.d.f. or a p.m.f.\\n\\\\(p(x)\\\\)\\n, and we\\ntry to estimate\\n\\\\(P\\\\)\\nby a new probability distribution\\n\\\\(Q\\\\)\\nwith a p.d.f. or a p.m.f.\\n\\\\(q(x)\\\\)\\n, then\\n(22.11.7)\\n¶\\n\\\\[H(X) = - E_{x \\\\sim P} [\\\\log p(x)] \\\\leq  - E_{x \\\\sim P} [\\\\log q(x)], \\\\textrm{ with equality if and only if } P = Q.\\\\]\\nAlternatively,\\n\\\\(H(X)\\\\)\\ngives a lower bound of the average\\nnumber of bits needed to encode symbols drawn from\\n\\\\(P\\\\)\\n.\\nIf\\n\\\\(X \\\\sim P\\\\)\\n, then\\n\\\\(x\\\\)\\nconveys the maximum amount of\\ninformation if it spreads evenly among all possible outcomes.\\nSpecifically, if the probability distribution\\n\\\\(P\\\\)\\nis discrete\\nwith\\n\\\\(k\\\\)\\n-class\\n\\\\(\\\\{p_1, \\\\ldots, p_k \\\\}\\\\)\\n, then\\n(22.11.8)\\n¶\\n\\\\[H(X) \\\\leq \\\\log(k), \\\\textrm{ with equality if and only if } p_i = \\\\frac{1}{k}, \\\\forall i.\\\\]\\nIf\\n\\\\(P\\\\)\\nis a continuous random variable, then the story\\nbecomes much more complicated. However, if we additionally impose\\nthat\\n\\\\(P\\\\)\\nis supported on a finite interval (with all values\\nbetween\\n\\\\(0\\\\)\\nand\\n\\\\(1\\\\)\\n), then\\n\\\\(P\\\\)\\nhas the highest\\nentropy if it is the uniform distribution on that interval.',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\(p_{X, Y}(x, y)\\\\)',\n",
       "     '\\\\(H(X, Y)\\\\)',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\(X = Y\\\\)',\n",
       "     '\\\\(H(X, Y) = H(X) = H(Y)\\\\)',\n",
       "     '\\\\(H(X, Y) = H(X) + H(Y)\\\\)',\n",
       "     'Let’s implement joint entropy from scratch.',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '\\\\(H(Y \\\\mid X)\\\\)',\n",
       "     '\\\\(p(y \\\\mid x) = \\\\frac{p_{X, Y}(x, y)}{p_X(x)}\\\\)',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\(H(Y \\\\mid X)\\\\)',\n",
       "     '\\\\(H(X, Y)\\\\)',\n",
       "     '\\\\(H(Y \\\\mid X)\\\\)',\n",
       "     '\\\\(H(X, Y)\\\\)',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\((X, Y)\\\\)',\n",
       "     '\\\\(I(X, Y)\\\\)',\n",
       "     '\\\\(H(X, Y)\\\\)',\n",
       "     '\\\\(H(X \\\\mid Y)\\\\)',\n",
       "     '\\\\(H(Y \\\\mid X)\\\\)',\n",
       "     'Indeed, this is a valid definition for the mutual information. If we\\nexpand out the definitions of these terms and combine them, a little\\nalgebra shows that this is the same as',\n",
       "     'Fig. 22.11.1',\n",
       "     'Fig. 22.11.1',\n",
       "     '\\\\(I(X, Y)\\\\)',\n",
       "     '\\\\(H(X) - H(X \\\\mid Y)\\\\)\\n\\\\(H(Y) - H(Y \\\\mid X)\\\\)\\n\\\\(H(X) + H(Y) - H(X, Y)\\\\)',\n",
       "     'Fig. 22.11.1',\n",
       "     'Mutual information’s relationship with joint entropy and conditional\\nentropy.',\n",
       "     'Section 22.6',\n",
       "     'Section 22.6',\n",
       "     'Now, let’s implement mutual information from scratch.',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     '# Operator `nansum` will sum up the non-nan number',\n",
       "     'Mutual information is symmetric, i.e.,\\n\\\\(I(X, Y) = I(Y, X)\\\\)\\n.\\nMutual information is non-negative, i.e.,\\n\\\\(I(X, Y) \\\\geq 0\\\\)\\n.\\n\\\\(I(X, Y) = 0\\\\)\\nif and only if\\n\\\\(X\\\\)\\nand\\n\\\\(Y\\\\)\\nare\\nindependent. For example, if\\n\\\\(X\\\\)\\nand\\n\\\\(Y\\\\)\\nare independent,\\nthen knowing\\n\\\\(Y\\\\)\\ndoes not give any information about\\n\\\\(X\\\\)\\nand vice versa, so their mutual information is zero.\\nAlternatively, if\\n\\\\(X\\\\)\\nis an invertible function of\\n\\\\(Y\\\\)\\n,\\nthen\\n\\\\(Y\\\\)\\nand\\n\\\\(X\\\\)\\nshare all information and\\n(22.11.19)\\n¶\\n\\\\[I(X, Y) = H(Y) = H(X).\\\\]',\n",
       "     '\\\\(p_X(x) p_Y(y)\\\\)',\n",
       "     'In this case, mutual information can help us resolve this ambiguity. We\\nfirst find the group of words that each has a relatively large mutual\\ninformation with the company Amazon, such as e-commerce, technology, and\\nonline. Second, we find another group of words that each has a\\nrelatively large mutual information with the Amazon rain forest, such as\\nrain, forest, and tropical. When we need to disambiguate “Amazon”, we\\ncan compare which group has more occurrence in the context of the word\\nAmazon. In this case the article would go on to describe the forest, and\\nmake the context clear.',\n",
       "     'Section 2.3',\n",
       "     'Section 2.3',\n",
       "     '\\\\(-\\\\log \\\\frac{q(x)}{p(x)} = -\\\\log(q(x)) - (-\\\\log(p(x)))\\\\)',\n",
       "     'Let’s implement the KL divergence from Scratch.',\n",
       "     'KL divergence is non-symmetric, i.e., there are\\n\\\\(P,Q\\\\)\\nsuch that\\n(22.11.22)\\n¶\\n\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\neq D_{\\\\textrm{KL}}(Q\\\\|P).\\\\]\\nKL divergence is non-negative, i.e.,\\n(22.11.23)\\n¶\\n\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\geq 0.\\\\]\\nNote that the equality holds only when\\n\\\\(P = Q\\\\)\\n.\\nIf there exists an\\n\\\\(x\\\\)\\nsuch that\\n\\\\(p(x) > 0\\\\)\\nand\\n\\\\(q(x) = 0\\\\)\\n, then\\n\\\\(D_{\\\\textrm{KL}}(P\\\\|Q) = \\\\infty\\\\)\\n.\\nThere is a close relationship between KL divergence and mutual\\ninformation. Besides the relationship shown in\\nFig. 22.11.1\\n,\\n\\\\(I(X, Y)\\\\)\\nis also\\nnumerically equivalent with the following terms:\\n\\\\(D_{\\\\textrm{KL}}(P(X, Y) \\\\ \\\\| \\\\ P(X)P(Y))\\\\)\\n;\\n\\\\(E_Y \\\\{ D_{\\\\textrm{KL}}(P(X \\\\mid Y) \\\\ \\\\| \\\\ P(X)) \\\\}\\\\)\\n;\\n\\\\(E_X \\\\{ D_{\\\\textrm{KL}}(P(Y \\\\mid X) \\\\ \\\\| \\\\ P(Y)) \\\\}\\\\)\\n.\\nFor the first term, we interpret mutual information as the KL\\ndivergence between\\n\\\\(P(X, Y)\\\\)\\nand the product of\\n\\\\(P(X)\\\\)\\nand\\n\\\\(P(Y)\\\\)\\n, and thus is a measure of how different the joint\\ndistribution is from the distribution if they were independent. For\\nthe second term, mutual information tells us the average reduction in\\nuncertainty about\\n\\\\(Y\\\\)\\nthat results from learning the value of\\nthe\\n\\\\(X\\\\)\\n’s distribution. Similarly to the third term.',\n",
       "     'Let’s go through a toy example to see the non-symmetry explicitly.',\n",
       "     '\\\\(N(0, 1)\\\\)',\n",
       "     '\\\\(N(-1, 1)\\\\)',\n",
       "     '\\\\(N(1, 1)\\\\)',\n",
       "     '\\\\(D_{\\\\textrm{KL}}(q_2 \\\\|p)\\\\)',\n",
       "     '\\\\(D_{\\\\textrm{KL}}(p \\\\| q_2)\\\\)',\n",
       "     '\\\\(x_1, \\\\ldots, x_n\\\\)',\n",
       "     '\\\\(\\\\hat{y}_i= p_{\\\\theta}(y_i \\\\mid x_i)\\\\)',\n",
       "     'Section 22.7',\n",
       "     'Section 22.7',\n",
       "     '\\\\(\\\\hat{y}_i= p_{\\\\theta}(y_i \\\\mid x_i)\\\\)',\n",
       "     '\\\\(\\\\pi_i= p_{\\\\theta}(y_i = 1 \\\\mid x_i)\\\\)',\n",
       "     '\\\\(- l(\\\\theta)\\\\)',\n",
       "     '\\\\(\\\\textrm{CE}(y, \\\\hat{y})\\\\)',\n",
       "     'We can implement the cross-entropy loss as below.',\n",
       "     '# `tf.gather_nd` is used to select specific indices of a tensor.',\n",
       "     'Now define two tensors for the labels and predictions, and calculate the\\ncross-entropy loss of them.',\n",
       "     'Maximizing predictive probability of\\n\\\\(Q\\\\)\\nfor distribution\\n\\\\(P\\\\)\\n, (i.e.,\\n\\\\(E_{x \\\\sim P} [\\\\log (q(x))]\\\\)\\n);\\nMinimizing cross-entropy\\n\\\\(\\\\textrm{CE} (P, Q)\\\\)\\n;\\nMinimizing the KL divergence\\n\\\\(D_{\\\\textrm{KL}}(P\\\\|Q)\\\\)\\n.',\n",
       "     '\\\\(\\\\mathbf{y}_i = (y_{i1}, \\\\ldots, y_{ik})\\\\)',\n",
       "     '\\\\(A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)\\\\)',\n",
       "     '\\\\(\\\\mathbf{z} = (z_{1}, \\\\ldots, z_{k})\\\\)',\n",
       "     '\\\\(\\\\mathbf{p} =\\\\)',\n",
       "     '\\\\(p_{1}, \\\\ldots, p_{k}\\\\)',\n",
       "     '\\\\(\\\\boldsymbol{\\\\pi} =\\\\)',\n",
       "     '\\\\(\\\\pi_{1}, \\\\ldots, \\\\pi_{k}\\\\)',\n",
       "     '\\\\(\\\\mathbf{\\\\pi}^{\\\\mathbf{y}_i} = \\\\prod_{j=1}^k \\\\pi_{j}^{y_{ij}}.\\\\)',\n",
       "     '\\\\(\\\\pi_{j} = p_{\\\\theta} (y_{ij} \\\\mid \\\\mathbf{x}_i)\\\\)',\n",
       "     '\\\\(\\\\textrm{CE}(y, \\\\hat{y})\\\\)',\n",
       "     '# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`',\n",
       "     '# and `nn.NLLLoss()`',\n",
       "     '# Convert labels to one-hot vectors.',\n",
       "     '# We will not calculate negative log-likelihood from the definition.',\n",
       "     '# Rather, we will follow a circular argument. Because NLL is same as',\n",
       "     '# `cross_entropy`, if we calculate cross_entropy that would give us NLL',\n",
       "     'Information theory is a field of study about encoding, decoding,\\ntransmitting, and manipulating information.\\nEntropy is the unit to measure how much information is presented in\\ndifferent signals.\\nKL divergence can also measure the divergence between two\\ndistributions.\\nCross-entropy can be viewed as an objective function of multi-class\\nclassification. Minimizing cross-entropy loss is equivalent to\\nmaximizing the log-likelihood function.',\n",
       "     'Verify that the card examples from the first section indeed have the\\nclaimed entropy.\\nShow that the KL divergence\\n\\\\(D(p\\\\|q)\\\\)\\nis nonnegative for all\\ndistributions\\n\\\\(p\\\\)\\nand\\n\\\\(q\\\\)\\n. Hint: use Jensen’s inequality,\\ni.e., use the fact that\\n\\\\(-\\\\log x\\\\)\\nis a convex function.\\nLet’s compute the entropy from a few data sources:\\nAssume that you are watching the output generated by a monkey at a\\ntypewriter. The monkey presses any of the\\n\\\\(44\\\\)\\nkeys of the\\ntypewriter at random (you can assume that it has not discovered\\nany special keys or the shift key yet). How many bits of\\nrandomness per character do you observe?\\nBeing unhappy with the monkey, you replaced it by a drunk\\ntypesetter. It is able to generate words, albeit not coherently.\\nInstead, it picks a random word out of a vocabulary of\\n\\\\(2,000\\\\)\\nwords. Let’s assume that the average length of a\\nword is\\n\\\\(4.5\\\\)\\nletters in English. How many bits of\\nrandomness per character do you observe now?\\nStill being unhappy with the result, you replace the typesetter by\\na high quality language model. The language model can currently\\nobtain a perplexity as low as\\n\\\\(15\\\\)\\npoints per word. The\\ncharacter\\nperplexity\\nof a language model is defined as the\\ninverse of the geometric mean of a set of probabilities, each\\nprobability is corresponding to a character in the word. To be\\nspecific, if the length of a given word is\\n\\\\(l\\\\)\\n, then\\n\\\\(\\\\textrm{PPL}(\\\\textrm{word}) = \\\\left[\\\\prod_i p(\\\\textrm{character}_i)\\\\right]^{ -\\\\frac{1}{l}} = \\\\exp \\\\left[ - \\\\frac{1}{l} \\\\sum_i{\\\\log p(\\\\textrm{character}_i)} \\\\right].\\\\)\\nAssume that the test word has 4.5 letters, how many bits of\\nrandomness per character do you observe now?\\nExplain intuitively why\\n\\\\(I(X, Y) = H(X) - H(X \\\\mid Y)\\\\)\\n. Then,\\nshow this is true by expressing both sides as an expectation with\\nrespect to the joint distribution.\\nWhat is the KL Divergence between the two Gaussian distributions\\n\\\\(\\\\mathcal{N}(\\\\mu_1, \\\\sigma_1^2)\\\\)\\nand\\n\\\\(\\\\mathcal{N}(\\\\mu_2, \\\\sigma_2^2)\\\\)\\n?',\n",
       "     'Table Of Contents',\n",
       "     '22.11. Information Theory\\n22.11.1. Information\\n22.11.1.1. Self-information\\n22.11.2. Entropy\\n22.11.2.1. Motivating Entropy\\n22.11.2.2. Definition\\n22.11.2.3. Interpretations\\n22.11.2.4. Properties of Entropy\\n22.11.3. Mutual Information\\n22.11.3.1. Joint Entropy\\n22.11.3.2. Conditional Entropy\\n22.11.3.3. Mutual Information\\n22.11.3.4. Properties of Mutual Information\\n22.11.3.5. Pointwise Mutual Information\\n22.11.3.6. Applications of Mutual Information\\n22.11.4. Kullback–Leibler Divergence\\n22.11.4.1. Definition\\n22.11.4.2. KL Divergence Properties\\n22.11.4.3. Example\\n22.11.5. Cross-Entropy\\n22.11.5.1. Formal Definition\\n22.11.5.2. Properties\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\n22.11.6. Summary\\n22.11.7. Exercises',\n",
       "     '22.10. Statistics',\n",
       "     '23. Appendix: Tools for Deep Learning']}],\n",
       "  'root': <uniflow.node.Node at 0x7ff3430bb8e0>}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "        instruction=\"Generate one question and its corresponding answer based on context. Following the format of the examples below to include the same context, question, and answer in the response.\",\n",
    "        few_shot_prompt=[\n",
    "            Context(\n",
    "                context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "                question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "                answer=\"Claude E. Shannon.\",\n",
    "            )\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(context='1. Introduction\\n2. Preliminaries\\n2.1. Data Manipulation\\n2.2. Data Preprocessing\\n2.3. Linear Algebra\\n2.4. Calculus\\n2.5. Automatic Differentiation\\n2.6. Probability and Statistics\\n2.7. Documentation\\n3. Linear Neural Networks for Regression\\n3.1. Linear Regression\\n3.2. Object-Oriented Design for Implementation\\n3.3. Synthetic Regression Data\\n3.4. Linear Regression Implementation from Scratch\\n3.5. Concise Implementation of Linear Regression\\n3.6. Generalization\\n3.7. Weight Decay\\n4. Linear Neural Networks for Classification\\n4.1. Softmax Regression\\n4.2. The Image Classification Dataset\\n4.3. The Base Classification Model\\n4.4. Softmax Regression Implementation from Scratch\\n4.5. Concise Implementation of Softmax Regression\\n4.6. Generalization in Classification\\n4.7. Environment and Distribution Shift\\n5. Multilayer Perceptrons\\n5.1. Multilayer Perceptrons\\n5.2. Implementation of Multilayer Perceptrons\\n5.3. Forward Propagation, Backward Propagation, and Computational Graphs\\n5.4. Numerical Stability and Initialization\\n5.5. Generalization in Deep Learning\\n5.6. Dropout\\n5.7. Predicting House Prices on Kaggle\\n6. Builders’ Guide\\n6.1. Layers and Modules\\n6.2. Parameter Management\\n6.3. Parameter Initialization\\n6.4. Lazy Initialization\\n6.5. Custom Layers\\n6.6. File I/O\\n6.7. GPUs\\n7. Convolutional Neural Networks\\n7.1. From Fully Connected Layers to Convolutions\\n7.2. Convolutions for Images\\n7.3. Padding and Stride\\n7.4. Multiple Input and Multiple Output Channels\\n7.5. Pooling\\n7.6. Convolutional Neural Networks (LeNet)\\n8. Modern Convolutional Neural Networks\\n8.1. Deep Convolutional Neural Networks (AlexNet)\\n8.2. Networks Using Blocks (VGG)\\n8.3. Network in Network (NiN)\\n8.4. Multi-Branch Networks (GoogLeNet)\\n8.5. Batch Normalization\\n8.6. Residual Networks (ResNet) and ResNeXt\\n8.7. Densely Connected Networks (DenseNet)\\n8.8. Designing Convolution Network Architectures\\n9. Recurrent Neural Networks\\n9.1. Working with Sequences\\n9.2. Converting Raw Text into Sequence Data\\n9.3. Language Models\\n9.4. Recurrent Neural Networks\\n9.5. Recurrent Neural Network Implementation from Scratch\\n9.6. Concise Implementation of Recurrent Neural Networks\\n9.7. Backpropagation Through Time\\n10. Modern Recurrent Neural Networks\\n10.1. Long Short-Term Memory (LSTM)\\n10.2. Gated Recurrent Units (GRU)\\n10.3. Deep Recurrent Neural Networks\\n10.4. Bidirectional Recurrent Neural Networks\\n10.5. Machine Translation and the Dataset\\n10.6. The Encoder–Decoder Architecture\\n10.7. Sequence-to-Sequence Learning for Machine Translation\\n10.8. Beam Search\\n11. Attention Mechanisms and Transformers\\n11.1. Queries, Keys, and Values\\n11.2. Attention Pooling by Similarity\\n11.3. Attention Scoring Functions\\n11.4. The Bahdanau Attention Mechanism\\n11.5. Multi-Head Attention\\n11.6. Self-Attention and Positional Encoding\\n11.7. The Transformer Architecture\\n11.8. Transformers for Vision\\n11.9. Large-Scale Pretraining with Transformers\\n12. Optimization Algorithms\\n12.1. Optimization and Deep Learning\\n12.2. Convexity\\n12.3. Gradient Descent\\n12.4. Stochastic Gradient Descent\\n12.5. Minibatch Stochastic Gradient Descent\\n12.6. Momentum\\n12.7. Adagrad\\n12.8. RMSProp\\n12.9. Adadelta\\n12.10. Adam\\n12.11. Learning Rate Scheduling\\n13. Computational Performance\\n13.1. Compilers and Interpreters\\n13.2. Asynchronous Computation\\n13.3. Automatic Parallelism\\n13.4. Hardware\\n13.5. Training on Multiple GPUs\\n13.6. Concise Implementation for Multiple GPUs\\n13.7. Parameter Servers\\n14. Computer Vision\\n14.1. Image Augmentation\\n14.2. Fine-Tuning\\n14.3. Object Detection and Bounding Boxes\\n14.4. Anchor Boxes\\n14.5. Multiscale Object Detection\\n14.6. The Object Detection Dataset\\n14.7. Single Shot Multibox Detection\\n14.8. Region-based CNNs (R-CNNs)\\n14.9. Semantic Segmentation and the Dataset\\n14.10. Transposed Convolution\\n14.11. Fully Convolutional Networks\\n14.12. Neural Style Transfer\\n14.13. Image Classification (CIFAR-10) on Kaggle\\n14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle\\n15. Natural Language Processing: Pretraining\\n15.1. Word Embedding (word2vec)\\n15.2. Approximate Training\\n15.3. The Dataset for Pretraining Word Embeddings\\n15.4. Pretraining word2vec\\n15.5. Word Embedding with Global Vectors (GloVe)\\n15.6. Subword Embedding\\n15.7. Word Similarity and Analogy\\n15.8. Bidirectional Encoder Representations from Transformers (BERT)\\n15.9. The Dataset for Pretraining BERT\\n15.10. Pretraining BERT\\n16. Natural Language Processing: Applications\\n16.1. Sentiment Analysis and the Dataset\\n16.2. Sentiment Analysis: Using Recurrent Neural Networks\\n16.3. Sentiment Analysis: Using Convolutional Neural Networks\\n16.4. Natural Language Inference and the Dataset\\n16.5. Natural Language Inference: Using Attention\\n16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications\\n16.7. Natural Language Inference: Fine-Tuning BERT\\n17. Reinforcement Learning\\n17.1. Markov Decision Process (MDP)\\n17.2. Value Iteration\\n17.3. Q-Learning\\n18. Gaussian Processes\\n18.1. Introduction to Gaussian Processes\\n18.2. Gaussian Process Priors\\n18.3. Gaussian Process Inference\\n19. Hyperparameter Optimization\\n19.1. What Is Hyperparameter Optimization?\\n19.2. Hyperparameter Optimization API\\n19.3. Asynchronous Random Search\\n19.4. Multi-Fidelity Hyperparameter Optimization\\n19.5. Asynchronous Successive Halving\\n20. Generative Adversarial Networks\\n20.1. Generative Adversarial Networks\\n20.2. Deep Convolutional Generative Adversarial Networks\\n21. Recommender Systems\\n21.1. Overview of Recommender Systems\\n21.2. The MovieLens Dataset\\n21.3. Matrix Factorization\\n21.4. AutoRec: Rating Prediction with Autoencoders\\n21.5. Personalized Ranking for Recommender Systems\\n21.6. Neural Collaborative Filtering for Personalized Ranking\\n21.7. Sequence-Aware Recommender Systems\\n21.8. Feature-Rich Recommender Systems\\n21.9. Factorization Machines\\n21.10. Deep Factorization Machines\\n22. Appendix: Mathematics for Deep Learning\\n22.1. Geometry and Linear Algebraic Operations\\n22.2. Eigendecompositions\\n22.3. Single Variable Calculus\\n22.4. Multivariable Calculus\\n22.5. Integral Calculus\\n22.6. Random Variables\\n22.7. Maximum Likelihood\\n22.8. Distributions\\n22.9. Naive Bayes\\n22.10. Statistics\\n22.11. Information Theory\\n23. Appendix: Tools for Deep Learning\\n23.1. Using Jupyter Notebooks\\n23.2. Using Amazon SageMaker\\n23.3. Using AWS EC2 Instances\\n23.4. Using Google Colab\\n23.5. Selecting Servers and GPUs\\n23.6. Contributing to This Book\\n23.7. Utility Functions and Classes\\n23.8. The\\nd2l\\nAPI Document'),\n",
       " Context(context='1. Introduction\\n2. Preliminaries\\n2.1. Data Manipulation\\n2.2. Data Preprocessing\\n2.3. Linear Algebra\\n2.4. Calculus\\n2.5. Automatic Differentiation\\n2.6. Probability and Statistics\\n2.7. Documentation\\n3. Linear Neural Networks for Regression\\n3.1. Linear Regression\\n3.2. Object-Oriented Design for Implementation\\n3.3. Synthetic Regression Data\\n3.4. Linear Regression Implementation from Scratch\\n3.5. Concise Implementation of Linear Regression\\n3.6. Generalization\\n3.7. Weight Decay\\n4. Linear Neural Networks for Classification\\n4.1. Softmax Regression\\n4.2. The Image Classification Dataset\\n4.3. The Base Classification Model\\n4.4. Softmax Regression Implementation from Scratch\\n4.5. Concise Implementation of Softmax Regression\\n4.6. Generalization in Classification\\n4.7. Environment and Distribution Shift\\n5. Multilayer Perceptrons\\n5.1. Multilayer Perceptrons\\n5.2. Implementation of Multilayer Perceptrons\\n5.3. Forward Propagation, Backward Propagation, and Computational Graphs\\n5.4. Numerical Stability and Initialization\\n5.5. Generalization in Deep Learning\\n5.6. Dropout\\n5.7. Predicting House Prices on Kaggle\\n6. Builders’ Guide\\n6.1. Layers and Modules\\n6.2. Parameter Management\\n6.3. Parameter Initialization\\n6.4. Lazy Initialization\\n6.5. Custom Layers\\n6.6. File I/O\\n6.7. GPUs\\n7. Convolutional Neural Networks\\n7.1. From Fully Connected Layers to Convolutions\\n7.2. Convolutions for Images\\n7.3. Padding and Stride\\n7.4. Multiple Input and Multiple Output Channels\\n7.5. Pooling\\n7.6. Convolutional Neural Networks (LeNet)\\n8. Modern Convolutional Neural Networks\\n8.1. Deep Convolutional Neural Networks (AlexNet)\\n8.2. Networks Using Blocks (VGG)\\n8.3. Network in Network (NiN)\\n8.4. Multi-Branch Networks (GoogLeNet)\\n8.5. Batch Normalization\\n8.6. Residual Networks (ResNet) and ResNeXt\\n8.7. Densely Connected Networks (DenseNet)\\n8.8. Designing Convolution Network Architectures\\n9. Recurrent Neural Networks\\n9.1. Working with Sequences\\n9.2. Converting Raw Text into Sequence Data\\n9.3. Language Models\\n9.4. Recurrent Neural Networks\\n9.5. Recurrent Neural Network Implementation from Scratch\\n9.6. Concise Implementation of Recurrent Neural Networks\\n9.7. Backpropagation Through Time\\n10. Modern Recurrent Neural Networks\\n10.1. Long Short-Term Memory (LSTM)\\n10.2. Gated Recurrent Units (GRU)\\n10.3. Deep Recurrent Neural Networks\\n10.4. Bidirectional Recurrent Neural Networks\\n10.5. Machine Translation and the Dataset\\n10.6. The Encoder–Decoder Architecture\\n10.7. Sequence-to-Sequence Learning for Machine Translation\\n10.8. Beam Search\\n11. Attention Mechanisms and Transformers\\n11.1. Queries, Keys, and Values\\n11.2. Attention Pooling by Similarity\\n11.3. Attention Scoring Functions\\n11.4. The Bahdanau Attention Mechanism\\n11.5. Multi-Head Attention\\n11.6. Self-Attention and Positional Encoding\\n11.7. The Transformer Architecture\\n11.8. Transformers for Vision\\n11.9. Large-Scale Pretraining with Transformers\\n12. Optimization Algorithms\\n12.1. Optimization and Deep Learning\\n12.2. Convexity\\n12.3. Gradient Descent\\n12.4. Stochastic Gradient Descent\\n12.5. Minibatch Stochastic Gradient Descent\\n12.6. Momentum\\n12.7. Adagrad\\n12.8. RMSProp\\n12.9. Adadelta\\n12.10. Adam\\n12.11. Learning Rate Scheduling\\n13. Computational Performance\\n13.1. Compilers and Interpreters\\n13.2. Asynchronous Computation\\n13.3. Automatic Parallelism\\n13.4. Hardware\\n13.5. Training on Multiple GPUs\\n13.6. Concise Implementation for Multiple GPUs\\n13.7. Parameter Servers\\n14. Computer Vision\\n14.1. Image Augmentation\\n14.2. Fine-Tuning\\n14.3. Object Detection and Bounding Boxes\\n14.4. Anchor Boxes\\n14.5. Multiscale Object Detection\\n14.6. The Object Detection Dataset\\n14.7. Single Shot Multibox Detection\\n14.8. Region-based CNNs (R-CNNs)\\n14.9. Semantic Segmentation and the Dataset\\n14.10. Transposed Convolution\\n14.11. Fully Convolutional Networks\\n14.12. Neural Style Transfer\\n14.13. Image Classification (CIFAR-10) on Kaggle\\n14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle\\n15. Natural Language Processing: Pretraining\\n15.1. Word Embedding (word2vec)\\n15.2. Approximate Training\\n15.3. The Dataset for Pretraining Word Embeddings\\n15.4. Pretraining word2vec\\n15.5. Word Embedding with Global Vectors (GloVe)\\n15.6. Subword Embedding\\n15.7. Word Similarity and Analogy\\n15.8. Bidirectional Encoder Representations from Transformers (BERT)\\n15.9. The Dataset for Pretraining BERT\\n15.10. Pretraining BERT\\n16. Natural Language Processing: Applications\\n16.1. Sentiment Analysis and the Dataset\\n16.2. Sentiment Analysis: Using Recurrent Neural Networks\\n16.3. Sentiment Analysis: Using Convolutional Neural Networks\\n16.4. Natural Language Inference and the Dataset\\n16.5. Natural Language Inference: Using Attention\\n16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications\\n16.7. Natural Language Inference: Fine-Tuning BERT\\n17. Reinforcement Learning\\n17.1. Markov Decision Process (MDP)\\n17.2. Value Iteration\\n17.3. Q-Learning\\n18. Gaussian Processes\\n18.1. Introduction to Gaussian Processes\\n18.2. Gaussian Process Priors\\n18.3. Gaussian Process Inference\\n19. Hyperparameter Optimization\\n19.1. What Is Hyperparameter Optimization?\\n19.2. Hyperparameter Optimization API\\n19.3. Asynchronous Random Search\\n19.4. Multi-Fidelity Hyperparameter Optimization\\n19.5. Asynchronous Successive Halving\\n20. Generative Adversarial Networks\\n20.1. Generative Adversarial Networks\\n20.2. Deep Convolutional Generative Adversarial Networks\\n21. Recommender Systems\\n21.1. Overview of Recommender Systems\\n21.2. The MovieLens Dataset\\n21.3. Matrix Factorization\\n21.4. AutoRec: Rating Prediction with Autoencoders\\n21.5. Personalized Ranking for Recommender Systems\\n21.6. Neural Collaborative Filtering for Personalized Ranking\\n21.7. Sequence-Aware Recommender Systems\\n21.8. Feature-Rich Recommender Systems\\n21.9. Factorization Machines\\n21.10. Deep Factorization Machines\\n22. Appendix: Mathematics for Deep Learning\\n22.1. Geometry and Linear Algebraic Operations\\n22.2. Eigendecompositions\\n22.3. Single Variable Calculus\\n22.4. Multivariable Calculus\\n22.5. Integral Calculus\\n22.6. Random Variables\\n22.7. Maximum Likelihood\\n22.8. Distributions\\n22.9. Naive Bayes\\n22.10. Statistics\\n22.11. Information Theory\\n23. Appendix: Tools for Deep Learning\\n23.1. Using Jupyter Notebooks\\n23.2. Using Amazon SageMaker\\n23.3. Using AWS EC2 Instances\\n23.4. Using Google Colab\\n23.5. Selecting Servers and GPUs\\n23.6. Contributing to This Book\\n23.7. Utility Functions and Classes\\n23.8. The\\nd2l\\nAPI Document'),\n",
       " Context(context='The universe is overflowing with information. Information provides a\\ncommon language across disciplinary rifts: from Shakespeare’s Sonnet to\\nresearchers’ paper on Cornell ArXiv, from Van Gogh’s printing Starry\\nNight to Beethoven’s music Symphony No.\\xa05, from the first programming\\nlanguage Plankalkül to the state-of-the-art machine learning algorithms.\\nEverything must follow the rules of information theory, no matter the\\nformat. With information theory, we can measure and compare how much\\ninformation is present in different signals. In this section, we will\\ninvestigate the fundamental concepts of information theory and\\napplications of information theory in machine learning.'),\n",
       " Context(context='Consider the following thought experiment. We have a friend with a deck\\nof cards. They will shuffle the deck, flip over some cards, and tell us\\nstatements about the cards. We will try to assess the information\\ncontent of each statement.'),\n",
       " Context(context='If we read through these thought experiments, we see a natural idea. As\\na starting point, rather than caring about the knowledge, we may build\\noff the idea that information represents the degree of surprise or the\\nabstract possibility of the event. For example, if we want to describe\\nan unusual event, we need a lot information. For a common event, we may\\nnot need much information.'),\n",
       " Context(context='The information we gain by observing a random variable does not\\ndepend on what we call the elements, or the presence of additional\\nelements which have probability zero.\\nThe information we gain by observing two random variables is no more\\nthan the sum of the information we gain by observing them separately.\\nIf they are independent, then it is exactly the sum.\\nThe information gained when observing (nearly) certain events is\\n(nearly) zero.'),\n",
       " Context(context='While proving this fact is beyond the scope of our text, it is important\\nto know that this uniquely determines the form that entropy must take.\\nThe only ambiguity that these allow is in the choice of fundamental\\nunits, which is most often normalized by making the choice we saw before\\nthat the information provided by a single fair coin flip is one bit.'),\n",
       " Context(context='\\\\(H(X) \\\\geq 0\\\\)\\nfor all discrete\\n\\\\(X\\\\)\\n(entropy can be\\nnegative for continuous\\n\\\\(X\\\\)\\n).\\nIf\\n\\\\(X \\\\sim P\\\\)\\nwith a p.d.f. or a p.m.f.\\n\\\\(p(x)\\\\)\\n, and we\\ntry to estimate\\n\\\\(P\\\\)\\nby a new probability distribution\\n\\\\(Q\\\\)\\nwith a p.d.f. or a p.m.f.\\n\\\\(q(x)\\\\)\\n, then\\n(22.11.7)\\n¶\\n\\\\[H(X) = - E_{x \\\\sim P} [\\\\log p(x)] \\\\leq  - E_{x \\\\sim P} [\\\\log q(x)], \\\\textrm{ with equality if and only if } P = Q.\\\\]\\nAlternatively,\\n\\\\(H(X)\\\\)\\ngives a lower bound of the average\\nnumber of bits needed to encode symbols drawn from\\n\\\\(P\\\\)\\n.\\nIf\\n\\\\(X \\\\sim P\\\\)\\n, then\\n\\\\(x\\\\)\\nconveys the maximum amount of\\ninformation if it spreads evenly among all possible outcomes.\\nSpecifically, if the probability distribution\\n\\\\(P\\\\)\\nis discrete\\nwith\\n\\\\(k\\\\)\\n-class\\n\\\\(\\\\{p_1, \\\\ldots, p_k \\\\}\\\\)\\n, then\\n(22.11.8)\\n¶\\n\\\\[H(X) \\\\leq \\\\log(k), \\\\textrm{ with equality if and only if } p_i = \\\\frac{1}{k}, \\\\forall i.\\\\]\\nIf\\n\\\\(P\\\\)\\nis a continuous random variable, then the story\\nbecomes much more complicated. However, if we additionally impose\\nthat\\n\\\\(P\\\\)\\nis supported on a finite interval (with all values\\nbetween\\n\\\\(0\\\\)\\nand\\n\\\\(1\\\\)\\n), then\\n\\\\(P\\\\)\\nhas the highest\\nentropy if it is the uniform distribution on that interval.'),\n",
       " Context(context='Mutual information is symmetric, i.e.,\\n\\\\(I(X, Y) = I(Y, X)\\\\)\\n.\\nMutual information is non-negative, i.e.,\\n\\\\(I(X, Y) \\\\geq 0\\\\)\\n.\\n\\\\(I(X, Y) = 0\\\\)\\nif and only if\\n\\\\(X\\\\)\\nand\\n\\\\(Y\\\\)\\nare\\nindependent. For example, if\\n\\\\(X\\\\)\\nand\\n\\\\(Y\\\\)\\nare independent,\\nthen knowing\\n\\\\(Y\\\\)\\ndoes not give any information about\\n\\\\(X\\\\)\\nand vice versa, so their mutual information is zero.\\nAlternatively, if\\n\\\\(X\\\\)\\nis an invertible function of\\n\\\\(Y\\\\)\\n,\\nthen\\n\\\\(Y\\\\)\\nand\\n\\\\(X\\\\)\\nshare all information and\\n(22.11.19)\\n¶\\n\\\\[I(X, Y) = H(Y) = H(X).\\\\]'),\n",
       " Context(context='In this case, mutual information can help us resolve this ambiguity. We\\nfirst find the group of words that each has a relatively large mutual\\ninformation with the company Amazon, such as e-commerce, technology, and\\nonline. Second, we find another group of words that each has a\\nrelatively large mutual information with the Amazon rain forest, such as\\nrain, forest, and tropical. When we need to disambiguate “Amazon”, we\\ncan compare which group has more occurrence in the context of the word\\nAmazon. In this case the article would go on to describe the forest, and\\nmake the context clear.'),\n",
       " Context(context='KL divergence is non-symmetric, i.e., there are\\n\\\\(P,Q\\\\)\\nsuch that\\n(22.11.22)\\n¶\\n\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\neq D_{\\\\textrm{KL}}(Q\\\\|P).\\\\]\\nKL divergence is non-negative, i.e.,\\n(22.11.23)\\n¶\\n\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\geq 0.\\\\]\\nNote that the equality holds only when\\n\\\\(P = Q\\\\)\\n.\\nIf there exists an\\n\\\\(x\\\\)\\nsuch that\\n\\\\(p(x) > 0\\\\)\\nand\\n\\\\(q(x) = 0\\\\)\\n, then\\n\\\\(D_{\\\\textrm{KL}}(P\\\\|Q) = \\\\infty\\\\)\\n.\\nThere is a close relationship between KL divergence and mutual\\ninformation. Besides the relationship shown in\\nFig. 22.11.1\\n,\\n\\\\(I(X, Y)\\\\)\\nis also\\nnumerically equivalent with the following terms:\\n\\\\(D_{\\\\textrm{KL}}(P(X, Y) \\\\ \\\\| \\\\ P(X)P(Y))\\\\)\\n;\\n\\\\(E_Y \\\\{ D_{\\\\textrm{KL}}(P(X \\\\mid Y) \\\\ \\\\| \\\\ P(X)) \\\\}\\\\)\\n;\\n\\\\(E_X \\\\{ D_{\\\\textrm{KL}}(P(Y \\\\mid X) \\\\ \\\\| \\\\ P(Y)) \\\\}\\\\)\\n.\\nFor the first term, we interpret mutual information as the KL\\ndivergence between\\n\\\\(P(X, Y)\\\\)\\nand the product of\\n\\\\(P(X)\\\\)\\nand\\n\\\\(P(Y)\\\\)\\n, and thus is a measure of how different the joint\\ndistribution is from the distribution if they were independent. For\\nthe second term, mutual information tells us the average reduction in\\nuncertainty about\\n\\\\(Y\\\\)\\nthat results from learning the value of\\nthe\\n\\\\(X\\\\)\\n’s distribution. Similarly to the third term.'),\n",
       " Context(context='Maximizing predictive probability of\\n\\\\(Q\\\\)\\nfor distribution\\n\\\\(P\\\\)\\n, (i.e.,\\n\\\\(E_{x \\\\sim P} [\\\\log (q(x))]\\\\)\\n);\\nMinimizing cross-entropy\\n\\\\(\\\\textrm{CE} (P, Q)\\\\)\\n;\\nMinimizing the KL divergence\\n\\\\(D_{\\\\textrm{KL}}(P\\\\|Q)\\\\)\\n.'),\n",
       " Context(context='Information theory is a field of study about encoding, decoding,\\ntransmitting, and manipulating information.\\nEntropy is the unit to measure how much information is presented in\\ndifferent signals.\\nKL divergence can also measure the divergence between two\\ndistributions.\\nCross-entropy can be viewed as an objective function of multi-class\\nclassification. Minimizing cross-entropy loss is equivalent to\\nmaximizing the log-likelihood function.'),\n",
       " Context(context='Verify that the card examples from the first section indeed have the\\nclaimed entropy.\\nShow that the KL divergence\\n\\\\(D(p\\\\|q)\\\\)\\nis nonnegative for all\\ndistributions\\n\\\\(p\\\\)\\nand\\n\\\\(q\\\\)\\n. Hint: use Jensen’s inequality,\\ni.e., use the fact that\\n\\\\(-\\\\log x\\\\)\\nis a convex function.\\nLet’s compute the entropy from a few data sources:\\nAssume that you are watching the output generated by a monkey at a\\ntypewriter. The monkey presses any of the\\n\\\\(44\\\\)\\nkeys of the\\ntypewriter at random (you can assume that it has not discovered\\nany special keys or the shift key yet). How many bits of\\nrandomness per character do you observe?\\nBeing unhappy with the monkey, you replaced it by a drunk\\ntypesetter. It is able to generate words, albeit not coherently.\\nInstead, it picks a random word out of a vocabulary of\\n\\\\(2,000\\\\)\\nwords. Let’s assume that the average length of a\\nword is\\n\\\\(4.5\\\\)\\nletters in English. How many bits of\\nrandomness per character do you observe now?\\nStill being unhappy with the result, you replace the typesetter by\\na high quality language model. The language model can currently\\nobtain a perplexity as low as\\n\\\\(15\\\\)\\npoints per word. The\\ncharacter\\nperplexity\\nof a language model is defined as the\\ninverse of the geometric mean of a set of probabilities, each\\nprobability is corresponding to a character in the word. To be\\nspecific, if the length of a given word is\\n\\\\(l\\\\)\\n, then\\n\\\\(\\\\textrm{PPL}(\\\\textrm{word}) = \\\\left[\\\\prod_i p(\\\\textrm{character}_i)\\\\right]^{ -\\\\frac{1}{l}} = \\\\exp \\\\left[ - \\\\frac{1}{l} \\\\sum_i{\\\\log p(\\\\textrm{character}_i)} \\\\right].\\\\)\\nAssume that the test word has 4.5 letters, how many bits of\\nrandomness per character do you observe now?\\nExplain intuitively why\\n\\\\(I(X, Y) = H(X) - H(X \\\\mid Y)\\\\)\\n. Then,\\nshow this is true by expressing both sides as an expectation with\\nrespect to the joint distribution.\\nWhat is the KL Divergence between the two Gaussian distributions\\n\\\\(\\\\mathcal{N}(\\\\mu_1, \\\\sigma_1^2)\\\\)\\nand\\n\\\\(\\\\mathcal{N}(\\\\mu_2, \\\\sigma_2^2)\\\\)\\n?'),\n",
       " Context(context='22.11. Information Theory\\n22.11.1. Information\\n22.11.1.1. Self-information\\n22.11.2. Entropy\\n22.11.2.1. Motivating Entropy\\n22.11.2.2. Definition\\n22.11.2.3. Interpretations\\n22.11.2.4. Properties of Entropy\\n22.11.3. Mutual Information\\n22.11.3.1. Joint Entropy\\n22.11.3.2. Conditional Entropy\\n22.11.3.3. Mutual Information\\n22.11.3.4. Properties of Mutual Information\\n22.11.3.5. Pointwise Mutual Information\\n22.11.3.6. Applications of Mutual Information\\n22.11.4. Kullback–Leibler Divergence\\n22.11.4.1. Definition\\n22.11.4.2. KL Divergence Properties\\n22.11.4.3. Example\\n22.11.5. Cross-Entropy\\n22.11.5.1. Formal Definition\\n22.11.5.2. Properties\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\n22.11.6. Summary\\n22.11.7. Exercises')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# issue: row 1 & 2 repeat\n",
    "data = [ Context(context=p) for p in extract_output[0]['output'][0]['text'] if len(p) > 200 ]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(context='Verify that the card examples from the first section indeed have the\\nclaimed entropy.\\nShow that the KL divergence\\n\\\\(D(p\\\\|q)\\\\)\\nis nonnegative for all\\ndistributions\\n\\\\(p\\\\)\\nand\\n\\\\(q\\\\)\\n. Hint: use Jensen’s inequality,\\ni.e., use the fact that\\n\\\\(-\\\\log x\\\\)\\nis a convex function.\\nLet’s compute the entropy from a few data sources:\\nAssume that you are watching the output generated by a monkey at a\\ntypewriter. The monkey presses any of the\\n\\\\(44\\\\)\\nkeys of the\\ntypewriter at random (you can assume that it has not discovered\\nany special keys or the shift key yet). How many bits of\\nrandomness per character do you observe?\\nBeing unhappy with the monkey, you replaced it by a drunk\\ntypesetter. It is able to generate words, albeit not coherently.\\nInstead, it picks a random word out of a vocabulary of\\n\\\\(2,000\\\\)\\nwords. Let’s assume that the average length of a\\nword is\\n\\\\(4.5\\\\)\\nletters in English. How many bits of\\nrandomness per character do you observe now?\\nStill being unhappy with the result, you replace the typesetter by\\na high quality language model. The language model can currently\\nobtain a perplexity as low as\\n\\\\(15\\\\)\\npoints per word. The\\ncharacter\\nperplexity\\nof a language model is defined as the\\ninverse of the geometric mean of a set of probabilities, each\\nprobability is corresponding to a character in the word. To be\\nspecific, if the length of a given word is\\n\\\\(l\\\\)\\n, then\\n\\\\(\\\\textrm{PPL}(\\\\textrm{word}) = \\\\left[\\\\prod_i p(\\\\textrm{character}_i)\\\\right]^{ -\\\\frac{1}{l}} = \\\\exp \\\\left[ - \\\\frac{1}{l} \\\\sum_i{\\\\log p(\\\\textrm{character}_i)} \\\\right].\\\\)\\nAssume that the test word has 4.5 letters, how many bits of\\nrandomness per character do you observe now?\\nExplain intuitively why\\n\\\\(I(X, Y) = H(X) - H(X \\\\mid Y)\\\\)\\n. Then,\\nshow this is true by expressing both sides as an expectation with\\nrespect to the joint distribution.\\nWhat is the KL Divergence between the two Gaussian distributions\\n\\\\(\\\\mathcal{N}(\\\\mu_1, \\\\sigma_1^2)\\\\)\\nand\\n\\\\(\\\\mathcal{N}(\\\\mu_2, \\\\sigma_2^2)\\\\)\\n?'),\n",
       " Context(context='22.11. Information Theory\\n22.11.1. Information\\n22.11.1.1. Self-information\\n22.11.2. Entropy\\n22.11.2.1. Motivating Entropy\\n22.11.2.2. Definition\\n22.11.2.3. Interpretations\\n22.11.2.4. Properties of Entropy\\n22.11.3. Mutual Information\\n22.11.3.1. Joint Entropy\\n22.11.3.2. Conditional Entropy\\n22.11.3.3. Mutual Information\\n22.11.3.4. Properties of Mutual Information\\n22.11.3.5. Pointwise Mutual Information\\n22.11.3.6. Applications of Mutual Information\\n22.11.4. Kullback–Leibler Divergence\\n22.11.4.1. Definition\\n22.11.4.2. KL Divergence Properties\\n22.11.4.3. Example\\n22.11.5. Cross-Entropy\\n22.11.5.1. Formal Definition\\n22.11.5.2. Properties\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\n22.11.6. Summary\\n22.11.7. Exercises')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[-2:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformAzureOpenAIConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=AzureOpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wfYPuuML62dfnDVp00OrBomSSxRX', 'object': 'chat.completion', 'created': 1708993449, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Assume that the test word has 4.5 letters, how many bits of randomness per character do you observe now?\",\\n  \"question\": \"What is the assumed average length of a word in the provided context?\",\\n  \"answer\": \"4.5 letters.\"\\n}'}}], 'usage': {'prompt_tokens': 756, 'completion_tokens': 61, 'total_tokens': 817}, 'system_fingerprint': 'fp_8abb16fa4e'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:02<01:02, 62.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n",
      "Received response: {'id': 'chatcmpl-8wfZQwNbmjoJejxd5OAlz0aXladUz', 'object': 'chat.completion', 'created': 1708993512, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"22.11. Information Theory\\\\n22.11.1. Information\\\\n22.11.1.1. Self-information\\\\n22.11.2. Entropy\\\\n22.11.2.1. Motivating Entropy\\\\n22.11.2.2. Definition\\\\n22.11.2.3. Interpretations\\\\n22.11.2.4. Properties of Entropy\\\\n22.11.3. Mutual Information\\\\n22.11.3.1. Joint Entropy\\\\n22.11.3.2. Conditional Entropy\\\\n22.11.3.3. Mutual Information\\\\n22.11.3.4. Properties of Mutual Information\\\\n22.11.3.5. Pointwise Mutual Information\\\\n22.11.3.6. Applications of Mutual Information\\\\n22.11.4. Kullback–Leibler Divergence\\\\n22.11.4.1. Definition\\\\n22.11.4.2. KL Divergence Properties\\\\n22.11.4.3. Example\\\\n22.11.5. Cross-Entropy\\\\n22.11.5.1. Formal Definition\\\\n22.11.5.2. Properties\\\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\\\n22.11.6. Summary\\\\n22.11.7. Exercises\",\\n  \"question\": \"What is considered as an objective function of multi-class classification in information theory?\",\\n  \"answer\": \"Cross-Entropy.\"\\n}'}}], 'usage': {'prompt_tokens': 433, 'completion_tokens': 319, 'total_tokens': 752}, 'system_fingerprint': 'fp_8abb16fa4e'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:12<00:00, 66.38s/it]\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output': [{'response': [{'context': 'Assume that the test word has 4.5 letters, how many bits of randomness per character do you observe now?',\n",
       "      'question': 'What is the assumed average length of a word in the provided context?',\n",
       "      'answer': '4.5 letters.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7ff340e32290>},\n",
       " {'output': [{'response': [{'context': '22.11. Information Theory\\n22.11.1. Information\\n22.11.1.1. Self-information\\n22.11.2. Entropy\\n22.11.2.1. Motivating Entropy\\n22.11.2.2. Definition\\n22.11.2.3. Interpretations\\n22.11.2.4. Properties of Entropy\\n22.11.3. Mutual Information\\n22.11.3.1. Joint Entropy\\n22.11.3.2. Conditional Entropy\\n22.11.3.3. Mutual Information\\n22.11.3.4. Properties of Mutual Information\\n22.11.3.5. Pointwise Mutual Information\\n22.11.3.6. Applications of Mutual Information\\n22.11.4. Kullback–Leibler Divergence\\n22.11.4.1. Definition\\n22.11.4.2. KL Divergence Properties\\n22.11.4.3. Example\\n22.11.5. Cross-Entropy\\n22.11.5.1. Formal Definition\\n22.11.5.2. Properties\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\n22.11.6. Summary\\n22.11.7. Exercises',\n",
       "      'question': 'What is considered as an objective function of multi-class classification in information theory?',\n",
       "      'answer': 'Cross-Entropy.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7ff340e31fc0>}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_af6f2 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_af6f2_row0_col0, #T_af6f2_row0_col1, #T_af6f2_row0_col2, #T_af6f2_row1_col0, #T_af6f2_row1_col1, #T_af6f2_row1_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_af6f2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_af6f2_level0_col0\" class=\"col_heading level0 col0\" >context</th>\n",
       "      <th id=\"T_af6f2_level0_col1\" class=\"col_heading level0 col1\" >question</th>\n",
       "      <th id=\"T_af6f2_level0_col2\" class=\"col_heading level0 col2\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_af6f2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_af6f2_row0_col0\" class=\"data row0 col0\" >Assume that the test word has 4.5 letters, how many bits of randomness per character do you observe now?</td>\n",
       "      <td id=\"T_af6f2_row0_col1\" class=\"data row0 col1\" >What is the assumed average length of a word in the provided context?</td>\n",
       "      <td id=\"T_af6f2_row0_col2\" class=\"data row0 col2\" >4.5 letters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6f2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_af6f2_row1_col0\" class=\"data row1 col0\" >22.11. Information Theory\n",
       "22.11.1. Information\n",
       "22.11.1.1. Self-information\n",
       "22.11.2. Entropy\n",
       "22.11.2.1. Motivating Entropy\n",
       "22.11.2.2. Definition\n",
       "22.11.2.3. Interpretations\n",
       "22.11.2.4. Properties of Entropy\n",
       "22.11.3. Mutual Information\n",
       "22.11.3.1. Joint Entropy\n",
       "22.11.3.2. Conditional Entropy\n",
       "22.11.3.3. Mutual Information\n",
       "22.11.3.4. Properties of Mutual Information\n",
       "22.11.3.5. Pointwise Mutual Information\n",
       "22.11.3.6. Applications of Mutual Information\n",
       "22.11.4. Kullback–Leibler Divergence\n",
       "22.11.4.1. Definition\n",
       "22.11.4.2. KL Divergence Properties\n",
       "22.11.4.3. Example\n",
       "22.11.5. Cross-Entropy\n",
       "22.11.5.1. Formal Definition\n",
       "22.11.5.2. Properties\n",
       "22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\n",
       "22.11.6. Summary\n",
       "22.11.7. Exercises</td>\n",
       "      <td id=\"T_af6f2_row1_col1\" class=\"data row1 col1\" >What is considered as an objective function of multi-class classification in information theory?</td>\n",
       "      <td id=\"T_af6f2_row1_col2\" class=\"data row1 col2\" >Cross-Entropy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff340e33c70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting context, question, and answer into a DataFrame\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in output:\n",
    "    for i in item['output']:\n",
    "        for response in i['response']:\n",
    "            contexts.append(response['context'])\n",
    "            questions.append(response['question'])\n",
    "            answers.append(response['answer'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)  # or use a specific width like 50\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "styled_df = df.style.set_properties(**{'text-align': 'left'}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('text-align', 'left')]\n",
    "}])\n",
    "styled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
