{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Using TransformConfig with Long Text\n",
    "\n",
    "In this example, we demonstrate how to leverage the Transform flow in handling long text that exceeds the token limitation of the ChatGPT API. There are three TransformConfigs for OpenAI that you can add the parameter `auto_split_long_text`\n",
    "- TransformOpenAIConfig\n",
    "- TransformForGenerationOpenAIGPT3p5Config\n",
    "- <del>TransformForClusteringOpenAIGPT4Config</del>\n",
    "\n",
    "We use the text from [War and Peace](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt) as our example.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we store the \"War and Peace\" text file in the `data/raw_input` directory as \"book-war-and-peace.txt\". You can download the file from [here](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b12b5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'cl100k_base'>\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformForGenerationOpenAIGPT3p5Config\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccd3d3",
   "metadata": {},
   "source": [
    "### Generate Context object with long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dfe82668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Context 1 (Preview): 'arn you, if you don't tell me that this means war,...'\n",
      "---\n",
      "---\n",
      "Context 2 (Preview): 'nly she jumped up onto a tub to be higher than he,...'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def extract_contexts(file_path, ranges):\n",
    "    # Read the entire file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Initialize the list for holding contexts\n",
    "    contexts = []\n",
    "\n",
    "    # Extract specified ranges\n",
    "    for start, end in ranges:\n",
    "        # Adjusting end index to fit within the file length if necessary\n",
    "        end = min(end, len(content))\n",
    "        context_text = content[start:end]\n",
    "        contexts.append(Context(context=context_text))\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# Define the ranges for the contexts\n",
    "ranges = [(100, 92000), (92000+12600, 92000+13200)]\n",
    "\n",
    "# Specify the file path\n",
    "file_path = './data/raw_input/book-war-and-peace.txt'\n",
    "\n",
    "# Extract contexts\n",
    "contexts = extract_contexts(file_path, ranges)\n",
    "\n",
    "for i, context in enumerate(contexts):\n",
    "    preview_text = context.context[:50] + \"...\" if len(context.context) > 50 else context.context\n",
    "    print(f\"---\\nContext {i+1} (Preview): '{preview_text}'\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    few_shot_prompt=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"\"\"The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantum for InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements;\"\"\",\n",
    "            question=\"What does the Compute & Networking segment include?\",\n",
    "            answer=\"\"\"The Compute & Networking segment includes Data Center accelerated computing platforms, end-to-end networking platforms (Quantum for InfiniBand and Spectrum for Ethernet), the NVIDIA DRIVE automated-driving platform, and automotive development agreements.\"\"\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17)'s default LLM to generate questions and answers.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `OpenAIConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`.\n",
    "\n",
    "Please note that we include the `auto_split_long_text` parameter in the transform configuration. This ensures that if a `Context` object contains text exceeding the specified token length limit, it will automatically be split into multiple `Context` objects. Each of these objects will contain text segments that adhere to the limit, ready for submission to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformForGenerationOpenAIGPT3p5Config(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=OpenAIModelConfig(),\n",
    "    auto_split_long_text=True\n",
    ")\n",
    "\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text (first 40 chars): 'arn you, if you don't tell me that this ' has token length: 4031\n",
      "Chunk text (first 40 chars): 'Anna Pavlovna's alarm was justified, for' has token length: 3888\n",
      "Chunk text (first 40 chars): 'Influence in society, however, is a capi' has token length: 4068\n",
      "Chunk text (first 40 chars): '\"Princesse, au revoir,\" cried he, stumbl' has token length: 3967\n",
      "Chunk text (first 40 chars): 'It was past one o'clock when Pierre left' has token length: 3984\n",
      "Chunk text (first 40 chars): '\"Dear Countess, what an age... She has b' has token length: 2056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:07<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who is the illegitimate son of Count '\n",
      "                           'Bezukhov?\\n'\n",
      "                           'answer: The illegitimate son of Count Bezukhov is '\n",
      "                           'a stout, heavily built young man with '\n",
      "                           'close-cropped hair, spectacles, and a brown dress '\n",
      "                           'coat.']}],\n",
      "  'root': <uniflow.node.Node object at 0x12633f430>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who is the elderly lady sitting with the '\n",
      "                           'old aunt?\\n'\n",
      "                           'answer: The elderly lady is Princess Drubetskaya, '\n",
      "                           'who belongs to one of the best families in Russia '\n",
      "                           'and is poor.']}],\n",
      "  'root': <uniflow.node.Node object at 0x12633fee0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who did the footmen assist into the '\n",
      "                           'carriage?\\n'\n",
      "                           'answer: The footman helped the princess into the '\n",
      "                           'carriage.']}],\n",
      "  'root': <uniflow.node.Node object at 0x121f5d2b0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who was Pierre staying with and sharing '\n",
      "                           'the dissipated life of their son Anatole?\\n'\n",
      "                           'answer: Pierre was staying at Prince Vasili '\n",
      "                           \"Kuragin's and sharing the dissipated life of his \"\n",
      "                           'son Anatole.']}],\n",
      "  'root': <uniflow.node.Node object at 0x12629e3a0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [\"question: Who is the countess' gigantic footman?\\n\"\n",
      "                           \"answer: The countess' gigantic footman is \"\n",
      "                           'announcing Marya Lvovna Karagina and her daughter '\n",
      "                           'into the drawing room.']}],\n",
      "  'root': <uniflow.node.Node object at 0x121ef39d0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': [\"question: Who was the count's eldest son?\\n\"\n",
      "                           \"answer: Nicholas, the undergraduate, the count's \"\n",
      "                           'eldest son.']}],\n",
      "  'root': <uniflow.node.Node object at 0x125da0be0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: What does Natasha count on her fingers '\n",
      "                           'in the last part of the passage?\\n'\n",
      "                           'answer: Natasha counts thirteen, fourteen, '\n",
      "                           'fifteen, sixteen on her slender little fingers.']}],\n",
      "  'root': <uniflow.node.Node object at 0x12629d730>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(contexts)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230239f6",
   "metadata": {},
   "source": [
    "### Display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "74ccffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is the illegitimate son of Count Bezukhov?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The illegitimate son of Count Bezukhov is a stout, heavily built young man with close-cropped hair, spectacles, and a brown dress coat.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is the elderly lady sitting with the old aunt?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The elderly lady is Princess Drubetskaya, who belongs to one of the best families in Russia and is poor.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who did the footmen assist into the carriage?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The footman helped the princess into the carriage.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who was Pierre staying with and sharing the dissipated life of their son Anatole?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Pierre was staying at Prince Vasili Kuragin's and sharing the dissipated life of his son Anatole.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is the countess' gigantic footman?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The countess' gigantic footman is announcing Marya Lvovna Karagina and her daughter into the drawing room.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who was the count's eldest son?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Nicholas, the undergraduate, the count's eldest son.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> What does Natasha count on her fingers in the last part of the passage?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Natasha counts thirteen, fourteen, fifteen, sixteen on her slender little fingers.\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define a function to wrap text in HTML tags with style\n",
    "def format_html_question_answer(question, answer):\n",
    "    return f\"\"\"\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
    "            <b>Question:</b> {question}\n",
    "        </div>\n",
    "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
    "            <b>Answer:</b> {answer}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_output = \"\"\n",
    "for o in output:\n",
    "    response = o['output'][0]['response'][0]\n",
    "    # Split the response based on the first occurrence of '\\nanswer'\n",
    "    split_index = response.find('\\nanswer')\n",
    "    question = response[:split_index].replace('question: ', '')\n",
    "    answer = response[split_index:].replace('\\nanswer: ', '')\n",
    "    html_output += format_html_question_answer(question, answer)\n",
    "\n",
    "# Display the formatted HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b67b7",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "- When splitter using token length for this case (Token length for each context: 4096): 100%|██████████| 7/7 [00:09<00:00,  1.33s/it]\n",
    "\n",
    "- When splitter using char length for this case (Char length for each context: 4096): 100%|██████████| 25/25 [00:27<00:00,  1.11s/it]\n",
    "\n",
    "(We are using the splitter based on token length for config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
