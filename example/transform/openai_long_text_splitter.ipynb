{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Using TransformConfig with Long Text\n",
    "\n",
    "In this example, we demonstrate how to leverage the Transform flow in handling long text that exceeds the token limitation of the ChatGPT API. There are three TransformConfigs for OpenAI that you can add the parameter `auto_split_long_text`\n",
    "- TransformOpenAIConfig\n",
    "- TransformForGenerationOpenAIGPT3p5Config\n",
    "- <del>TransformForClusteringOpenAIGPT4Config</del>\n",
    "\n",
    "We use the text from [War and Peace](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt) as our example.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we store the \"War and Peace\" text file in the `data/raw_input` directory as \"book-war-and-peace.txt\". You can download the file from [here](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'cl100k_base'>\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformForGenerationOpenAIGPT3p5Config\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccd3d3",
   "metadata": {},
   "source": [
    "### Generate Context object with long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe82668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Context 1 (Preview): 'arn you, if you don't tell me that this means war,...'\n",
      "---\n",
      "---\n",
      "Context 2 (Preview): 'nly she jumped up onto a tub to be higher than he,...'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def extract_contexts(file_path, ranges):\n",
    "    # Read the entire file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Initialize the list for holding contexts\n",
    "    contexts = []\n",
    "\n",
    "    # Extract specified ranges\n",
    "    for start, end in ranges:\n",
    "        # Adjusting end index to fit within the file length if necessary\n",
    "        end = min(end, len(content))\n",
    "        context_text = content[start:end]\n",
    "        contexts.append(Context(context=context_text))\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# Define the ranges for the contexts\n",
    "ranges = [(100, 92000), (92000+12600, 92000+13200)]\n",
    "\n",
    "# Specify the file path\n",
    "file_path = './data/raw_input/book-war-and-peace.txt'\n",
    "\n",
    "# Extract contexts\n",
    "contexts = extract_contexts(file_path, ranges)\n",
    "\n",
    "for i, context in enumerate(contexts):\n",
    "    preview_text = context.context[:50] + \"...\" if len(context.context) > 50 else context.context\n",
    "    print(f\"---\\nContext {i+1} (Preview): '{preview_text}'\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    few_shot_prompt=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"\"\"The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantum for InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements;\"\"\",\n",
    "            question=\"What does the Compute & Networking segment include?\",\n",
    "            answer=\"\"\"The Compute & Networking segment includes Data Center accelerated computing platforms, end-to-end networking platforms (Quantum for InfiniBand and Spectrum for Ethernet), the NVIDIA DRIVE automated-driving platform, and automotive development agreements.\"\"\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17)'s default LLM to generate questions and answers.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `OpenAIConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`.\n",
    "\n",
    "Please note that we include the `auto_split_long_text` parameter in the transform configuration. This ensures that if a `Context` object contains text exceeding the specified token length limit, it will automatically be split into multiple `Context` objects. Each of these objects will contain text segments that adhere to the limit, ready for submission to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformForGenerationOpenAIGPT3p5Config(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=OpenAIModelConfig(),\n",
    "    auto_split_long_text=True\n",
    ")\n",
    "\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text (first 40 chars): 'arn you, if you don't tell me that this ' has token length: 4031\n",
      "Chunk text (first 40 chars): 'Anna Pavlovna's alarm was justified, for' has token length: 3888\n",
      "Chunk text (first 40 chars): 'Influence in society, however, is a capi' has token length: 4068\n",
      "Chunk text (first 40 chars): '\"Princesse, au revoir,\" cried he, stumbl' has token length: 3967\n",
      "Chunk text (first 40 chars): 'It was past one o'clock when Pierre left' has token length: 3984\n",
      "Chunk text (first 40 chars): '\"Dear Countess, what an age... She has b' has token length: 2056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:06<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who was the well-known Anna Pavlovna '\n",
      "                           \"Scherer's maid of honor and favorite of the \"\n",
      "                           'Empress Marya Fedorovna?\\n'\n",
      "                           \"answer: The well-known Anna Pavlovna Scherer's \"\n",
      "                           'maid of honor and favorite of the Empress Marya '\n",
      "                           'Fedorovna was Prince Vasili Kuragin.']}],\n",
      "  'root': <uniflow.node.Node object at 0x125da0550>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Which character in the story is '\n",
      "                           'described as \"very lovely\"?\\n'\n",
      "                           'answer: Princess Helene.']}],\n",
      "  'root': <uniflow.node.Node object at 0x12629d760>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who left the hall after taking leave of '\n",
      "                           'Anna Pavlovna, having managed to speak to Lise '\n",
      "                           'about a match between Anatole and the little '\n",
      "                           \"princess' sister-in-law?\\n\"\n",
      "                           'answer: Anna Pavlovna left the hall after taking '\n",
      "                           'leave of the princess and managing to speak to '\n",
      "                           'Lise about a match between Anatole and the little '\n",
      "                           \"princess' sister-in-law.\"]}],\n",
      "  'root': <uniflow.node.Node object at 0x1262441c0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who is Pierre staying with and sharing a '\n",
      "                           'dissipated life with?\\n'\n",
      "                           'answer: Pierre is staying at Prince Vasili '\n",
      "                           \"Kuragin's and sharing the dissipated life of his \"\n",
      "                           'son Anatole, whom they are planning to reform by '\n",
      "                           \"marrying him to Prince Andrew's sister.\"]}],\n",
      "  'root': <uniflow.node.Node object at 0x126302100>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who is the protagonist of this story?\\n'\n",
      "                           'answer: The protagonist of this story is Pierre, '\n",
      "                           'who is attending a drinking and gambling gathering '\n",
      "                           \"at Kuragin's house.\"]}],\n",
      "  'root': <uniflow.node.Node object at 0x126302be0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: Who is the countess addressing when she '\n",
      "                           'says \"Ma chere, there is a time for everything,\" '\n",
      "                           'with feigned severity?\\n'\n",
      "                           'answer: The count, Ilya.']}],\n",
      "  'root': <uniflow.node.Node object at 0x125db0dc0>},\n",
      " {'output': [{'error': 'No errors.',\n",
      "              'response': ['question: What did Natasha count on her little '\n",
      "                           'finger?\\n'\n",
      "                           'answer: Natasha counted \"thirteen, fourteen, '\n",
      "                           'fifteen, sixteen\" on her slender little finger.']}],\n",
      "  'root': <uniflow.node.Node object at 0x1262b8820>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(contexts)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230239f6",
   "metadata": {},
   "source": [
    "### Display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who was the well-known Anna Pavlovna Scherer's maid of honor and favorite of the Empress Marya Fedorovna?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The well-known Anna Pavlovna Scherer's maid of honor and favorite of the Empress Marya Fedorovna was Prince Vasili Kuragin.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Which character in the story is described as \"very lovely\"?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Princess Helene.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who left the hall after taking leave of Anna Pavlovna, having managed to speak to Lise about a match between Anatole and the little princess' sister-in-law?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Anna Pavlovna left the hall after taking leave of the princess and managing to speak to Lise about a match between Anatole and the little princess' sister-in-law.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is Pierre staying with and sharing a dissipated life with?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Pierre is staying at Prince Vasili Kuragin's and sharing the dissipated life of his son Anatole, whom they are planning to reform by marrying him to Prince Andrew's sister.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is the protagonist of this story?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The protagonist of this story is Pierre, who is attending a drinking and gambling gathering at Kuragin's house.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is the countess addressing when she says \"Ma chere, there is a time for everything,\" with feigned severity?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The count, Ilya.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> What did Natasha count on her little finger?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Natasha counted \"thirteen, fourteen, fifteen, sixteen\" on her slender little finger.\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define a function to wrap text in HTML tags with style\n",
    "def format_html_question_answer(question, answer):\n",
    "    return f\"\"\"\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
    "            <b>Question:</b> {question}\n",
    "        </div>\n",
    "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
    "            <b>Answer:</b> {answer}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_output = \"\"\n",
    "for o in output:\n",
    "    response = o['output'][0]['response'][0]\n",
    "    # Split the response based on the first occurrence of '\\nanswer'\n",
    "    split_index = response.find('\\nanswer')\n",
    "    question = response[:split_index].replace('question: ', '')\n",
    "    answer = response[split_index:].replace('\\nanswer: ', '')\n",
    "    html_output += format_html_question_answer(question, answer)\n",
    "\n",
    "# Display the formatted HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b67b7",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "- When splitter using token length for this case (Token length for each context: 4096): 100%|██████████| 7/7 [00:09<00:00,  1.33s/it]\n",
    "\n",
    "- When splitter using char length for this case (Char length for each context: 4096): 100%|██████████| 25/25 [00:27<00:00,  1.11s/it]\n",
    "\n",
    "(We are using the splitter based on token length for config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
