{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "# Using TransformConfig with Long Text\n",
    "\n",
    "In this example, we demonstrate how to leverage the Transform flow in handling long text that exceeds the token limitation of the ChatGPT API. There are three TransformConfigs for OpenAI that you can add the parameter `auto_split_long_text`\n",
    "- TransformOpenAIConfig\n",
    "- TransformForGenerationOpenAIGPT3p5Config\n",
    "- <del>TransformForClusteringOpenAIGPT4Config</del>\n",
    "\n",
    "We use the text from [War and Peace](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt) as our example.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)\n",
    "\n",
    "Finally, we store the \"War and Peace\" text file in the `data/raw_input` directory as \"book-war-and-peace.txt\". You can download the file from [here](https://github.com/mmcky/nyu-econ-370/blob/master/notebooks/data/book-war-and-peace.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install helper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b12b5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'cl100k_base'>\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc)\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import TransformClient\n",
    "from uniflow.flow.config import TransformForGenerationOpenAIGPT3p5Config\n",
    "from uniflow.op.model.model_config import OpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccd3d3",
   "metadata": {},
   "source": [
    "### Generate Context object with long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe82668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Context 1 (Preview): 'arn you, if you don't tell me that this means war,...'\n",
      "---\n",
      "---\n",
      "Context 2 (Preview): 'nly she jumped up onto a tub to be higher than he,...'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def extract_contexts(file_path, ranges):\n",
    "    # Read the entire file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Initialize the list for holding contexts\n",
    "    contexts = []\n",
    "\n",
    "    # Extract specified ranges\n",
    "    for start, end in ranges:\n",
    "        # Adjusting end index to fit within the file length if necessary\n",
    "        end = min(end, len(content))\n",
    "        context_text = content[start:end]\n",
    "        contexts.append(Context(context=context_text))\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# Define the ranges for the contexts\n",
    "ranges = [(100, 92000), (92000+12600, 92000+13200)]\n",
    "\n",
    "# Specify the file path\n",
    "file_path = './data/raw_input/book-war-and-peace.txt'\n",
    "\n",
    "# Extract contexts\n",
    "contexts = extract_contexts(file_path, ranges)\n",
    "\n",
    "for i, context in enumerate(contexts):\n",
    "    preview_text = context.context[:50] + \"...\" if len(context.context) > 50 else context.context\n",
    "    print(f\"---\\nContext {i+1} (Preview): '{preview_text}'\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "    instruction=\"\"\"Generate one question and its corresponding answer based on the last context in the last\n",
    "    example. Follow the format of the examples below to include context, question, and answer in the response\"\"\",\n",
    "    few_shot_prompt=[\n",
    "        Context(\n",
    "            context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "            question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "            answer=\"Claude E. Shannon.\",\n",
    "        ),\n",
    "        Context(\n",
    "            context=\"\"\"The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantum for InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements;\"\"\",\n",
    "            question=\"What does the Compute & Networking segment include?\",\n",
    "            answer=\"\"\"The Compute & Networking segment includes Data Center accelerated computing platforms, end-to-end networking platforms (Quantum for InfiniBand and Spectrum for Ethernet), the NVIDIA DRIVE automated-driving platform, and automotive development agreements.\"\"\",\n",
    "        ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to generate data\n",
    "\n",
    "In this example, we will use the [OpenAIModelConfig](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17)'s default LLM to generate questions and answers.\n",
    "\n",
    "Here, we pass in our `guided_prompt` to the `OpenAIConfig` to use our customized instructions and examples, instead of the `uniflow` default ones.\n",
    "\n",
    "We also want to get the response in the `json` format instead of the `text` default, so we set the `response_format` to `json_object`.\n",
    "\n",
    "Please note that we include the `auto_split_long_text` parameter in the transform configuration. This ensures that if a `Context` object contains text exceeding the specified token length limit, it will automatically be split into multiple `Context` objects. Each of these objects will contain text segments that adhere to the limit, ready for submission to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformForGenerationOpenAIGPT3p5Config(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=OpenAIModelConfig(),\n",
    "    auto_split_long_text=True\n",
    ")\n",
    "\n",
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `run` method on the `client` object to execute the question-answer generation operation on the data shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18150255c75a40178058f502d3b993b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = client.run(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230239f6",
   "metadata": {},
   "source": [
    "### Display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74ccffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who is Anna Pavlovna referring to when she says \"You know,\" and then proceeds to ask what the war is for?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Anna Pavlovna is referring to the little princess, who is asking Prince Vasili about the purpose of the war.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who entered the drawing room just then and was addressed by Anna Pavlovna?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Prince Andrew Bolkonski, the little princess' husband.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> What was Prince Hippolyte's reaction to Anna Pavlovna's question about seeing Pierre again?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Prince Hippolyte's reaction was to express joy and delight at the evening, saying he was glad he did not go to the ambassador's, and that it had been a delightful evening. He also snatched the shawl from the footman, wrapped it around the princess, and kept his arm around her for a long time, as though embracing her.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who was Pierre staying with at Prince Vasili Kuragin's?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Pierre was staying at Prince Vasili Kuragin's and sharing the dissipated life of his son Anatole, the son whom they were planning to reform by marrying him to Prince Andrew's sister.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who was the first person to attempt the dangerous bet of drinking a whole bottle of rum without taking it from his mouth?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> Dolokhov was the first person to attempt the dangerous bet of drinking a whole bottle of rum without taking it from his mouth.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> Who are the younger generation of family members settled in the drawing room?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> The younger generation of family members includes Boris, the officer, Anna Mikhaylovna's son; Nicholas, the undergraduate, the count's eldest son; Sonya, the count's fifteen-year-old niece, and little Petya, his youngest boy.\n",
       "        </div>\n",
       "    </div>\n",
       "    \n",
       "    <div style=\"margin-bottom: 20px;\">\n",
       "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
       "            <b>Question:</b> er. \"All right, I will wait. But it's a long time!\" she added, turning to Pierre.\n",
       "\n",
       "How long does Pierre ask Natasha to wait before he will ask for her hand in marriage?\n",
       "        </div>\n",
       "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
       "            <b>Answer:</b> In another four years.\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define a function to wrap text in HTML tags with style\n",
    "def format_html_question_answer(question, answer):\n",
    "    return f\"\"\"\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px;\">\n",
    "            <b>Question:</b> {question}\n",
    "        </div>\n",
    "        <div style=\"background-color: #d9edf7; padding: 10px; border-radius: 5px; margin-top: 5px;\">\n",
    "            <b>Answer:</b> {answer}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_output = \"\"\n",
    "for o in output:\n",
    "    response = o['output'][0]['response'][0]\n",
    "    # Split the response based on the first occurrence of '\\nanswer'\n",
    "    split_index = response.find('\\nanswer')\n",
    "    question = response[:split_index].replace('question: ', '')\n",
    "    answer = response[split_index:].replace('\\nanswer: ', '')\n",
    "    html_output += format_html_question_answer(question, answer)\n",
    "\n",
    "# Display the formatted HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b67b7",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "- When splitter using token length for this case (Token length for each context: 4096): 100%|██████████| 7/7 [00:09<00:00,  1.33s/it]\n",
    "\n",
    "- When splitter using char length for this case (Char length for each context: 4096): 100%|██████████| 25/25 [00:27<00:00,  1.11s/it]\n",
    "\n",
    "(We are using the splitter based on token length for config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
