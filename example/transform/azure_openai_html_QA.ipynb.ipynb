{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of generating QAs for an ML book using Azure OpenAI\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "Make sure you have a .env file in the root directory with following parameter values in the root directory of this project\n",
    "```\n",
    "    AZURE_API_KEY=\"YOUR_API_KEY\"\n",
    "    AZURE_ENDPOINT=\"YOUR_ENDPOINT\"\n",
    "    AZURE_DEPLOYMENT_NAME=\"YOUR_DEPLOYMENT_NAME\"\n",
    "    AZURE_API_VERSION=\"YOUR_API_VERSION\"\n",
    "```\n",
    "`AZURE_API_KEY`, `AZURE_ENDPOINT`, and `AZURE_DEPLOYMENT_NAME` can be accessed at your Azure OpenAI portal. Available `AZURE_API_VERSION` can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import ExtractClient, TransformClient\n",
    "from uniflow.flow.config import ExtractHTMLConfig, TransformAzureOpenAIConfig\n",
    "from uniflow.flow.flow_factory import FlowFactory\n",
    "from uniflow.op.model.model_config import AzureOpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extract': ['ExtractHTMLFlow',\n",
       "  'ExtractImageFlow',\n",
       "  'ExtractIpynbFlow',\n",
       "  'ExtractMarkdownFlow',\n",
       "  'ExtractPDFFlow',\n",
       "  'ExtractTxtFlow'],\n",
       " 'transform': ['TransformAzureOpenAIFlow',\n",
       "  'TransformCopyFlow',\n",
       "  'TransformGoogleFlow',\n",
       "  'TransformGoogleMultiModalModelFlow',\n",
       "  'TransformHuggingFaceFlow',\n",
       "  'TransformLMQGFlow',\n",
       "  'TransformOpenAIFlow'],\n",
       " 'rater': ['RaterFlow']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlowFactory.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file = \"22.11_information-theory.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set current directory and input data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the html file via ExtractClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [{\"filename\": input_file}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_client = ExtractClient(ExtractHTMLConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_output = extract_client.run(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "        instruction=\"Generate one question and its corresponding answer based on context. Following the format of the examples below to include the same context, question, and answer in the response.\",\n",
    "        few_shot_prompt=[\n",
    "            Context(\n",
    "                context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "                question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "                answer=\"Claude E. Shannon.\",\n",
    "            )\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ Context(context=p) for p in extract_output[0]['output'][0]['text'] if len(p) > 200 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ModelFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformAzureOpenAIConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=AzureOpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:05<00:50,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTyGzjjpGzIVJztqgDYCweZrRBSR', 'object': 'chat.completion', 'created': 1709187252, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\"context\": \"If we dive deep into the classification objective function with cross-entropy loss \\\\\\\\(\\\\\\\\textrm{CE}\\\\\\\\), we will find minimizing \\\\\\\\(\\\\\\\\textrm{CE}\\\\\\\\) is equivalent to maximizing the log-likelihood function \\\\\\\\(L\\\\\\\\).\", \"question\": \"What is minimizing cross-entropy loss equivalent to in the context of the classification objective function?\", \"answer\": \"Maximizing the log-likelihood function \\\\\\\\(L\\\\\\\\).\"}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 203, 'completion_tokens': 97, 'total_tokens': 300}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:15<01:03,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTyMOsN0JKeECTk9SYcDn6dV4HVa', 'object': 'chat.completion', 'created': 1709187258, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"To begin with, suppose that we are given a dataset with \\\\\\\\(n\\\\\\\\) examples, and it can be classified into \\\\\\\\(k\\\\\\\\) -classes. For each data example \\\\\\\\(i\\\\\\\\), we represent any \\\\\\\\(k\\\\\\\\)-class label \\\\\\\\(\\\\\\\\mathbf{y}_i = (y_{i1}, \\\\\\\\ldots, y_{ik})\\\\\\\\) by one-hot encoding. To be specific, if the example \\\\\\\\(i\\\\\\\\) belongs to class \\\\\\\\(j\\\\\\\\), then we set the \\\\\\\\(j\\\\\\\\)-th entry to \\\\\\\\(1\\\\\\\\), and all other components to \\\\\\\\(0\\\\\\\\), i.e.,\",\\n  \"question\": \"What encoding method is used to represent class labels in the given dataset?\",\\n  \"answer\": \"One-hot encoding.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 289, 'completion_tokens': 169, 'total_tokens': 458}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:22<00:52,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTyVbwemV6KX0K663uduEruxFXu4', 'object': 'chat.completion', 'created': 1709187267, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"For instance, if a multi-class classification problem contains three classes \\\\\\\\(A\\\\\\\\) , \\\\\\\\(B\\\\\\\\) , and \\\\\\\\(C\\\\\\\\) , then the labels \\\\\\\\(\\\\\\\\mathbf{y}_i\\\\\\\\) can be encoded in { \\\\\\\\(A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)\\\\\\\\) }.\",\\n  \"question\": \"How are the labels for classes A, B, and C encoded in a multi-class classification problem?\",\\n  \"answer\": \"A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 238, 'completion_tokens': 159, 'total_tokens': 397}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:30<00:46,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTycGyAWWpI4j8fB3msuYsXywiFT', 'object': 'chat.completion', 'created': 1709187274, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\"context\": \"On the other side, we can also approach the problem through maximum likelihood estimation. To begin with, let\\'s quickly introduce a k-class multinoulli distribution. It is an extension of the Bernoulli distribution from binary class to multi-class. If a random variable z = (z_{1}, \\\\\\\\ldots, z_{k}) follows a k-class multinoulli distribution with probabilities p = (p_{1}, \\\\\\\\ldots, p_{k}), i.e.,\", \"question\": \"What is a k-class multinoulli distribution an extension of?\", \"answer\": \"The Bernoulli distribution.\"}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 281, 'completion_tokens': 129, 'total_tokens': 410}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:34<00:31,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTykATLKrOQuQgpkCDlPMnGacCDc', 'object': 'chat.completion', 'created': 1709187282, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of\\\\nCommunication (Shannon, 1948) establishing the theory of\\\\ninformation. In his article, Shannon introduced the concept of\\\\ninformation entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 article?\",\\n  \"answer\": \"The concept of information entropy.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 294, 'completion_tokens': 97, 'total_tokens': 391}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:43<00:29,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTyoPxWaXI751uEM0ViMteVK0TDz', 'object': 'chat.completion', 'created': 1709187286, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Since in maximum likelihood estimation, we maximizing the objective function \\\\\\\\(l(\\\\\\\\theta)\\\\\\\\) by having \\\\\\\\(\\\\\\\\pi_{j} = p_{\\\\\\\\theta} (y_{ij} \\\\\\\\mid \\\\\\\\mathbf{x}_i)\\\\\\\\) . Therefore, for any multi-class classification, maximizing the above log-likelihood function \\\\\\\\(l(\\\\\\\\theta)\\\\\\\\) is equivalent to minimizing the CE loss \\\\\\\\(\\\\\\\\textrm{CE}(y, \\\\\\\\hat{y})\\\\\\\\) .\",\\n  \"question\": \"What is equivalent to minimizing the CE loss in multi-class classification?\",\\n  \"answer\": \"Maximizing the log-likelihood function \\\\\\\\(l(\\\\\\\\theta)\\\\\\\\).\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 251, 'completion_tokens': 151, 'total_tokens': 402}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:47<00:19,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTyx3cAqB8B0oAd54louszkSm5b0', 'object': 'chat.completion', 'created': 1709187295, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"To test the above proof, let’s apply the built-in measure NegativeLogLikelihood. Using the same labels and preds as in the earlier example, we will get the same numerical loss as the previous example up to the 5 decimal place.\",\\n  \"question\": \"What measure is applied to test the proof in the context?\",\\n  \"answer\": \"NegativeLogLikelihood.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 201, 'completion_tokens': 83, 'total_tokens': 284}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:56<00:14,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTz2STxqVx7wAkuhcIXJKzxxby3Q', 'object': 'chat.completion', 'created': 1709187300, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Information theory is a field of study about encoding, decoding, transmitting, and manipulating information. Entropy is the unit to measure how much information is presented in different signals. KL divergence can also measure the divergence between two distributions. Cross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.\",\\n  \"question\": \"What is the objective of minimizing cross-entropy loss in multi-class classification?\",\\n  \"answer\": \"Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 241, 'completion_tokens': 126, 'total_tokens': 367}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:02<00:06,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTzBskRd8aDDh6qUzLQeZmYDNtDs', 'object': 'chat.completion', 'created': 1709187309, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 article?\",\\n  \"answer\": \"Information entropy.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 756, 'completion_tokens': 91, 'total_tokens': 847}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:19<00:00,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8xTzGwv9aG41kI51Aaw5zBkz5JUrt', 'object': 'chat.completion', 'created': 1709187314, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"22.11. Information Theory\\\\n22.11.1. Information\\\\n22.11.1.1. Self-information\\\\n22.11.2. Entropy\\\\n22.11.2.1. Motivating Entropy\\\\n22.11.2.2. Definition\\\\n22.11.2.3. Interpretations\\\\n22.11.2.4. Properties of Entropy\\\\n22.11.3. Mutual Information\\\\n22.11.3.1. Joint Entropy\\\\n22.11.3.2. Conditional Entropy\\\\n22.11.3.3. Mutual Information\\\\n22.11.3.4. Properties of Mutual Information\\\\n22.11.3.5. Pointwise Mutual Information\\\\n22.11.3.6. Applications of Mutual Information\\\\n22.11.4. Kullback–Leibler Divergence\\\\n22.11.4.1. Definition\\\\n22.11.4.2. KL Divergence Properties\\\\n22.11.4.3. Example\\\\n22.11.5. Cross-Entropy\\\\n22.11.5.1. Formal Definition\\\\n22.11.5.2. Properties\\\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\\\n22.11.6. Summary\\\\n22.11.7. Exercises\",\\n  \"question\": \"What concept serves as an objective function for multi-class classification according to the context?\",\\n  \"answer\": \"Cross-Entropy.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 433, 'completion_tokens': 320, 'total_tokens': 753}, 'system_fingerprint': 'fp_8abb16fa4e'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format result into pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f5051 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f5051_row0_col0, #T_f5051_row0_col1, #T_f5051_row0_col2, #T_f5051_row1_col0, #T_f5051_row1_col1, #T_f5051_row1_col2, #T_f5051_row2_col0, #T_f5051_row2_col1, #T_f5051_row2_col2, #T_f5051_row3_col0, #T_f5051_row3_col1, #T_f5051_row3_col2, #T_f5051_row4_col0, #T_f5051_row4_col1, #T_f5051_row4_col2, #T_f5051_row5_col0, #T_f5051_row5_col1, #T_f5051_row5_col2, #T_f5051_row6_col0, #T_f5051_row6_col1, #T_f5051_row6_col2, #T_f5051_row7_col0, #T_f5051_row7_col1, #T_f5051_row7_col2, #T_f5051_row8_col0, #T_f5051_row8_col1, #T_f5051_row8_col2, #T_f5051_row9_col0, #T_f5051_row9_col1, #T_f5051_row9_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f5051\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5051_level0_col0\" class=\"col_heading level0 col0\" >context</th>\n",
       "      <th id=\"T_f5051_level0_col1\" class=\"col_heading level0 col1\" >question</th>\n",
       "      <th id=\"T_f5051_level0_col2\" class=\"col_heading level0 col2\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f5051_row0_col0\" class=\"data row0 col0\" >If we dive deep into the classification objective function with cross-entropy loss \\(\\textrm{CE}\\), we will find minimizing \\(\\textrm{CE}\\) is equivalent to maximizing the log-likelihood function \\(L\\).</td>\n",
       "      <td id=\"T_f5051_row0_col1\" class=\"data row0 col1\" >What is minimizing cross-entropy loss equivalent to in the context of the classification objective function?</td>\n",
       "      <td id=\"T_f5051_row0_col2\" class=\"data row0 col2\" >Maximizing the log-likelihood function \\(L\\).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f5051_row1_col0\" class=\"data row1 col0\" >To begin with, suppose that we are given a dataset with \\(n\\) examples, and it can be classified into \\(k\\) -classes. For each data example \\(i\\), we represent any \\(k\\)-class label \\(\\mathbf{y}_i = (y_{i1}, \\ldots, y_{ik})\\) by one-hot encoding. To be specific, if the example \\(i\\) belongs to class \\(j\\), then we set the \\(j\\)-th entry to \\(1\\), and all other components to \\(0\\), i.e.,</td>\n",
       "      <td id=\"T_f5051_row1_col1\" class=\"data row1 col1\" >What encoding method is used to represent class labels in the given dataset?</td>\n",
       "      <td id=\"T_f5051_row1_col2\" class=\"data row1 col2\" >One-hot encoding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f5051_row2_col0\" class=\"data row2 col0\" >For instance, if a multi-class classification problem contains three classes \\(A\\) , \\(B\\) , and \\(C\\) , then the labels \\(\\mathbf{y}_i\\) can be encoded in { \\(A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)\\) }.</td>\n",
       "      <td id=\"T_f5051_row2_col1\" class=\"data row2 col1\" >How are the labels for classes A, B, and C encoded in a multi-class classification problem?</td>\n",
       "      <td id=\"T_f5051_row2_col2\" class=\"data row2 col2\" >A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f5051_row3_col0\" class=\"data row3 col0\" >On the other side, we can also approach the problem through maximum likelihood estimation. To begin with, let's quickly introduce a k-class multinoulli distribution. It is an extension of the Bernoulli distribution from binary class to multi-class. If a random variable z = (z_{1}, \\ldots, z_{k}) follows a k-class multinoulli distribution with probabilities p = (p_{1}, \\ldots, p_{k}), i.e.,</td>\n",
       "      <td id=\"T_f5051_row3_col1\" class=\"data row3 col1\" >What is a k-class multinoulli distribution an extension of?</td>\n",
       "      <td id=\"T_f5051_row3_col2\" class=\"data row3 col2\" >The Bernoulli distribution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f5051_row4_col0\" class=\"data row4 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of\n",
       "Communication (Shannon, 1948) establishing the theory of\n",
       "information. In his article, Shannon introduced the concept of\n",
       "information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_f5051_row4_col1\" class=\"data row4 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 article?</td>\n",
       "      <td id=\"T_f5051_row4_col2\" class=\"data row4 col2\" >The concept of information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f5051_row5_col0\" class=\"data row5 col0\" >Since in maximum likelihood estimation, we maximizing the objective function \\(l(\\theta)\\) by having \\(\\pi_{j} = p_{\\theta} (y_{ij} \\mid \\mathbf{x}_i)\\) . Therefore, for any multi-class classification, maximizing the above log-likelihood function \\(l(\\theta)\\) is equivalent to minimizing the CE loss \\(\\textrm{CE}(y, \\hat{y})\\) .</td>\n",
       "      <td id=\"T_f5051_row5_col1\" class=\"data row5 col1\" >What is equivalent to minimizing the CE loss in multi-class classification?</td>\n",
       "      <td id=\"T_f5051_row5_col2\" class=\"data row5 col2\" >Maximizing the log-likelihood function \\(l(\\theta)\\).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f5051_row6_col0\" class=\"data row6 col0\" >To test the above proof, let’s apply the built-in measure NegativeLogLikelihood. Using the same labels and preds as in the earlier example, we will get the same numerical loss as the previous example up to the 5 decimal place.</td>\n",
       "      <td id=\"T_f5051_row6_col1\" class=\"data row6 col1\" >What measure is applied to test the proof in the context?</td>\n",
       "      <td id=\"T_f5051_row6_col2\" class=\"data row6 col2\" >NegativeLogLikelihood.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f5051_row7_col0\" class=\"data row7 col0\" >Information theory is a field of study about encoding, decoding, transmitting, and manipulating information. Entropy is the unit to measure how much information is presented in different signals. KL divergence can also measure the divergence between two distributions. Cross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.</td>\n",
       "      <td id=\"T_f5051_row7_col1\" class=\"data row7 col1\" >What is the objective of minimizing cross-entropy loss in multi-class classification?</td>\n",
       "      <td id=\"T_f5051_row7_col2\" class=\"data row7 col2\" >Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f5051_row8_col0\" class=\"data row8 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_f5051_row8_col1\" class=\"data row8 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 article?</td>\n",
       "      <td id=\"T_f5051_row8_col2\" class=\"data row8 col2\" >Information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5051_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f5051_row9_col0\" class=\"data row9 col0\" >22.11. Information Theory\n",
       "22.11.1. Information\n",
       "22.11.1.1. Self-information\n",
       "22.11.2. Entropy\n",
       "22.11.2.1. Motivating Entropy\n",
       "22.11.2.2. Definition\n",
       "22.11.2.3. Interpretations\n",
       "22.11.2.4. Properties of Entropy\n",
       "22.11.3. Mutual Information\n",
       "22.11.3.1. Joint Entropy\n",
       "22.11.3.2. Conditional Entropy\n",
       "22.11.3.3. Mutual Information\n",
       "22.11.3.4. Properties of Mutual Information\n",
       "22.11.3.5. Pointwise Mutual Information\n",
       "22.11.3.6. Applications of Mutual Information\n",
       "22.11.4. Kullback–Leibler Divergence\n",
       "22.11.4.1. Definition\n",
       "22.11.4.2. KL Divergence Properties\n",
       "22.11.4.3. Example\n",
       "22.11.5. Cross-Entropy\n",
       "22.11.5.1. Formal Definition\n",
       "22.11.5.2. Properties\n",
       "22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\n",
       "22.11.6. Summary\n",
       "22.11.7. Exercises</td>\n",
       "      <td id=\"T_f5051_row9_col1\" class=\"data row9 col1\" >What concept serves as an objective function for multi-class classification according to the context?</td>\n",
       "      <td id=\"T_f5051_row9_col2\" class=\"data row9 col2\" >Cross-Entropy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7feb1f6e5120>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting context, question, and answer into a DataFrame\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in output:\n",
    "    for i in item['output']:\n",
    "        for response in i['response']:\n",
    "            contexts.append(response['context'])\n",
    "            questions.append(response['question'])\n",
    "            answers.append(response['answer'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)  # or use a specific width like 50\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "styled_df = df.style.set_properties(**{'text-align': 'left'}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('text-align', 'left')]\n",
    "}])\n",
    "styled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
