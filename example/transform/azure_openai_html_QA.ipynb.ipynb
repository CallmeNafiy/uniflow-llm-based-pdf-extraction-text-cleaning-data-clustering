{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of generating QAs for an ML book using Azure OpenAI\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to have the following packages installed:\n",
    "```\n",
    "    pip install langchain pandas unstructured\n",
    "```\n",
    "\n",
    "Also, make sure you have a .env file with your following parameter values in the root directory of this project\n",
    "```\n",
    "    api_key=\"YOUR_API_KEY\"\n",
    "    endpoint=\"YOUR_END_POINT\"\n",
    "    deployment_id=\"YOUR_DEPLOYMENT_ID\"\n",
    "    model_version=\"YOUR_MODEL_VERSION\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import ExtractClient, TransformClient\n",
    "from uniflow.flow.config import ExtractHTMLConfig, TransformAzureOpenAIConfig\n",
    "from uniflow.flow.flow_factory import FlowFactory\n",
    "from uniflow.op.model.model_config import AzureOpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extract': ['ExtractHTMLFlow',\n",
       "  'ExtractImageFlow',\n",
       "  'ExtractIpynbFlow',\n",
       "  'ExtractMarkdownFlow',\n",
       "  'ExtractPDFFlow',\n",
       "  'ExtractTxtFlow'],\n",
       " 'transform': ['TransformAzureOpenAIFlow',\n",
       "  'TransformCopyFlow',\n",
       "  'TransformHuggingFaceFlow',\n",
       "  'TransformLMQGFlow',\n",
       "  'TransformOpenAIFlow'],\n",
       " 'rater': ['RaterFlow']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlowFactory.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file = \"22.11_information-theory.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set current directory and input data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the html file via ExtractClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [{\"filename\": input_file}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_client = ExtractClient(ExtractHTMLConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_output = extract_client.run(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "        instruction=\"Generate one question and its corresponding answer based on context. Following the format of the examples below to include the same context, question, and answer in the response.\",\n",
    "        few_shot_prompt=[\n",
    "            Context(\n",
    "                context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "                question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "                answer=\"Claude E. Shannon.\",\n",
    "            )\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ Context(context=p) for p in extract_output[0]['output'][0]['text'] if len(p) > 200 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ModelFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformAzureOpenAIConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=AzureOpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:04<00:58,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1Xq6TdHaVCelyTOG8qM2mXErSqE', 'object': 'chat.completion', 'created': 1709077982, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of\\\\nCommunication (Shannon, 1948) establishing the theory of\\\\ninformation. In his article, Shannon introduced the concept of\\\\ninformation entropy for the first time. We will begin our journey here.\", \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 article?\", \"answer\": \"The concept of information entropy.\"}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 1975, 'completion_tokens': 93, 'total_tokens': 2068}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:11<01:19,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1XuC7LlLg62S63JXm76LhXzi7Zz', 'object': 'chat.completion', 'created': 1709077986, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 article?\",\\n  \"answer\": \"Information entropy.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 1975, 'completion_tokens': 91, 'total_tokens': 2066}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:17<01:12,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1Y2bww7TI1kkQMt3vz7SdUJdlef', 'object': 'chat.completion', 'created': 1709077994, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"The universe is overflowing with information. Information provides a common language across disciplinary rifts: from Shakespeare’s Sonnet to researchers’ paper on Cornell ArXiv, from Van Gogh’s printing Starry Night to Beethoven’s music Symphony No. 5, from the first programming language Plankalkül to the state-of-the-art machine learning algorithms. Everything must follow the rules of information theory, no matter the format. With information theory, we can measure and compare how much information is present in different signals. In this section, we will investigate the fundamental concepts of information theory and applications of information theory in machine learning.\",\\n  \"question\": \"What is the common language that connects different disciplines according to the context?\",\\n  \"answer\": \"Information.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 301, 'completion_tokens': 158, 'total_tokens': 459}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:20<00:54,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1Y8EukJtl9AmS2DUS91LGEncB4b', 'object': 'chat.completion', 'created': 1709078000, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Consider the following thought experiment. We have a friend with a deck of cards. They will shuffle the deck, flip over some cards, and tell us statements about the cards. We will try to assess the information content of each statement.\",\\n  \"question\": \"What is the purpose of the thought experiment with a deck of cards?\",\\n  \"answer\": \"To assess the information content of statements about the cards.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 199, 'completion_tokens': 89, 'total_tokens': 288}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:27<00:55,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1YB5LrGbUzFkR95syPjPg11Z1y0', 'object': 'chat.completion', 'created': 1709078003, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"If we read through these thought experiments, we see a natural idea. As a starting point, rather than caring about the knowledge, we may build off the idea that information represents the degree of surprise or the abstract possibility of the event. For example, if we want to describe an unusual event, we need a lot information. For a common event, we may not need much information.\",\\n  \"question\": \"What does information represent according to the thought experiments?\",\\n  \"answer\": \"Information represents the degree of surprise or the abstract possibility of the event.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 231, 'completion_tokens': 118, 'total_tokens': 349}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:32<00:48,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1YIF6EzEJuutFt2pdBuRVGgxQY2', 'object': 'chat.completion', 'created': 1709078010, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"The information we gain by observing a random variable does not depend on what we call the elements, or the presence of additional elements which have probability zero.\\\\nThe information we gain by observing two random variables is no more than the sum of the information we gain by observing them separately.\\\\nIf they are independent, then it is exactly the sum.\\\\nThe information gained when observing (nearly) certain events is (nearly) zero.\",\\n  \"question\": \"Does the information gained from observing two independent random variables equal the sum of the information gained from observing each one separately?\",\\n  \"answer\": \"Yes.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 240, 'completion_tokens': 128, 'total_tokens': 368}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:36<00:40,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1YNGp0ikGdZPDhVCICjRWqgrEdk', 'object': 'chat.completion', 'created': 1709078015, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"While proving this fact is beyond the scope of our text, it is important to know that this uniquely determines the form that entropy must take. The only ambiguity that these allow is in the choice of fundamental units, which is most often normalized by making the choice we saw before that the information provided by a single fair coin flip is one bit.\",\\n  \"question\": \"What unit is often used as the fundamental unit of information?\",\\n  \"answer\": \"One bit.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 221, 'completion_tokens': 100, 'total_tokens': 321}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:41<00:34,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1YRvZ1esSVRqhl6o1QwQorCBXJB', 'object': 'chat.completion', 'created': 1709078019, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"If P is a continuous random variable, then the story becomes much more complicated. However, if we additionally impose that P is supported on a finite interval (with all values between 0 and 1), then P has the highest entropy if it is the uniform distribution on that interval.\",\\n  \"question\": \"Under what condition does a continuous random variable have the highest entropy?\",\\n  \"answer\": \"A continuous random variable has the highest entropy if it is supported on a finite interval (with all values between 0 and 1) and is the uniform distribution on that interval.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 604, 'completion_tokens': 123, 'total_tokens': 727}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:52<00:40,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1YW0FW81m8qiO2BVxupgYygNPgk', 'object': 'chat.completion', 'created': 1709078024, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Mutual information is symmetric, i.e.,\\\\n\\\\\\\\(I(X, Y) = I(Y, X)\\\\\\\\)\\\\n.\\\\nMutual information is non-negative, i.e.,\\\\n\\\\\\\\(I(X, Y) \\\\\\\\geq 0\\\\\\\\)\\\\n.\\\\n\\\\\\\\(I(X, Y) = 0\\\\\\\\)\\\\nif and only if\\\\n\\\\\\\\(X\\\\\\\\)\\\\nand\\\\n\\\\\\\\(Y\\\\\\\\)\\\\nare\\\\nindependent. For example, if\\\\n\\\\\\\\(X\\\\\\\\)\\\\nand\\\\n\\\\\\\\(Y\\\\\\\\)\\\\nare independent,\\\\nthen knowing\\\\n\\\\\\\\(Y\\\\\\\\)\\\\ndoes not give any information about\\\\n\\\\\\\\(X\\\\\\\\)\\\\nand vice versa, so their mutual information is zero.\\\\nAlternatively, if\\\\n\\\\\\\\(X\\\\\\\\)\\\\nis an invertible function of\\\\n\\\\\\\\(Y\\\\\\\\)\\\\n,\\\\nthen\\\\n\\\\\\\\(Y\\\\\\\\)\\\\nand\\\\n\\\\\\\\(X\\\\\\\\)\\\\nshare all information and\\\\n\\\\\\\\[I(X, Y) = H(Y) = H(X).\\\\\\\\]\",\\n  \"question\": \"What does it mean when the mutual information \\\\\\\\(I(X, Y)\\\\\\\\) between two variables X and Y is zero?\",\\n  \"answer\": \"It means that X and Y are independent, implying that knowing Y does not give any information about X and vice versa.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 376, 'completion_tokens': 281, 'total_tokens': 657}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO [abs_llm_processor]: Attempt 1 failed, retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1YhWZsPxIbRWH5qKWRsy5Y0rZQ5', 'object': 'chat.completion', 'created': 1709078035, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate \\\\\\\\\"\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        '}, 'logprobs': None}], 'usage': {'prompt_tokens': 285, 'completion_tokens': 691, 'total_tokens': 976}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [04:00<05:13, 62.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1bcEteVVLOHopLtHDsZKbMwOdPA', 'object': 'chat.completion', 'created': 1709078216, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate “Amazon”, we can compare which group has more occurrence in the context of the word Amazon. In this case the article would go on to describe the forest, and make the context clear.\",\\n  \"question\": \"What method is used to disambiguate the word \\'Amazon\\'?\",\\n  \"answer\": \"Mutual information is used to disambiguate the word \\'Amazon\\'.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 285, 'completion_tokens': 168, 'total_tokens': 453}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [04:30<03:31, 52.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1bi2S5dtZap6GLHAZecDlu35B6D', 'object': 'chat.completion', 'created': 1709078222, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"KL divergence is non-symmetric, i.e., there are\\\\n\\\\\\\\(P,Q\\\\\\\\)\\\\nsuch that\\\\n(22.11.22)\\\\n\\\\n\\\\\\\\[D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q) \\\\\\\\neq D_{\\\\\\\\textrm{KL}}(Q\\\\\\\\|P).\\\\\\\\]\\\\nKL divergence is non-negative, i.e.,\\\\n(22.11.23)\\\\n\\\\n\\\\\\\\[D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q) \\\\\\\\geq 0.\\\\\\\\]\\\\nNote that the equality holds only when\\\\n\\\\\\\\(P = Q\\\\\\\\)\\\\n.\\\\nIf there exists an\\\\n\\\\\\\\(x\\\\\\\\)\\\\nsuch that\\\\n\\\\\\\\(p(x) > 0\\\\\\\\)\\\\nand\\\\n\\\\\\\\(q(x) = 0\\\\\\\\)\\\\n, then\\\\n\\\\\\\\(D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q) = \\\\\\\\infty\\\\\\\\)\\\\n.\\\\nThere is a close relationship between KL divergence and mutual\\\\ninformation. Besides the relationship shown in\\\\nFig. 22.11.1\\\\n,\\\\n\\\\\\\\(I(X, Y)\\\\\\\\)\\\\nis also\\\\nnumerically equivalent with the following terms:\\\\n\\\\\\\\(D_{\\\\\\\\textrm{KL}}(P(X, Y) \\\\\\\\ \\\\\\\\| \\\\\\\\ P(X)P(Y))\\\\\\\\)\\\\n;\\\\n\\\\\\\\(E_Y \\\\\\\\{ D_{\\\\\\\\textrm{KL}}(P(X \\\\\\\\mid Y) \\\\\\\\ \\\\\\\\| \\\\\\\\ P(X)) \\\\\\\\}\\\\\\\\)\\\\n;\\\\n\\\\\\\\(E_X \\\\\\\\{ D_{\\\\\\\\textrm{KL}}(P(Y \\\\\\\\mid X) \\\\\\\\ \\\\\\\\| \\\\\\\\ P(Y)) \\\\\\\\}\\\\\\\\)\\\\n.\\\\nFor the first term, we interpret mutual information as the KL\\\\ndivergence between\\\\n\\\\\\\\(P(X, Y)\\\\\\\\)\\\\nand the product of\\\\n\\\\\\\\(P(X)\\\\\\\\)\\\\nand\\\\n\\\\\\\\(P(Y)\\\\\\\\)\\\\n, and thus is a measure of how different the joint\\\\ndistribution is from the distribution if they were independent. For\\\\nthe second term, mutual information tells us the average reduction in\\\\nuncertainty about\\\\n\\\\\\\\(Y\\\\\\\\)\\\\nthat results from learning the value of\\\\nthe\\\\n\\\\\\\\(X\\\\\\\\)\\\\n’s distribution. Similarly to the third term.\",\\n  \"question\": \"What does mutual information interpret in terms of KL divergence according to the first term?\",\\n  \"answer\": \"Mutual information is interpreted as the KL divergence between the joint distribution \\\\\\\\(P(X, Y)\\\\\\\\) and the product of \\\\\\\\(P(X)\\\\\\\\) and \\\\\\\\(P(Y)\\\\\\\\), measuring how different the joint distribution is from the distribution if X and Y were independent.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 638, 'completion_tokens': 569, 'total_tokens': 1207}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [04:38<01:56, 38.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1cDLNZ83sv7P3S4LJfKYaHugUmH', 'object': 'chat.completion', 'created': 1709078253, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Maximizing predictive probability of\\\\n\\\\\\\\(Q\\\\\\\\)\\\\nfor distribution\\\\n\\\\\\\\(P\\\\\\\\)\\\\n, (i.e.,\\\\n\\\\\\\\(E_{x \\\\\\\\sim P} [\\\\\\\\log (q(x))]\\\\\\\\)\\\\n);\\\\nMinimizing cross-entropy\\\\n\\\\\\\\(\\\\\\\\textrm{CE} (P, Q)\\\\\\\\)\\\\n;\\\\nMinimizing the KL divergence\\\\n\\\\\\\\(D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q)\\\\\\\\)\\\\n.\",\\n  \"question\": \"What are the three objectives mentioned in the context?\",\\n  \"answer\": \"Maximizing predictive probability of Q for distribution P, minimizing cross-entropy CE (P, Q), and minimizing the KL divergence D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q).\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 253, 'completion_tokens': 173, 'total_tokens': 426}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [04:41<00:56, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1cKFkDrkjpo6fsTP5WL6DQ3vNWc', 'object': 'chat.completion', 'created': 1709078260, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Information theory is a field of study about encoding, decoding, transmitting, and manipulating information.\\\\nEntropy is the unit to measure how much information is presented in different signals.\\\\nKL divergence can also measure the divergence between two distributions.\\\\nCross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.\",\\n  \"question\": \"What is cross-entropy commonly used as in multi-class classification?\",\\n  \"answer\": \"An objective function.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 241, 'completion_tokens': 113, 'total_tokens': 354}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [04:45<00:20, 20.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1cOfMNWwwhyuUf4Ow2icOFvaN0T', 'object': 'chat.completion', 'created': 1709078264, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of\\\\nCommunication (Shannon, 1948) establishing the theory of\\\\ninformation. In his article, Shannon introduced the concept of\\\\ninformation entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 publication?\",\\n  \"answer\": \"Information entropy.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 756, 'completion_tokens': 94, 'total_tokens': 850}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:08<00:00, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8x1cSFnwh6h3Qqr03YAumFVDrKwuL', 'object': 'chat.completion', 'created': 1709078268, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"22.11. Information Theory\\\\n22.11.1. Information\\\\n22.11.1.1. Self-information\\\\n22.11.2. Entropy\\\\n22.11.2.1. Motivating Entropy\\\\n22.11.2.2. Definition\\\\n22.11.2.3. Interpretations\\\\n22.11.2.4. Properties of Entropy\\\\n22.11.3. Mutual Information\\\\n22.11.3.1. Joint Entropy\\\\n22.11.3.2. Conditional Entropy\\\\n22.11.3.3. Mutual Information\\\\n22.11.3.4. Properties of Mutual Information\\\\n22.11.3.5. Pointwise Mutual Information\\\\n22.11.3.6. Applications of Mutual Information\\\\n22.11.4. Kullback–Leibler Divergence\\\\n22.11.4.1. Definition\\\\n22.11.4.2. KL Divergence Properties\\\\n22.11.4.3. Example\\\\n22.11.5. Cross-Entropy\\\\n22.11.5.1. Formal Definition\\\\n22.11.5.2. Properties\\\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\\\n22.11.6. Summary\\\\n22.11.7. Exercises\",\\n  \"question\": \"What is the objective function in multi-class classification mentioned in the context?\",\\n  \"answer\": \"Cross-Entropy.\"\\n}'}, 'logprobs': None}], 'usage': {'prompt_tokens': 433, 'completion_tokens': 318, 'total_tokens': 751}, 'system_fingerprint': 'fp_8abb16fa4e'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format result into pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2523b th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_2523b_row0_col0, #T_2523b_row0_col1, #T_2523b_row0_col2, #T_2523b_row1_col0, #T_2523b_row1_col1, #T_2523b_row1_col2, #T_2523b_row2_col0, #T_2523b_row2_col1, #T_2523b_row2_col2, #T_2523b_row3_col0, #T_2523b_row3_col1, #T_2523b_row3_col2, #T_2523b_row4_col0, #T_2523b_row4_col1, #T_2523b_row4_col2, #T_2523b_row5_col0, #T_2523b_row5_col1, #T_2523b_row5_col2, #T_2523b_row6_col0, #T_2523b_row6_col1, #T_2523b_row6_col2, #T_2523b_row7_col0, #T_2523b_row7_col1, #T_2523b_row7_col2, #T_2523b_row8_col0, #T_2523b_row8_col1, #T_2523b_row8_col2, #T_2523b_row9_col0, #T_2523b_row9_col1, #T_2523b_row9_col2, #T_2523b_row10_col0, #T_2523b_row10_col1, #T_2523b_row10_col2, #T_2523b_row11_col0, #T_2523b_row11_col1, #T_2523b_row11_col2, #T_2523b_row12_col0, #T_2523b_row12_col1, #T_2523b_row12_col2, #T_2523b_row13_col0, #T_2523b_row13_col1, #T_2523b_row13_col2, #T_2523b_row14_col0, #T_2523b_row14_col1, #T_2523b_row14_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2523b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2523b_level0_col0\" class=\"col_heading level0 col0\" >context</th>\n",
       "      <th id=\"T_2523b_level0_col1\" class=\"col_heading level0 col1\" >question</th>\n",
       "      <th id=\"T_2523b_level0_col2\" class=\"col_heading level0 col2\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2523b_row0_col0\" class=\"data row0 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of\n",
       "Communication (Shannon, 1948) establishing the theory of\n",
       "information. In his article, Shannon introduced the concept of\n",
       "information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_2523b_row0_col1\" class=\"data row0 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 article?</td>\n",
       "      <td id=\"T_2523b_row0_col2\" class=\"data row0 col2\" >The concept of information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2523b_row1_col0\" class=\"data row1 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_2523b_row1_col1\" class=\"data row1 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 article?</td>\n",
       "      <td id=\"T_2523b_row1_col2\" class=\"data row1 col2\" >Information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2523b_row2_col0\" class=\"data row2 col0\" >The universe is overflowing with information. Information provides a common language across disciplinary rifts: from Shakespeare’s Sonnet to researchers’ paper on Cornell ArXiv, from Van Gogh’s printing Starry Night to Beethoven’s music Symphony No. 5, from the first programming language Plankalkül to the state-of-the-art machine learning algorithms. Everything must follow the rules of information theory, no matter the format. With information theory, we can measure and compare how much information is present in different signals. In this section, we will investigate the fundamental concepts of information theory and applications of information theory in machine learning.</td>\n",
       "      <td id=\"T_2523b_row2_col1\" class=\"data row2 col1\" >What is the common language that connects different disciplines according to the context?</td>\n",
       "      <td id=\"T_2523b_row2_col2\" class=\"data row2 col2\" >Information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2523b_row3_col0\" class=\"data row3 col0\" >Consider the following thought experiment. We have a friend with a deck of cards. They will shuffle the deck, flip over some cards, and tell us statements about the cards. We will try to assess the information content of each statement.</td>\n",
       "      <td id=\"T_2523b_row3_col1\" class=\"data row3 col1\" >What is the purpose of the thought experiment with a deck of cards?</td>\n",
       "      <td id=\"T_2523b_row3_col2\" class=\"data row3 col2\" >To assess the information content of statements about the cards.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_2523b_row4_col0\" class=\"data row4 col0\" >If we read through these thought experiments, we see a natural idea. As a starting point, rather than caring about the knowledge, we may build off the idea that information represents the degree of surprise or the abstract possibility of the event. For example, if we want to describe an unusual event, we need a lot information. For a common event, we may not need much information.</td>\n",
       "      <td id=\"T_2523b_row4_col1\" class=\"data row4 col1\" >What does information represent according to the thought experiments?</td>\n",
       "      <td id=\"T_2523b_row4_col2\" class=\"data row4 col2\" >Information represents the degree of surprise or the abstract possibility of the event.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_2523b_row5_col0\" class=\"data row5 col0\" >The information we gain by observing a random variable does not depend on what we call the elements, or the presence of additional elements which have probability zero.\n",
       "The information we gain by observing two random variables is no more than the sum of the information we gain by observing them separately.\n",
       "If they are independent, then it is exactly the sum.\n",
       "The information gained when observing (nearly) certain events is (nearly) zero.</td>\n",
       "      <td id=\"T_2523b_row5_col1\" class=\"data row5 col1\" >Does the information gained from observing two independent random variables equal the sum of the information gained from observing each one separately?</td>\n",
       "      <td id=\"T_2523b_row5_col2\" class=\"data row5 col2\" >Yes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_2523b_row6_col0\" class=\"data row6 col0\" >While proving this fact is beyond the scope of our text, it is important to know that this uniquely determines the form that entropy must take. The only ambiguity that these allow is in the choice of fundamental units, which is most often normalized by making the choice we saw before that the information provided by a single fair coin flip is one bit.</td>\n",
       "      <td id=\"T_2523b_row6_col1\" class=\"data row6 col1\" >What unit is often used as the fundamental unit of information?</td>\n",
       "      <td id=\"T_2523b_row6_col2\" class=\"data row6 col2\" >One bit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_2523b_row7_col0\" class=\"data row7 col0\" >If P is a continuous random variable, then the story becomes much more complicated. However, if we additionally impose that P is supported on a finite interval (with all values between 0 and 1), then P has the highest entropy if it is the uniform distribution on that interval.</td>\n",
       "      <td id=\"T_2523b_row7_col1\" class=\"data row7 col1\" >Under what condition does a continuous random variable have the highest entropy?</td>\n",
       "      <td id=\"T_2523b_row7_col2\" class=\"data row7 col2\" >A continuous random variable has the highest entropy if it is supported on a finite interval (with all values between 0 and 1) and is the uniform distribution on that interval.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_2523b_row8_col0\" class=\"data row8 col0\" >Mutual information is symmetric, i.e.,\n",
       "\\(I(X, Y) = I(Y, X)\\)\n",
       ".\n",
       "Mutual information is non-negative, i.e.,\n",
       "\\(I(X, Y) \\geq 0\\)\n",
       ".\n",
       "\\(I(X, Y) = 0\\)\n",
       "if and only if\n",
       "\\(X\\)\n",
       "and\n",
       "\\(Y\\)\n",
       "are\n",
       "independent. For example, if\n",
       "\\(X\\)\n",
       "and\n",
       "\\(Y\\)\n",
       "are independent,\n",
       "then knowing\n",
       "\\(Y\\)\n",
       "does not give any information about\n",
       "\\(X\\)\n",
       "and vice versa, so their mutual information is zero.\n",
       "Alternatively, if\n",
       "\\(X\\)\n",
       "is an invertible function of\n",
       "\\(Y\\)\n",
       ",\n",
       "then\n",
       "\\(Y\\)\n",
       "and\n",
       "\\(X\\)\n",
       "share all information and\n",
       "\\[I(X, Y) = H(Y) = H(X).\\]</td>\n",
       "      <td id=\"T_2523b_row8_col1\" class=\"data row8 col1\" >What does it mean when the mutual information \\(I(X, Y)\\) between two variables X and Y is zero?</td>\n",
       "      <td id=\"T_2523b_row8_col2\" class=\"data row8 col2\" >It means that X and Y are independent, implying that knowing Y does not give any information about X and vice versa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_2523b_row9_col0\" class=\"data row9 col0\" >In this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate “Amazon”, we can compare which group has more occurrence in the context of the word Amazon. In this case the article would go on to describe the forest, and make the context clear.</td>\n",
       "      <td id=\"T_2523b_row9_col1\" class=\"data row9 col1\" >What method is used to disambiguate the word 'Amazon'?</td>\n",
       "      <td id=\"T_2523b_row9_col2\" class=\"data row9 col2\" >Mutual information is used to disambiguate the word 'Amazon'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_2523b_row10_col0\" class=\"data row10 col0\" >KL divergence is non-symmetric, i.e., there are\n",
       "\\(P,Q\\)\n",
       "such that\n",
       "(22.11.22)\n",
       "\n",
       "\\[D_{\\textrm{KL}}(P\\|Q) \\neq D_{\\textrm{KL}}(Q\\|P).\\]\n",
       "KL divergence is non-negative, i.e.,\n",
       "(22.11.23)\n",
       "\n",
       "\\[D_{\\textrm{KL}}(P\\|Q) \\geq 0.\\]\n",
       "Note that the equality holds only when\n",
       "\\(P = Q\\)\n",
       ".\n",
       "If there exists an\n",
       "\\(x\\)\n",
       "such that\n",
       "\\(p(x) > 0\\)\n",
       "and\n",
       "\\(q(x) = 0\\)\n",
       ", then\n",
       "\\(D_{\\textrm{KL}}(P\\|Q) = \\infty\\)\n",
       ".\n",
       "There is a close relationship between KL divergence and mutual\n",
       "information. Besides the relationship shown in\n",
       "Fig. 22.11.1\n",
       ",\n",
       "\\(I(X, Y)\\)\n",
       "is also\n",
       "numerically equivalent with the following terms:\n",
       "\\(D_{\\textrm{KL}}(P(X, Y) \\ \\| \\ P(X)P(Y))\\)\n",
       ";\n",
       "\\(E_Y \\{ D_{\\textrm{KL}}(P(X \\mid Y) \\ \\| \\ P(X)) \\}\\)\n",
       ";\n",
       "\\(E_X \\{ D_{\\textrm{KL}}(P(Y \\mid X) \\ \\| \\ P(Y)) \\}\\)\n",
       ".\n",
       "For the first term, we interpret mutual information as the KL\n",
       "divergence between\n",
       "\\(P(X, Y)\\)\n",
       "and the product of\n",
       "\\(P(X)\\)\n",
       "and\n",
       "\\(P(Y)\\)\n",
       ", and thus is a measure of how different the joint\n",
       "distribution is from the distribution if they were independent. For\n",
       "the second term, mutual information tells us the average reduction in\n",
       "uncertainty about\n",
       "\\(Y\\)\n",
       "that results from learning the value of\n",
       "the\n",
       "\\(X\\)\n",
       "’s distribution. Similarly to the third term.</td>\n",
       "      <td id=\"T_2523b_row10_col1\" class=\"data row10 col1\" >What does mutual information interpret in terms of KL divergence according to the first term?</td>\n",
       "      <td id=\"T_2523b_row10_col2\" class=\"data row10 col2\" >Mutual information is interpreted as the KL divergence between the joint distribution \\(P(X, Y)\\) and the product of \\(P(X)\\) and \\(P(Y)\\), measuring how different the joint distribution is from the distribution if X and Y were independent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_2523b_row11_col0\" class=\"data row11 col0\" >Maximizing predictive probability of\n",
       "\\(Q\\)\n",
       "for distribution\n",
       "\\(P\\)\n",
       ", (i.e.,\n",
       "\\(E_{x \\sim P} [\\log (q(x))]\\)\n",
       ");\n",
       "Minimizing cross-entropy\n",
       "\\(\\textrm{CE} (P, Q)\\)\n",
       ";\n",
       "Minimizing the KL divergence\n",
       "\\(D_{\\textrm{KL}}(P\\|Q)\\)\n",
       ".</td>\n",
       "      <td id=\"T_2523b_row11_col1\" class=\"data row11 col1\" >What are the three objectives mentioned in the context?</td>\n",
       "      <td id=\"T_2523b_row11_col2\" class=\"data row11 col2\" >Maximizing predictive probability of Q for distribution P, minimizing cross-entropy CE (P, Q), and minimizing the KL divergence D_{\\textrm{KL}}(P\\|Q).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_2523b_row12_col0\" class=\"data row12 col0\" >Information theory is a field of study about encoding, decoding, transmitting, and manipulating information.\n",
       "Entropy is the unit to measure how much information is presented in different signals.\n",
       "KL divergence can also measure the divergence between two distributions.\n",
       "Cross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.</td>\n",
       "      <td id=\"T_2523b_row12_col1\" class=\"data row12 col1\" >What is cross-entropy commonly used as in multi-class classification?</td>\n",
       "      <td id=\"T_2523b_row12_col2\" class=\"data row12 col2\" >An objective function.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_2523b_row13_col0\" class=\"data row13 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of\n",
       "Communication (Shannon, 1948) establishing the theory of\n",
       "information. In his article, Shannon introduced the concept of\n",
       "information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_2523b_row13_col1\" class=\"data row13 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 publication?</td>\n",
       "      <td id=\"T_2523b_row13_col2\" class=\"data row13 col2\" >Information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2523b_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_2523b_row14_col0\" class=\"data row14 col0\" >22.11. Information Theory\n",
       "22.11.1. Information\n",
       "22.11.1.1. Self-information\n",
       "22.11.2. Entropy\n",
       "22.11.2.1. Motivating Entropy\n",
       "22.11.2.2. Definition\n",
       "22.11.2.3. Interpretations\n",
       "22.11.2.4. Properties of Entropy\n",
       "22.11.3. Mutual Information\n",
       "22.11.3.1. Joint Entropy\n",
       "22.11.3.2. Conditional Entropy\n",
       "22.11.3.3. Mutual Information\n",
       "22.11.3.4. Properties of Mutual Information\n",
       "22.11.3.5. Pointwise Mutual Information\n",
       "22.11.3.6. Applications of Mutual Information\n",
       "22.11.4. Kullback–Leibler Divergence\n",
       "22.11.4.1. Definition\n",
       "22.11.4.2. KL Divergence Properties\n",
       "22.11.4.3. Example\n",
       "22.11.5. Cross-Entropy\n",
       "22.11.5.1. Formal Definition\n",
       "22.11.5.2. Properties\n",
       "22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\n",
       "22.11.6. Summary\n",
       "22.11.7. Exercises</td>\n",
       "      <td id=\"T_2523b_row14_col1\" class=\"data row14 col1\" >What is the objective function in multi-class classification mentioned in the context?</td>\n",
       "      <td id=\"T_2523b_row14_col2\" class=\"data row14 col2\" >Cross-Entropy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fd7e9097100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting context, question, and answer into a DataFrame\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in output:\n",
    "    for i in item['output']:\n",
    "        for response in i['response']:\n",
    "            contexts.append(response['context'])\n",
    "            questions.append(response['question'])\n",
    "            answers.append(response['answer'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)  # or use a specific width like 50\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "styled_df = df.style.set_properties(**{'text-align': 'left'}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('text-align', 'left')]\n",
    "}])\n",
    "styled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
