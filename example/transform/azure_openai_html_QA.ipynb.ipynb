{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of generating QAs for an ML book using Azure OpenAI\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to have the following packages installed:\n",
    "```\n",
    "    pip install langchain pandas unstructured\n",
    "```\n",
    "\n",
    "Also, make sure you have a .env file with your following parameter values in the root directory of this project\n",
    "```\n",
    "    api_key=\"YOUR_API_KEY\"\n",
    "    endpoint=\"YOUR_END_POINT\"\n",
    "    deployment_id=\"YOUR_DEPLOYMENT_ID\"\n",
    "    model_version=\"YOUR_MODEL_VERSION\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from uniflow.flow.client import ExtractClient, TransformClient\n",
    "from uniflow.flow.config import ExtractHTMLConfig, TransformAzureOpenAIConfig\n",
    "from uniflow.flow.flow_factory import FlowFactory\n",
    "from uniflow.op.model.model_config import AzureOpenAIModelConfig\n",
    "from uniflow.op.prompt import Context, PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extract': ['ExtractHTMLFlow',\n",
       "  'ExtractImageFlow',\n",
       "  'ExtractIpynbFlow',\n",
       "  'ExtractMarkdownFlow',\n",
       "  'ExtractPDFFlow',\n",
       "  'ExtractTxtFlow'],\n",
       " 'transform': ['TransformAzureOpenAIFlow',\n",
       "  'TransformCopyFlow',\n",
       "  'TransformHuggingFaceFlow',\n",
       "  'TransformLMQGFlow',\n",
       "  'TransformOpenAIFlow'],\n",
       " 'rater': ['RaterFlow']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlowFactory.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file = \"22.11_information-theory.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set current directory and input data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()\n",
    "input_file = os.path.join(f\"{dir_cur}/data/raw_input/\", html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the html file via ExtractClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [{\"filename\": input_file}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_client = ExtractClient(ExtractHTMLConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_output = extract_client.run(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_prompt = PromptTemplate(\n",
    "        instruction=\"Generate one question and its corresponding answer based on context. Following the format of the examples below to include the same context, question, and answer in the response.\",\n",
    "        few_shot_prompt=[\n",
    "            Context(\n",
    "                context=\"In 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\",\n",
    "                question=\"Who published A Mathematical Theory of Communication in 1948?\",\n",
    "                answer=\"Claude E. Shannon.\",\n",
    "            )\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ Context(context=p) for p in extract_output[0]['output'][0]['text'] if len(p) > 200 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ModelFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformAzureOpenAIConfig(\n",
    "    prompt_template=guided_prompt,\n",
    "    model_config=AzureOpenAIModelConfig(response_format={\"type\": \"json_object\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TransformClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:07<01:40,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjCykr1ujfPvP3XQRKSf2I83lUyd', 'object': 'chat.completion', 'created': 1709007496, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 article?\",\\n  \"answer\": \"The concept of information entropy.\"\\n}'}}], 'usage': {'prompt_tokens': 1975, 'completion_tokens': 94, 'total_tokens': 2069}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:14<01:32,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjD5v4vDw2LorCVeCevJEWoaRPSu', 'object': 'chat.completion', 'created': 1709007503, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 article?\",\\n  \"answer\": \"Information entropy.\"\\n}'}}], 'usage': {'prompt_tokens': 1975, 'completion_tokens': 91, 'total_tokens': 2066}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:21<01:27,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDCxOxdxQomnShEwcid7korG3VA', 'object': 'chat.completion', 'created': 1709007510, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"The universe is overflowing with information. Information provides a common language across disciplinary rifts: from Shakespeare\\'s Sonnet to researchers\\' paper on Cornell ArXiv, from Van Gogh\\'s printing Starry Night to Beethoven\\'s music Symphony No.\\\\n5, from the first programming language Plankalkül to the state-of-the-art machine learning algorithms. Everything must follow the rules of information theory, no matter the format. With information theory, we can measure and compare how much information is present in different signals. In this section, we will investigate the fundamental concepts of information theory and applications of information theory in machine learning.\",\\n  \"question\": \"What provides a common language across disciplinary rifts according to the context?\",\\n  \"answer\": \"Information provides a common language across disciplinary rifts.\"\\n}'}}], 'usage': {'prompt_tokens': 301, 'completion_tokens': 166, 'total_tokens': 467}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:26<01:09,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDKvwFrKtpW0aTBdClJoCv0beqO', 'object': 'chat.completion', 'created': 1709007518, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Consider the following thought experiment. We have a friend with a deck of cards. They will shuffle the deck, flip over some cards, and tell us statements about the cards. We will try to assess the information content of each statement.\",\\n  \"question\": \"What is the purpose of the thought experiment with a deck of cards?\",\\n  \"answer\": \"To assess the information content of each statement made about the cards.\"\\n}'}}], 'usage': {'prompt_tokens': 199, 'completion_tokens': 91, 'total_tokens': 290}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:30<00:55,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDPVRQZ2gn6VnWHbpyHLZOipelh', 'object': 'chat.completion', 'created': 1709007523, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"If we read through these thought experiments, we see a natural idea. As a starting point, rather than caring about the knowledge, we may build off the idea that information represents the degree of surprise or the abstract possibility of the event. For example, if we want to describe an unusual event, we need a lot information. For a common event, we may not need much information.\",\\n  \"question\": \"What does information represent according to the thought experiments?\",\\n  \"answer\": \"Information represents the degree of surprise or the abstract possibility of the event.\"\\n}'}}], 'usage': {'prompt_tokens': 231, 'completion_tokens': 118, 'total_tokens': 349}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:37<00:52,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDTOlQzoT4VhYvY96bZflNrtV0w', 'object': 'chat.completion', 'created': 1709007527, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"The information we gain by observing a random variable does not\\\\ndepend on what we call the elements, or the presence of additional\\\\nelements which have probability zero.\\\\nThe information we gain by observing two random variables is no more\\\\nthan the sum of the information we gain by observing them separately.\\\\nIf they are independent, then it is exactly the sum.\\\\nThe information gained when observing (nearly) certain events is\\\\n(nearly) zero.\",\\n  \"question\": \"Does the information gained when observing (nearly) certain events have any significant value?\",\\n  \"answer\": \"No, the information gained when observing (nearly) certain events is (nearly) zero.\"\\n}'}}], 'usage': {'prompt_tokens': 240, 'completion_tokens': 145, 'total_tokens': 385}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:40<00:40,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDZynsskrA8YE1m7RaIZHIoaN98', 'object': 'chat.completion', 'created': 1709007533, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"While proving this fact is beyond the scope of our text, it is important\\\\nto know that this uniquely determines the form that entropy must take.\\\\nThe only ambiguity that these allow is in the choice of fundamental\\\\nunits, which is most often normalized by making the choice we saw before\\\\nthat the information provided by a single fair coin flip is one bit.\",\\n  \"question\": \"What is the information provided by a single fair coin flip normalized to?\",\\n  \"answer\": \"One bit.\"\\n}'}}], 'usage': {'prompt_tokens': 221, 'completion_tokens': 106, 'total_tokens': 327}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:43<00:31,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDdfuVcyAriOYGrZASlAsUlT03R', 'object': 'chat.completion', 'created': 1709007537, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 publication?\",\\n  \"answer\": \"The concept of information entropy.\"\\n}'}}], 'usage': {'prompt_tokens': 604, 'completion_tokens': 94, 'total_tokens': 698}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:51<00:31,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDgoBGJZW3LS7BqksEAvfydg3rs', 'object': 'chat.completion', 'created': 1709007540, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\"context\": \"In 1965, Gordon Moore, co-founder of Intel, observed that the number of transistors on a microchip doubles approximately every two years. This observation came to be known as Moore\\'s Law. It has had a profound impact on the development of computer technology, influencing the pace at which computing power has advanced over the years.\", \"question\": \"What observation did Gordon Moore make in 1965 that has impacted the development of computer technology?\", \"answer\": \"Gordon Moore observed that the number of transistors on a microchip doubles approximately every two years, an observation that came to be known as Moore\\'s Law.\"}'}}], 'usage': {'prompt_tokens': 376, 'completion_tokens': 133, 'total_tokens': 509}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO [abs_llm_processor]: Attempt 1 failed, retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjDnHDv4boej0xHv5VVnwpxV3HlK', 'object': 'chat.completion', 'created': 1709007547, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate \\\\\\\\\"\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        '}}], 'usage': {'prompt_tokens': 285, 'completion_tokens': 691, 'total_tokens': 976}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [03:51<04:57, 59.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjGYEkkV1HYmp48QPtuXp5ZOPPiR', 'object': 'chat.completion', 'created': 1709007718, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate “Amazon”, we can compare which group has more occurrence in the context of the word Amazon. In this case the article would go on to describe the forest, and make the context clear.\",\\n  \"question\": \"How can mutual information be used to disambiguate the word \\'Amazon\\'?\",\\n  \"answer\": \"Mutual information can be used to disambiguate the word \\'Amazon\\' by comparing the occurrence of two groups of words; one group associated with the company Amazon (e.g., e-commerce, technology, online) and another group associated with the Amazon rain forest (e.g., rain, forest, tropical) in the given context.\"\\n}'}}], 'usage': {'prompt_tokens': 285, 'completion_tokens': 222, 'total_tokens': 507}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [04:11<03:09, 47.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjGiSLgmErSQsm7p7Wm18be2LpOY', 'object': 'chat.completion', 'created': 1709007728, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"KL divergence is non-symmetric, i.e., there are\\\\n\\\\\\\\(P,Q\\\\\\\\)\\\\nsuch that\\\\n(22.11.22)\\\\n\\\\n\\\\\\\\[D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q) \\\\\\\\neq D_{\\\\\\\\textrm{KL}}(Q\\\\\\\\|P).\\\\\\\\]\\\\nKL divergence is non-negative, i.e.,\\\\n(22.11.23)\\\\n\\\\n\\\\\\\\[D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q) \\\\\\\\geq 0.\\\\\\\\]\\\\nNote that the equality holds only when\\\\n\\\\\\\\(P = Q\\\\\\\\)\\\\n.\\\\nIf there exists an\\\\n\\\\\\\\(x\\\\\\\\)\\\\nsuch that\\\\n\\\\\\\\(p(x) > 0\\\\\\\\)\\\\nand\\\\n\\\\\\\\(q(x) = 0\\\\\\\\)\\\\n, then\\\\n\\\\\\\\(D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q) = \\\\\\\\infty\\\\\\\\)\\\\n.\\\\nThere is a close relationship between KL divergence and mutual\\\\ninformation. Besides the relationship shown in\\\\nFig. 22.11.1\\\\n,\\\\n\\\\\\\\(I(X, Y)\\\\\\\\)\\\\nis also\\\\nnumerically equivalent with the following terms:\\\\n\\\\\\\\(D_{\\\\\\\\textrm{KL}}(P(X, Y) \\\\\\\\ \\\\\\\\| \\\\\\\\ P(X)P(Y))\\\\\\\\)\\\\n;\\\\n\\\\\\\\(E_Y \\\\\\\\{ D_{\\\\\\\\textrm{KL}}(P(X \\\\\\\\mid Y) \\\\\\\\ \\\\\\\\| \\\\\\\\ P(X)) \\\\\\\\}\\\\\\\\)\\\\n;\\\\n\\\\\\\\(E_X \\\\\\\\{ D_{\\\\\\\\textrm{KL}}(P(Y \\\\\\\\mid X) \\\\\\\\ \\\\\\\\| \\\\\\\\ P(Y)) \\\\\\\\}\\\\\\\\)\\\\n.\\\\nFor the first term, we interpret mutual information as the KL\\\\ndivergence between\\\\n\\\\\\\\(P(X, Y)\\\\\\\\)\\\\nand the product of\\\\n\\\\\\\\(P(X)\\\\\\\\)\\\\nand\\\\n\\\\\\\\(P(Y)\\\\\\\\)\\\\n, and thus is a measure of how different the joint\\\\ndistribution is from the distribution if they were independent. For\\\\nthe second term, mutual information tells us the average reduction in\\\\nuncertainty about\\\\n\\\\\\\\(Y\\\\\\\\)\\\\nthat results from learning the value of\\\\nthe\\\\n\\\\\\\\(X\\\\\\\\)\\\\n’s distribution. Similarly to the third term.\",\\n  \"question\": \"What does KL divergence being non-symmetric imply about \\\\\\\\(P\\\\\\\\) and \\\\\\\\(Q\\\\\\\\)?\",\\n  \"answer\": \"It implies that \\\\\\\\(D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q)\\\\\\\\) is not necessarily equal to \\\\\\\\(D_{\\\\\\\\textrm{KL}}(Q\\\\\\\\|P)\\\\\\\\), indicating the measure of difference from \\\\\\\\(P\\\\\\\\) to \\\\\\\\(Q\\\\\\\\) is not the same as from \\\\\\\\(Q\\\\\\\\) to \\\\\\\\(P\\\\\\\\).\"\\n}'}}], 'usage': {'prompt_tokens': 638, 'completion_tokens': 598, 'total_tokens': 1236}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [04:24<01:50, 36.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjH2bg3RTHxWLkfWmOsWI4LCgZbx', 'object': 'chat.completion', 'created': 1709007748, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Maximizing predictive probability of\\\\n\\\\\\\\(Q\\\\\\\\)\\\\nfor distribution\\\\n\\\\\\\\(P\\\\\\\\)\\\\n, (i.e.,\\\\n\\\\\\\\(E_{x \\\\\\\\sim P} [\\\\\\\\log (q(x))]\\\\\\\\)\\\\n);\\\\nMinimizing cross-entropy\\\\n\\\\\\\\(\\\\\\\\textrm{CE} (P, Q)\\\\\\\\)\\\\n;\\\\nMinimizing the KL divergence\\\\n\\\\\\\\(D_{\\\\\\\\textrm{KL}}(P\\\\\\\\|Q)\\\\\\\\)\\\\n.\",\\n  \"question\": \"What are the goals mentioned in the context related to distribution \\\\\\\\(P\\\\\\\\) and \\\\\\\\(Q\\\\\\\\)?\",\\n  \"answer\": \"Maximizing predictive probability of \\\\\\\\(Q\\\\\\\\) for distribution \\\\\\\\(P\\\\\\\\), minimizing cross-entropy between \\\\\\\\(P\\\\\\\\) and \\\\\\\\(Q\\\\\\\\), and minimizing the KL divergence from \\\\\\\\(P\\\\\\\\) to \\\\\\\\(Q\\\\\\\\).\"\\n}'}}], 'usage': {'prompt_tokens': 253, 'completion_tokens': 196, 'total_tokens': 449}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [04:28<00:53, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjHFrf1vfDLcMlsgRHidGKGQR33S', 'object': 'chat.completion', 'created': 1709007761, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"Information theory is a field of study about encoding, decoding, transmitting, and manipulating information. Entropy is the unit to measure how much information is presented in different signals. KL divergence can also measure the divergence between two distributions. Cross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.\",\\n  \"question\": \"What is the purpose of minimizing cross-entropy loss in multi-class classification?\",\\n  \"answer\": \"Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.\"\\n}'}}], 'usage': {'prompt_tokens': 241, 'completion_tokens': 126, 'total_tokens': 367}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [04:31<00:19, 19.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjHJUMqAQQ8DBniPPTiIl9e8f8eq', 'object': 'chat.completion', 'created': 1709007765, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.\",\\n  \"question\": \"What concept did Claude E. Shannon introduce for the first time in his 1948 article?\",\\n  \"answer\": \"The concept of information entropy.\"\\n}'}}], 'usage': {'prompt_tokens': 756, 'completion_tokens': 94, 'total_tokens': 850}, 'system_fingerprint': 'fp_8abb16fa4e'}\n",
      "Making API call with data: {\"instruction\": \"Generate one question and its corresponding answer based on context. Following the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [04:45<00:00, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response: {'id': 'chatcmpl-8wjHMIXB2V8auTckBueC4nCvew8fg', 'object': 'chat.completion', 'created': 1709007768, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': '{\\n  \"context\": \"22.11. Information Theory\\\\n22.11.1. Information\\\\n22.11.1.1. Self-information\\\\n22.11.2. Entropy\\\\n22.11.2.1. Motivating Entropy\\\\n22.11.2.2. Definition\\\\n22.11.2.3. Interpretations\\\\n22.11.2.4. Properties of Entropy\\\\n22.11.3. Mutual Information\\\\n22.11.3.1. Joint Entropy\\\\n22.11.3.2. Conditional Entropy\\\\n22.11.3.3. Mutual Information\\\\n22.11.3.4. Properties of Mutual Information\\\\n22.11.3.5. Pointwise Mutual Information\\\\n22.11.3.6. Applications of Mutual Information\\\\n22.11.4. Kullback–Leibler Divergence\\\\n22.11.4.1. Definition\\\\n22.11.4.2. KL Divergence Properties\\\\n22.11.4.3. Example\\\\n22.11.5. Cross-Entropy\\\\n22.11.5.1. Formal Definition\\\\n22.11.5.2. Properties\\\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\\\n22.11.6. Summary\\\\n22.11.7. Exercises\",\\n  \"question\": \"What is the objective function of multi-class classification mentioned in the context?\",\\n  \"answer\": \"Cross-Entropy.\"\\n}'}}], 'usage': {'prompt_tokens': 433, 'completion_tokens': 318, 'total_tokens': 751}, 'system_fingerprint': 'fp_8abb16fa4e'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output': [{'response': [{'context': 'In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.',\n",
       "      'question': 'What concept did Claude E. Shannon introduce for the first time in his 1948 article?',\n",
       "      'answer': 'The concept of information entropy.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744081ba0>},\n",
       " {'output': [{'response': [{'context': 'In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.',\n",
       "      'question': 'What concept did Claude E. Shannon introduce for the first time in his 1948 article?',\n",
       "      'answer': 'Information entropy.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f37440818a0>},\n",
       " {'output': [{'response': [{'context': \"The universe is overflowing with information. Information provides a common language across disciplinary rifts: from Shakespeare's Sonnet to researchers' paper on Cornell ArXiv, from Van Gogh's printing Starry Night to Beethoven's music Symphony No.\\n5, from the first programming language Plankalkül to the state-of-the-art machine learning algorithms. Everything must follow the rules of information theory, no matter the format. With information theory, we can measure and compare how much information is present in different signals. In this section, we will investigate the fundamental concepts of information theory and applications of information theory in machine learning.\",\n",
       "      'question': 'What provides a common language across disciplinary rifts according to the context?',\n",
       "      'answer': 'Information provides a common language across disciplinary rifts.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744081e70>},\n",
       " {'output': [{'response': [{'context': 'Consider the following thought experiment. We have a friend with a deck of cards. They will shuffle the deck, flip over some cards, and tell us statements about the cards. We will try to assess the information content of each statement.',\n",
       "      'question': 'What is the purpose of the thought experiment with a deck of cards?',\n",
       "      'answer': 'To assess the information content of each statement made about the cards.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744081f30>},\n",
       " {'output': [{'response': [{'context': 'If we read through these thought experiments, we see a natural idea. As a starting point, rather than caring about the knowledge, we may build off the idea that information represents the degree of surprise or the abstract possibility of the event. For example, if we want to describe an unusual event, we need a lot information. For a common event, we may not need much information.',\n",
       "      'question': 'What does information represent according to the thought experiments?',\n",
       "      'answer': 'Information represents the degree of surprise or the abstract possibility of the event.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744081ff0>},\n",
       " {'output': [{'response': [{'context': 'The information we gain by observing a random variable does not\\ndepend on what we call the elements, or the presence of additional\\nelements which have probability zero.\\nThe information we gain by observing two random variables is no more\\nthan the sum of the information we gain by observing them separately.\\nIf they are independent, then it is exactly the sum.\\nThe information gained when observing (nearly) certain events is\\n(nearly) zero.',\n",
       "      'question': 'Does the information gained when observing (nearly) certain events have any significant value?',\n",
       "      'answer': 'No, the information gained when observing (nearly) certain events is (nearly) zero.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f37440820b0>},\n",
       " {'output': [{'response': [{'context': 'While proving this fact is beyond the scope of our text, it is important\\nto know that this uniquely determines the form that entropy must take.\\nThe only ambiguity that these allow is in the choice of fundamental\\nunits, which is most often normalized by making the choice we saw before\\nthat the information provided by a single fair coin flip is one bit.',\n",
       "      'question': 'What is the information provided by a single fair coin flip normalized to?',\n",
       "      'answer': 'One bit.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744082170>},\n",
       " {'output': [{'response': [{'context': 'In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.',\n",
       "      'question': 'What concept did Claude E. Shannon introduce for the first time in his 1948 publication?',\n",
       "      'answer': 'The concept of information entropy.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744082230>},\n",
       " {'output': [{'response': [{'context': \"In 1965, Gordon Moore, co-founder of Intel, observed that the number of transistors on a microchip doubles approximately every two years. This observation came to be known as Moore's Law. It has had a profound impact on the development of computer technology, influencing the pace at which computing power has advanced over the years.\",\n",
       "      'question': 'What observation did Gordon Moore make in 1965 that has impacted the development of computer technology?',\n",
       "      'answer': \"Gordon Moore observed that the number of transistors on a microchip doubles approximately every two years, an observation that came to be known as Moore's Law.\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f37440822f0>},\n",
       " {'output': [{'response': [{'context': 'In this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate “Amazon”, we can compare which group has more occurrence in the context of the word Amazon. In this case the article would go on to describe the forest, and make the context clear.',\n",
       "      'question': \"How can mutual information be used to disambiguate the word 'Amazon'?\",\n",
       "      'answer': \"Mutual information can be used to disambiguate the word 'Amazon' by comparing the occurrence of two groups of words; one group associated with the company Amazon (e.g., e-commerce, technology, online) and another group associated with the Amazon rain forest (e.g., rain, forest, tropical) in the given context.\"}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f37440823b0>},\n",
       " {'output': [{'response': [{'context': 'KL divergence is non-symmetric, i.e., there are\\n\\\\(P,Q\\\\)\\nsuch that\\n(22.11.22)\\n\\n\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\neq D_{\\\\textrm{KL}}(Q\\\\|P).\\\\]\\nKL divergence is non-negative, i.e.,\\n(22.11.23)\\n\\n\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\geq 0.\\\\]\\nNote that the equality holds only when\\n\\\\(P = Q\\\\)\\n.\\nIf there exists an\\n\\\\(x\\\\)\\nsuch that\\n\\\\(p(x) > 0\\\\)\\nand\\n\\\\(q(x) = 0\\\\)\\n, then\\n\\\\(D_{\\\\textrm{KL}}(P\\\\|Q) = \\\\infty\\\\)\\n.\\nThere is a close relationship between KL divergence and mutual\\ninformation. Besides the relationship shown in\\nFig. 22.11.1\\n,\\n\\\\(I(X, Y)\\\\)\\nis also\\nnumerically equivalent with the following terms:\\n\\\\(D_{\\\\textrm{KL}}(P(X, Y) \\\\ \\\\| \\\\ P(X)P(Y))\\\\)\\n;\\n\\\\(E_Y \\\\{ D_{\\\\textrm{KL}}(P(X \\\\mid Y) \\\\ \\\\| \\\\ P(X)) \\\\}\\\\)\\n;\\n\\\\(E_X \\\\{ D_{\\\\textrm{KL}}(P(Y \\\\mid X) \\\\ \\\\| \\\\ P(Y)) \\\\}\\\\)\\n.\\nFor the first term, we interpret mutual information as the KL\\ndivergence between\\n\\\\(P(X, Y)\\\\)\\nand the product of\\n\\\\(P(X)\\\\)\\nand\\n\\\\(P(Y)\\\\)\\n, and thus is a measure of how different the joint\\ndistribution is from the distribution if they were independent. For\\nthe second term, mutual information tells us the average reduction in\\nuncertainty about\\n\\\\(Y\\\\)\\nthat results from learning the value of\\nthe\\n\\\\(X\\\\)\\n’s distribution. Similarly to the third term.',\n",
       "      'question': 'What does KL divergence being non-symmetric imply about \\\\(P\\\\) and \\\\(Q\\\\)?',\n",
       "      'answer': 'It implies that \\\\(D_{\\\\textrm{KL}}(P\\\\|Q)\\\\) is not necessarily equal to \\\\(D_{\\\\textrm{KL}}(Q\\\\|P)\\\\), indicating the measure of difference from \\\\(P\\\\) to \\\\(Q\\\\) is not the same as from \\\\(Q\\\\) to \\\\(P\\\\).'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744082470>},\n",
       " {'output': [{'response': [{'context': 'Maximizing predictive probability of\\n\\\\(Q\\\\)\\nfor distribution\\n\\\\(P\\\\)\\n, (i.e.,\\n\\\\(E_{x \\\\sim P} [\\\\log (q(x))]\\\\)\\n);\\nMinimizing cross-entropy\\n\\\\(\\\\textrm{CE} (P, Q)\\\\)\\n;\\nMinimizing the KL divergence\\n\\\\(D_{\\\\textrm{KL}}(P\\\\|Q)\\\\)\\n.',\n",
       "      'question': 'What are the goals mentioned in the context related to distribution \\\\(P\\\\) and \\\\(Q\\\\)?',\n",
       "      'answer': 'Maximizing predictive probability of \\\\(Q\\\\) for distribution \\\\(P\\\\), minimizing cross-entropy between \\\\(P\\\\) and \\\\(Q\\\\), and minimizing the KL divergence from \\\\(P\\\\) to \\\\(Q\\\\).'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744082530>},\n",
       " {'output': [{'response': [{'context': 'Information theory is a field of study about encoding, decoding, transmitting, and manipulating information. Entropy is the unit to measure how much information is presented in different signals. KL divergence can also measure the divergence between two distributions. Cross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.',\n",
       "      'question': 'What is the purpose of minimizing cross-entropy loss in multi-class classification?',\n",
       "      'answer': 'Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f37440825f0>},\n",
       " {'output': [{'response': [{'context': 'In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.',\n",
       "      'question': 'What concept did Claude E. Shannon introduce for the first time in his 1948 article?',\n",
       "      'answer': 'The concept of information entropy.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f37440826b0>},\n",
       " {'output': [{'response': [{'context': '22.11. Information Theory\\n22.11.1. Information\\n22.11.1.1. Self-information\\n22.11.2. Entropy\\n22.11.2.1. Motivating Entropy\\n22.11.2.2. Definition\\n22.11.2.3. Interpretations\\n22.11.2.4. Properties of Entropy\\n22.11.3. Mutual Information\\n22.11.3.1. Joint Entropy\\n22.11.3.2. Conditional Entropy\\n22.11.3.3. Mutual Information\\n22.11.3.4. Properties of Mutual Information\\n22.11.3.5. Pointwise Mutual Information\\n22.11.3.6. Applications of Mutual Information\\n22.11.4. Kullback–Leibler Divergence\\n22.11.4.1. Definition\\n22.11.4.2. KL Divergence Properties\\n22.11.4.3. Example\\n22.11.5. Cross-Entropy\\n22.11.5.1. Formal Definition\\n22.11.5.2. Properties\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\n22.11.6. Summary\\n22.11.7. Exercises',\n",
       "      'question': 'What is the objective function of multi-class classification mentioned in the context?',\n",
       "      'answer': 'Cross-Entropy.'}],\n",
       "    'error': 'No errors.'}],\n",
       "  'root': <uniflow.node.Node at 0x7f3744082770>}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format result into pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7332c th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_7332c_row0_col0, #T_7332c_row0_col1, #T_7332c_row0_col2, #T_7332c_row1_col0, #T_7332c_row1_col1, #T_7332c_row1_col2, #T_7332c_row2_col0, #T_7332c_row2_col1, #T_7332c_row2_col2, #T_7332c_row3_col0, #T_7332c_row3_col1, #T_7332c_row3_col2, #T_7332c_row4_col0, #T_7332c_row4_col1, #T_7332c_row4_col2, #T_7332c_row5_col0, #T_7332c_row5_col1, #T_7332c_row5_col2, #T_7332c_row6_col0, #T_7332c_row6_col1, #T_7332c_row6_col2, #T_7332c_row7_col0, #T_7332c_row7_col1, #T_7332c_row7_col2, #T_7332c_row8_col0, #T_7332c_row8_col1, #T_7332c_row8_col2, #T_7332c_row9_col0, #T_7332c_row9_col1, #T_7332c_row9_col2, #T_7332c_row10_col0, #T_7332c_row10_col1, #T_7332c_row10_col2, #T_7332c_row11_col0, #T_7332c_row11_col1, #T_7332c_row11_col2, #T_7332c_row12_col0, #T_7332c_row12_col1, #T_7332c_row12_col2, #T_7332c_row13_col0, #T_7332c_row13_col1, #T_7332c_row13_col2, #T_7332c_row14_col0, #T_7332c_row14_col1, #T_7332c_row14_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7332c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7332c_level0_col0\" class=\"col_heading level0 col0\" >context</th>\n",
       "      <th id=\"T_7332c_level0_col1\" class=\"col_heading level0 col1\" >question</th>\n",
       "      <th id=\"T_7332c_level0_col2\" class=\"col_heading level0 col2\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7332c_row0_col0\" class=\"data row0 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_7332c_row0_col1\" class=\"data row0 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 article?</td>\n",
       "      <td id=\"T_7332c_row0_col2\" class=\"data row0 col2\" >The concept of information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7332c_row1_col0\" class=\"data row1 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_7332c_row1_col1\" class=\"data row1 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 article?</td>\n",
       "      <td id=\"T_7332c_row1_col2\" class=\"data row1 col2\" >Information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7332c_row2_col0\" class=\"data row2 col0\" >The universe is overflowing with information. Information provides a common language across disciplinary rifts: from Shakespeare's Sonnet to researchers' paper on Cornell ArXiv, from Van Gogh's printing Starry Night to Beethoven's music Symphony No.\n",
       "5, from the first programming language Plankalkül to the state-of-the-art machine learning algorithms. Everything must follow the rules of information theory, no matter the format. With information theory, we can measure and compare how much information is present in different signals. In this section, we will investigate the fundamental concepts of information theory and applications of information theory in machine learning.</td>\n",
       "      <td id=\"T_7332c_row2_col1\" class=\"data row2 col1\" >What provides a common language across disciplinary rifts according to the context?</td>\n",
       "      <td id=\"T_7332c_row2_col2\" class=\"data row2 col2\" >Information provides a common language across disciplinary rifts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7332c_row3_col0\" class=\"data row3 col0\" >Consider the following thought experiment. We have a friend with a deck of cards. They will shuffle the deck, flip over some cards, and tell us statements about the cards. We will try to assess the information content of each statement.</td>\n",
       "      <td id=\"T_7332c_row3_col1\" class=\"data row3 col1\" >What is the purpose of the thought experiment with a deck of cards?</td>\n",
       "      <td id=\"T_7332c_row3_col2\" class=\"data row3 col2\" >To assess the information content of each statement made about the cards.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7332c_row4_col0\" class=\"data row4 col0\" >If we read through these thought experiments, we see a natural idea. As a starting point, rather than caring about the knowledge, we may build off the idea that information represents the degree of surprise or the abstract possibility of the event. For example, if we want to describe an unusual event, we need a lot information. For a common event, we may not need much information.</td>\n",
       "      <td id=\"T_7332c_row4_col1\" class=\"data row4 col1\" >What does information represent according to the thought experiments?</td>\n",
       "      <td id=\"T_7332c_row4_col2\" class=\"data row4 col2\" >Information represents the degree of surprise or the abstract possibility of the event.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7332c_row5_col0\" class=\"data row5 col0\" >The information we gain by observing a random variable does not\n",
       "depend on what we call the elements, or the presence of additional\n",
       "elements which have probability zero.\n",
       "The information we gain by observing two random variables is no more\n",
       "than the sum of the information we gain by observing them separately.\n",
       "If they are independent, then it is exactly the sum.\n",
       "The information gained when observing (nearly) certain events is\n",
       "(nearly) zero.</td>\n",
       "      <td id=\"T_7332c_row5_col1\" class=\"data row5 col1\" >Does the information gained when observing (nearly) certain events have any significant value?</td>\n",
       "      <td id=\"T_7332c_row5_col2\" class=\"data row5 col2\" >No, the information gained when observing (nearly) certain events is (nearly) zero.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7332c_row6_col0\" class=\"data row6 col0\" >While proving this fact is beyond the scope of our text, it is important\n",
       "to know that this uniquely determines the form that entropy must take.\n",
       "The only ambiguity that these allow is in the choice of fundamental\n",
       "units, which is most often normalized by making the choice we saw before\n",
       "that the information provided by a single fair coin flip is one bit.</td>\n",
       "      <td id=\"T_7332c_row6_col1\" class=\"data row6 col1\" >What is the information provided by a single fair coin flip normalized to?</td>\n",
       "      <td id=\"T_7332c_row6_col2\" class=\"data row6 col2\" >One bit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7332c_row7_col0\" class=\"data row7 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_7332c_row7_col1\" class=\"data row7 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 publication?</td>\n",
       "      <td id=\"T_7332c_row7_col2\" class=\"data row7 col2\" >The concept of information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7332c_row8_col0\" class=\"data row8 col0\" >In 1965, Gordon Moore, co-founder of Intel, observed that the number of transistors on a microchip doubles approximately every two years. This observation came to be known as Moore's Law. It has had a profound impact on the development of computer technology, influencing the pace at which computing power has advanced over the years.</td>\n",
       "      <td id=\"T_7332c_row8_col1\" class=\"data row8 col1\" >What observation did Gordon Moore make in 1965 that has impacted the development of computer technology?</td>\n",
       "      <td id=\"T_7332c_row8_col2\" class=\"data row8 col2\" >Gordon Moore observed that the number of transistors on a microchip doubles approximately every two years, an observation that came to be known as Moore's Law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7332c_row9_col0\" class=\"data row9 col0\" >In this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate “Amazon”, we can compare which group has more occurrence in the context of the word Amazon. In this case the article would go on to describe the forest, and make the context clear.</td>\n",
       "      <td id=\"T_7332c_row9_col1\" class=\"data row9 col1\" >How can mutual information be used to disambiguate the word 'Amazon'?</td>\n",
       "      <td id=\"T_7332c_row9_col2\" class=\"data row9 col2\" >Mutual information can be used to disambiguate the word 'Amazon' by comparing the occurrence of two groups of words; one group associated with the company Amazon (e.g., e-commerce, technology, online) and another group associated with the Amazon rain forest (e.g., rain, forest, tropical) in the given context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_7332c_row10_col0\" class=\"data row10 col0\" >KL divergence is non-symmetric, i.e., there are\n",
       "\\(P,Q\\)\n",
       "such that\n",
       "(22.11.22)\n",
       "\n",
       "\\[D_{\\textrm{KL}}(P\\|Q) \\neq D_{\\textrm{KL}}(Q\\|P).\\]\n",
       "KL divergence is non-negative, i.e.,\n",
       "(22.11.23)\n",
       "\n",
       "\\[D_{\\textrm{KL}}(P\\|Q) \\geq 0.\\]\n",
       "Note that the equality holds only when\n",
       "\\(P = Q\\)\n",
       ".\n",
       "If there exists an\n",
       "\\(x\\)\n",
       "such that\n",
       "\\(p(x) > 0\\)\n",
       "and\n",
       "\\(q(x) = 0\\)\n",
       ", then\n",
       "\\(D_{\\textrm{KL}}(P\\|Q) = \\infty\\)\n",
       ".\n",
       "There is a close relationship between KL divergence and mutual\n",
       "information. Besides the relationship shown in\n",
       "Fig. 22.11.1\n",
       ",\n",
       "\\(I(X, Y)\\)\n",
       "is also\n",
       "numerically equivalent with the following terms:\n",
       "\\(D_{\\textrm{KL}}(P(X, Y) \\ \\| \\ P(X)P(Y))\\)\n",
       ";\n",
       "\\(E_Y \\{ D_{\\textrm{KL}}(P(X \\mid Y) \\ \\| \\ P(X)) \\}\\)\n",
       ";\n",
       "\\(E_X \\{ D_{\\textrm{KL}}(P(Y \\mid X) \\ \\| \\ P(Y)) \\}\\)\n",
       ".\n",
       "For the first term, we interpret mutual information as the KL\n",
       "divergence between\n",
       "\\(P(X, Y)\\)\n",
       "and the product of\n",
       "\\(P(X)\\)\n",
       "and\n",
       "\\(P(Y)\\)\n",
       ", and thus is a measure of how different the joint\n",
       "distribution is from the distribution if they were independent. For\n",
       "the second term, mutual information tells us the average reduction in\n",
       "uncertainty about\n",
       "\\(Y\\)\n",
       "that results from learning the value of\n",
       "the\n",
       "\\(X\\)\n",
       "’s distribution. Similarly to the third term.</td>\n",
       "      <td id=\"T_7332c_row10_col1\" class=\"data row10 col1\" >What does KL divergence being non-symmetric imply about \\(P\\) and \\(Q\\)?</td>\n",
       "      <td id=\"T_7332c_row10_col2\" class=\"data row10 col2\" >It implies that \\(D_{\\textrm{KL}}(P\\|Q)\\) is not necessarily equal to \\(D_{\\textrm{KL}}(Q\\|P)\\), indicating the measure of difference from \\(P\\) to \\(Q\\) is not the same as from \\(Q\\) to \\(P\\).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_7332c_row11_col0\" class=\"data row11 col0\" >Maximizing predictive probability of\n",
       "\\(Q\\)\n",
       "for distribution\n",
       "\\(P\\)\n",
       ", (i.e.,\n",
       "\\(E_{x \\sim P} [\\log (q(x))]\\)\n",
       ");\n",
       "Minimizing cross-entropy\n",
       "\\(\\textrm{CE} (P, Q)\\)\n",
       ";\n",
       "Minimizing the KL divergence\n",
       "\\(D_{\\textrm{KL}}(P\\|Q)\\)\n",
       ".</td>\n",
       "      <td id=\"T_7332c_row11_col1\" class=\"data row11 col1\" >What are the goals mentioned in the context related to distribution \\(P\\) and \\(Q\\)?</td>\n",
       "      <td id=\"T_7332c_row11_col2\" class=\"data row11 col2\" >Maximizing predictive probability of \\(Q\\) for distribution \\(P\\), minimizing cross-entropy between \\(P\\) and \\(Q\\), and minimizing the KL divergence from \\(P\\) to \\(Q\\).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_7332c_row12_col0\" class=\"data row12 col0\" >Information theory is a field of study about encoding, decoding, transmitting, and manipulating information. Entropy is the unit to measure how much information is presented in different signals. KL divergence can also measure the divergence between two distributions. Cross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.</td>\n",
       "      <td id=\"T_7332c_row12_col1\" class=\"data row12 col1\" >What is the purpose of minimizing cross-entropy loss in multi-class classification?</td>\n",
       "      <td id=\"T_7332c_row12_col2\" class=\"data row12 col2\" >Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_7332c_row13_col0\" class=\"data row13 col0\" >In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948) establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here.</td>\n",
       "      <td id=\"T_7332c_row13_col1\" class=\"data row13 col1\" >What concept did Claude E. Shannon introduce for the first time in his 1948 article?</td>\n",
       "      <td id=\"T_7332c_row13_col2\" class=\"data row13 col2\" >The concept of information entropy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332c_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_7332c_row14_col0\" class=\"data row14 col0\" >22.11. Information Theory\n",
       "22.11.1. Information\n",
       "22.11.1.1. Self-information\n",
       "22.11.2. Entropy\n",
       "22.11.2.1. Motivating Entropy\n",
       "22.11.2.2. Definition\n",
       "22.11.2.3. Interpretations\n",
       "22.11.2.4. Properties of Entropy\n",
       "22.11.3. Mutual Information\n",
       "22.11.3.1. Joint Entropy\n",
       "22.11.3.2. Conditional Entropy\n",
       "22.11.3.3. Mutual Information\n",
       "22.11.3.4. Properties of Mutual Information\n",
       "22.11.3.5. Pointwise Mutual Information\n",
       "22.11.3.6. Applications of Mutual Information\n",
       "22.11.4. Kullback–Leibler Divergence\n",
       "22.11.4.1. Definition\n",
       "22.11.4.2. KL Divergence Properties\n",
       "22.11.4.3. Example\n",
       "22.11.5. Cross-Entropy\n",
       "22.11.5.1. Formal Definition\n",
       "22.11.5.2. Properties\n",
       "22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\n",
       "22.11.6. Summary\n",
       "22.11.7. Exercises</td>\n",
       "      <td id=\"T_7332c_row14_col1\" class=\"data row14 col1\" >What is the objective function of multi-class classification mentioned in the context?</td>\n",
       "      <td id=\"T_7332c_row14_col2\" class=\"data row14 col2\" >Cross-Entropy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3744083940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting context, question, and answer into a DataFrame\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in output:\n",
    "    for i in item['output']:\n",
    "        for response in i['response']:\n",
    "            contexts.append(response['context'])\n",
    "            questions.append(response['question'])\n",
    "            answers.append(response['answer'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)  # or use a specific width like 50\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "styled_df = df.style.set_properties(**{'text-align': 'left'}).set_table_styles([{\n",
    "    'selector': 'th',\n",
    "    'props': [('text-align', 'left')]\n",
    "}])\n",
    "styled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
