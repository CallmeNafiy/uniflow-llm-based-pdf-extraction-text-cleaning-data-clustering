{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions using Huggingface Open Source Models\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM)? In this example, we demonstrate how to use AutoRater for verifying the correctness of an answer to a specific question and its context, using open-source Huggingface models.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths, install and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "!{sys.executable} -m pip install -q transformers accelerate bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForClassificationHuggingfaceConfig,\n",
    "    HuggingfaceModelConfig,\n",
    ")\n",
    "from uniflow.op.prompt import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three example raw inputs. Each one is a tuple consisting of context, question, and answer to be labeled. The ground truth label of the first one is 'correct', and the others are 'incorrect'. Then, we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using Mistral-7B-Instruct-v0.2\n",
    "\n",
    "In this example, we will use the Mistral-Instruct-7b model as the default LLM. If you want to use open-source models, you can replace with Huggingface models.\n",
    "\n",
    "We use the default `guided_prompt` in `RaterForClassificationHuggingfaceConfig`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the huggingface model. Configuration for the huggingeface model. Includes model_name(\"mistralai/Mistral-7B-Instruct-v0.2\"), model_server (\"HuggingfaceModelServer\"), batch_size (1), neuron (False), load_in_4bit (False), load_in_8bit (True), responese_start_key(\"exaplanation\"), response_format({\"type\": \"json_object\"})\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"Yes\": 1.0, \"No\": 0.0}.\n",
    "- `prompt_template` (GuidedPrompt): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, answer, label, and explanation for each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In `model_config`, `response_format` decides whether model generates a plain text or a json object. `response_start_key` is what you want model to first generate. Because we are using chain of thoughts (CoT) prompt in default, so the first generate field is `explanation` (see default `few_shot_prompt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'model_server': 'HuggingfaceModelServer', 'batch_size': 1, 'neuron': False, 'load_in_4bit': False, 'load_in_8bit': True, 'response_start_key': 'explanation', 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, prompt_template=PromptTemplate(instruction=\"Evaluate if a given answer is appropriate based on the question and the context.\\n            Follow the format of the examples below, consisting of context, question, answer, explanation and label (you must choose one from ['Yes', 'No']).\", few_shot_prompt=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The answer is consistency to the fact that Eiffel Tower was constructed in 1889 mentioned in context, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells but not mitochondria indicated by answer, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "config = RaterForClassificationHuggingfaceConfig(\n",
    "    model_config=HuggingfaceModelConfig(\n",
    "        response_start_key=\"explanation\", \n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        batch_size=1\n",
    "    )\n",
    ")\n",
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': [{'answer': 'The largest ocean on Earth is the '\n",
      "                                      'Pacific Ocean.',\n",
      "                            'context': 'The Pacific Ocean is the largest and '\n",
      "                                       \"deepest of Earth's oceanic divisions. \"\n",
      "                                       'It extends from the Arctic Ocean in '\n",
      "                                       'the north to the Southern Ocean in the '\n",
      "                                       'south.',\n",
      "                            'explanation': 'The answer is consistent with the '\n",
      "                                           'fact stated in the context that '\n",
      "                                           'the Pacific Ocean is the largest '\n",
      "                                           'ocean on Earth, so the answer is '\n",
      "                                           'correct.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'What is the largest ocean on Earth?'}],\n",
      "              'scores': [1.0],\n",
      "              'votes': ['yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f8d7c765b40>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'Shakespeare wrote 31 plays.',\n",
      "                            'context': 'Shakespeare, a renowned English '\n",
      "                                       'playwright and poet, wrote 39 plays '\n",
      "                                       'during his lifetime. His works include '\n",
      "                                       \"famous plays like 'Hamlet' and 'Romeo \"\n",
      "                                       \"and Juliet'.\",\n",
      "                            'explanation': 'The answer is inconsistent with '\n",
      "                                           'the fact stated in the context '\n",
      "                                           'that Shakespeare wrote 39 plays, '\n",
      "                                           'so the answer is incorrect.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'How many plays did Shakespeare '\n",
      "                                        'write?'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f8f0901f700>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'The human brain is responsible for '\n",
      "                                      'physical movement.',\n",
      "                            'context': 'The human brain is an intricate organ '\n",
      "                                       'responsible for intelligence, memory, '\n",
      "                                       'and emotions. It is made up of '\n",
      "                                       'approximately 86 billion neurons.',\n",
      "                            'explanation': 'The answer is inconsistent with '\n",
      "                                           'the context which states that the '\n",
      "                                           'human brain is responsible for '\n",
      "                                           'intelligence, memory, and '\n",
      "                                           'emotions, so the answer is '\n",
      "                                           'incorrect.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'What is the human brain responsible '\n",
      "                                        'for?'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f8f0901e3b0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The largest ocean on Earth is the Pacific Ocean.',\n",
      " 'context': \"The Pacific Ocean is the largest and deepest of Earth's oceanic \"\n",
      "            'divisions. It extends from the Arctic Ocean in the north to the '\n",
      "            'Southern Ocean in the south.',\n",
      " 'explanation': 'The answer is consistent with the fact stated in the context '\n",
      "                'that the Pacific Ocean is the largest ocean on Earth, so the '\n",
      "                'answer is correct.',\n",
      " 'label': 'Yes',\n",
      " 'question': 'What is the largest ocean on Earth?'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31myes\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n",
      "data 2 has label \u001b[31mno\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using Mistral-7B-Instruct-v0.2\n",
    "\n",
    "Following the previous settings, but we will change the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'model_server': 'HuggingfaceModelServer', 'batch_size': 1, 'neuron': False, 'load_in_4bit': False, 'load_in_8bit': True, 'response_start_key': 'explanation', 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, prompt_template=PromptTemplate(instruction=\"Evaluate if a given answer is appropriate based on the question and the context.\\n            Follow the format of the examples below, consisting of context, question, answer, explanation and label (you must choose one from ['Yes', 'No']).\", few_shot_prompt=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The answer is consistency to the fact that Eiffel Tower was constructed in 1889 mentioned in context, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells but not mitochondria indicated by answer, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForClassificationHuggingfaceConfig(\n",
    "    model_config=HuggingfaceModelConfig(\n",
    "        response_start_key=\"explanation\", \n",
    "        response_format={\"type\": \"text\"},\n",
    "        batch_size=1\n",
    "    )\n",
    ")\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': ['instruction: Evaluate if a given answer is '\n",
      "                           'appropriate based on the question and the '\n",
      "                           'context.\\n'\n",
      "                           '            Follow the format of the examples '\n",
      "                           'below, consisting of context, question, answer, '\n",
      "                           'explanation and label (you must choose one from '\n",
      "                           \"['Yes', 'No']).\\n\"\n",
      "                           'context: The Eiffel Tower, located in Paris, '\n",
      "                           'France, is one of the most famous landmarks in the '\n",
      "                           'world. It was constructed in 1889 and stands at a '\n",
      "                           'height of 324 meters.\\n'\n",
      "                           'question: When was the Eiffel Tower constructed?\\n'\n",
      "                           'answer: The Eiffel Tower was constructed in 1889.\\n'\n",
      "                           'explanation: The answer is consistency to the fact '\n",
      "                           'that Eiffel Tower was constructed in 1889 '\n",
      "                           'mentioned in context, so the answer is correct.\\n'\n",
      "                           'label: Yes\\n'\n",
      "                           'context: Photosynthesis is a process used by '\n",
      "                           'plants to convert light energy into chemical '\n",
      "                           'energy. This process primarily occurs in the '\n",
      "                           'chloroplasts of plant cells.\\n'\n",
      "                           'question: Where does photosynthesis primarily '\n",
      "                           'occur in plant cells?\\n'\n",
      "                           'answer: Photosynthesis primarily occurs in the '\n",
      "                           'mitochondria of plant cells.\\n'\n",
      "                           'explanation: The context mentions that '\n",
      "                           'photosynthesis primarily occurs in the '\n",
      "                           'chloroplasts of plant cells but not mitochondria '\n",
      "                           'indicated by answer, so the answer is incorrect.\\n'\n",
      "                           'label: No\\n'\n",
      "                           'context: The Pacific Ocean is the largest and '\n",
      "                           \"deepest of Earth's oceanic divisions. It extends \"\n",
      "                           'from the Arctic Ocean in the north to the Southern '\n",
      "                           'Ocean in the south.\\n'\n",
      "                           'question: What is the largest ocean on Earth?\\n'\n",
      "                           'answer: The largest ocean on Earth is the Pacific '\n",
      "                           'Ocean. \\n'\n",
      "                           'explanation: The answer is consistent with the '\n",
      "                           'fact stated in the context that the Pacific Ocean '\n",
      "                           'is the largest ocean on Earth, so the answer is '\n",
      "                           'correct.\\n'\n",
      "                           '\\n'\n",
      "                           'label: Yes'],\n",
      "              'scores': [1.0],\n",
      "              'votes': ['yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f8d7c1ba3e0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['instruction: Evaluate if a given answer is '\n",
      "                           'appropriate based on the question and the '\n",
      "                           'context.\\n'\n",
      "                           '            Follow the format of the examples '\n",
      "                           'below, consisting of context, question, answer, '\n",
      "                           'explanation and label (you must choose one from '\n",
      "                           \"['Yes', 'No']).\\n\"\n",
      "                           'context: The Eiffel Tower, located in Paris, '\n",
      "                           'France, is one of the most famous landmarks in the '\n",
      "                           'world. It was constructed in 1889 and stands at a '\n",
      "                           'height of 324 meters.\\n'\n",
      "                           'question: When was the Eiffel Tower constructed?\\n'\n",
      "                           'answer: The Eiffel Tower was constructed in 1889.\\n'\n",
      "                           'explanation: The answer is consistency to the fact '\n",
      "                           'that Eiffel Tower was constructed in 1889 '\n",
      "                           'mentioned in context, so the answer is correct.\\n'\n",
      "                           'label: Yes\\n'\n",
      "                           'context: Photosynthesis is a process used by '\n",
      "                           'plants to convert light energy into chemical '\n",
      "                           'energy. This process primarily occurs in the '\n",
      "                           'chloroplasts of plant cells.\\n'\n",
      "                           'question: Where does photosynthesis primarily '\n",
      "                           'occur in plant cells?\\n'\n",
      "                           'answer: Photosynthesis primarily occurs in the '\n",
      "                           'mitochondria of plant cells.\\n'\n",
      "                           'explanation: The context mentions that '\n",
      "                           'photosynthesis primarily occurs in the '\n",
      "                           'chloroplasts of plant cells but not mitochondria '\n",
      "                           'indicated by answer, so the answer is incorrect.\\n'\n",
      "                           'label: No\\n'\n",
      "                           'context: Shakespeare, a renowned English '\n",
      "                           'playwright and poet, wrote 39 plays during his '\n",
      "                           'lifetime. His works include famous plays like '\n",
      "                           \"'Hamlet' and 'Romeo and Juliet'.\\n\"\n",
      "                           'question: How many plays did Shakespeare write?\\n'\n",
      "                           'answer: Shakespeare wrote 31 plays. \\n'\n",
      "                           'explanation: The answer is inconsistent with the '\n",
      "                           'fact stated in the context that Shakespeare wrote '\n",
      "                           '39 plays, so the answer is incorrect.\\n'\n",
      "                           '\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f8d7c1c2bf0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['instruction: Evaluate if a given answer is '\n",
      "                           'appropriate based on the question and the '\n",
      "                           'context.\\n'\n",
      "                           '            Follow the format of the examples '\n",
      "                           'below, consisting of context, question, answer, '\n",
      "                           'explanation and label (you must choose one from '\n",
      "                           \"['Yes', 'No']).\\n\"\n",
      "                           'context: The Eiffel Tower, located in Paris, '\n",
      "                           'France, is one of the most famous landmarks in the '\n",
      "                           'world. It was constructed in 1889 and stands at a '\n",
      "                           'height of 324 meters.\\n'\n",
      "                           'question: When was the Eiffel Tower constructed?\\n'\n",
      "                           'answer: The Eiffel Tower was constructed in 1889.\\n'\n",
      "                           'explanation: The answer is consistency to the fact '\n",
      "                           'that Eiffel Tower was constructed in 1889 '\n",
      "                           'mentioned in context, so the answer is correct.\\n'\n",
      "                           'label: Yes\\n'\n",
      "                           'context: Photosynthesis is a process used by '\n",
      "                           'plants to convert light energy into chemical '\n",
      "                           'energy. This process primarily occurs in the '\n",
      "                           'chloroplasts of plant cells.\\n'\n",
      "                           'question: Where does photosynthesis primarily '\n",
      "                           'occur in plant cells?\\n'\n",
      "                           'answer: Photosynthesis primarily occurs in the '\n",
      "                           'mitochondria of plant cells.\\n'\n",
      "                           'explanation: The context mentions that '\n",
      "                           'photosynthesis primarily occurs in the '\n",
      "                           'chloroplasts of plant cells but not mitochondria '\n",
      "                           'indicated by answer, so the answer is incorrect.\\n'\n",
      "                           'label: No\\n'\n",
      "                           'context: The human brain is an intricate organ '\n",
      "                           'responsible for intelligence, memory, and '\n",
      "                           'emotions. It is made up of approximately 86 '\n",
      "                           'billion neurons.\\n'\n",
      "                           'question: What is the human brain responsible '\n",
      "                           'for?\\n'\n",
      "                           'answer: The human brain is responsible for '\n",
      "                           'physical movement. \\n'\n",
      "                           'explanation: The answer is inconsistent with the '\n",
      "                           'context which states that the human brain is '\n",
      "                           'responsible for intelligence, memory, and '\n",
      "                           'emotions, so the answer is incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f8f0901f970>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
